[
    {
        "url": "https://github.com/modularml/mojo/blob/main/LICENSE",
        "content": "==============================================================================================\nThe Mojo repository is licensed under the Apache License v2.0 with LLVM Exceptions:\n==============================================================================================\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n    1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n    2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n    3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n    4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n    5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n    6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n    7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n    8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n    9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n    END OF TERMS AND CONDITIONS\n\n    APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n    Copyright [yyyy] [name of copyright owner]\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n---- LLVM Exceptions to the Apache 2.0 License ----\n\nAs an exception, if, as a result of your compiling your source code, portions\nof this Software are embedded into an Object form of such source code, you\nmay redistribute such embedded portions in such Object form without complying\nwith the conditions of Sections 4(a), 4(b) and 4(d) of the License.\n\nIn addition, if you combine or link compiled forms of this Software with\nsoftware that is licensed under the GPLv2 (\"Combined Software\") and if a\ncourt of competent jurisdiction determines that the patent provision (Section\n3), the indemnity provision (Section 9) or other Section of the License\nconflicts with the conditions of the GPLv2, you may retroactively and\nprospectively choose to deem waived or otherwise exclude such Section(s) of\nthe License, but only in their entirety and only with respect to the Combined\nSoftware.\n\n==============================================================================\nSoftware from third parties included in the LLVM Project:\n==============================================================================\n\nThe LLVM Project contains third party software which is under different license\nterms. All such code will be identified clearly using at least one of two\nmechanisms:\n\n1) It will be in a separate directory tree with its own `LICENSE.txt` or\n   `LICENSE` file at the top containing the specific license and restrictions\n   which apply to that software, or\n2) It will contain specific license and restriction terms at the top of every\n   file.\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/README.md",
        "content": "# Mojo code examples\n\nA collection of sample programs and Mojo notebooks written in the  \n[Mojo](https://docs.modular.com/mojo/programming-manual.html) programming language.\n\n## Getting Started\n\nAccess a Mojo programming environment available from the  \nMojo product [page](https://www.modular.com/mojo).\n\nGit clone the repository of Mojo samples using the command below:\n\n```bash\ngit clone https://github.com/modularml/mojo.git\n```\n\n## Running\n\nUse the following sample command-line to run the programs:\n\n```bash\nmojo matmul.mojo\n```\n\nYou can run the Mojo notebooks using [JupyterLab or Visual Studio  \nCode](notebooks/README.md) with the Mojo extension available on the Marketplace.\n\n### Mojo SDK Container\n\nThe repo also contains a Dockerfile that can be used to create a  \nMojo SDK container for developing and running Mojo programs. Use the  \ncontainer in conjunction with the Visual Studio Code devcontainers  \nextension to develop directly inside the container.\n\nThe Dockerfile also sets up a `conda` environment and by default,  \nstarts a `jupyter` server (which you can access via the browser).\n\nTo build a Mojo container, either use\n[docker-compose](https://docs.docker.com/compose/) in `mojo/examples/docker`:\n\n```bash\ndocker compose up -d\n```\n\nOr the convenience script provided:\n\n```bash\n./build-image.sh --auth-key <your-modular-auth-key> \\\n   --mojo-version 0.3\n```\n\nThe script also supports building with `podman` instead of `docker`:\n\n```bash\n./build-image.sh --auth-key <your-modular-auth-key> \\\n   --use-podman \\\n   --mojo-version 0.3\n   \n```\n\nYou can then run with either `docker` or `podman`. In the example below,  \nwe map the ports, bind mount the current directory and open a shell into  \nthe container:\n\n```bash\ndocker run \\\n   -it --rm \\\n   -p 8888:8888 \\\n   --net host \\\n   -v ${PWD}:${PWD} \\\n   modular/mojo-v0.3-20232109-1205 bash\n```\n\n`podman` requires an additional argument to add the `SYS_PTRACE` capabilities:\n\n```bash\npodman run \\\n   --cap-add SYS_PTRACE \\\n   -it --rm \\\n   -p 8888:8888 \\\n   --net host \\\n   -v ${PWD}:${PWD} \\\n   modular/mojo-v0.3-20232109-1205 bash\n```\n\n## License\n\nThe Mojo examples and notebooks in this repository are licensed  \nunder the Apache License v2.0 with LLVM Exceptions  \n(see the LLVM [License](https://llvm.org/LICENSE.txt)).\n\n## Contributing\n\nThanks for your interest in contributing to this repository!  \nWe are not accepting pull requests at this time, but are actively  \nworking on a process to accept contributions. Please stay tuned.\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/modverse-weekly/issue-11.md",
        "content": "# ModVerse Weekly - Issue 11\n\n## Introduction üì∞\n\nWelcome to Issue 11!\n\nModCon is less than one week away, and we've been crunching to land some extra\nfeatures for `mojo v0.6.0` in preparation for the demos. The team is very\nexcited to share what we've built!\n\nDue to overwhelming demand and selling out tickets, we decided to livestream the\nkeynote so that everyone can enjoy the announcements together. The videos from\nthe smaller lightning talks will be released to YouTube at a later date. Chat\nwill be disabled on YouTube so make sure to join the Discord where everyone will\nbe hanging out in the `#modcon` channel.\n\n## General üíª\n\n- [Jack](https://www.modular.com/team/jack-clayton) Released a blog post:\n  [Mojo üî• Advent of Code 2023](https://www.modular.com/blog/mojo-advent-of-code-2023)\n\n## Community üì°\n\n### ModCon Finalists\n\nCongratulations to the six finalists for our ModCon Contest!\n\nThese will all be presented on stage at ModCon, and one winner will be selected,\nmake sure to check them all out.\n\n- Stijn with [dainemo](https://github.com/StijnWoestenborghs/dainemo): A Machine\n  Learning framework from scratch in Mojo üî•\n- TilleFe with [Infermo](https://github.com/TilliFe/Infermo): Tensors and\n  dynamic Neural Networks in Mojo üî•\n- Vilson with [mojograd](http://github.com/automata/mojograd): a Mojo\n  implementation of Andrej Karparthy's\n  [micrograd](https://github.com/karpathy/micrograd)\n- Leandro with [specials](https://github.com/leandrolcampos/specials): Special\n  functions with hardware acceleration\n- Maxim with [mojo-flx](https://github.com/mzaks/mojo-flx/): a Mojo\n  implementation of FlexBuffers\n- Alex with\n  [spatial envelope optimization in Mojo](https://github.com/guidorice/modcon23-contest)\n  which also makes use of his mojo-pytest project\n\n### Runners Up\n\nAnd congratulations to all the runners-up submissions:\n\n- Andrew with [langtrain](https://github.com/andrewlayer/langtrain): teaching an\n  LLM esoteric languages\n- Viet Anh with [chess.mojo](https://github.com/vietanhdev/chess.mojo): The\n  first UCI chess engine in Mojo üî•\n- Kevin with [Mojonut](https://github.com/hoxha-saber/Mojonut): a mojo\n  implementation of donut.c\n- Bernado with\n  [material-color-utilities](https://github.com/bernaferrari/material-color-utilities-mojo)\n  a Mojo port of the Material Color Utilities‚Äç\n- Amit who started a\n  [Mojo Programming Manual‚Äç](https://amitxshukla.github.io/Mojo/intro.html)\n- Helenex with [Moplex](https://github.com/helehex/moplex): complex types for\n  Mojo üî•\n\n### Other Projects\n\nThere were some more projects released over the last week that weren't submitted:\n\n- Modular staff [Zac](https://www.modular.com/team/zac-bowling) with\n  [conway.mojo](https://gist.github.com/zbowling/79a18bcd70f3e82eb4f957687a93e8a1):\n  a Mojo implementation of Conway's Game of Life\n- rd4com with his\n  [tutorial](https://github.com/rd4com/mojo-learning/blob/main/tutorials/parameters-alias-struct-parameter-deduction.md)\n  covering many alias, parameter and compile-time features. Check out his\n  [entire catalog of tutorials](https://github.com/rd4com/mojo-learning/tree/main/tutorials)\n- Leonardo with [simpletools](https://github.com/lleites/simpletools) containing\n  a list and testing module\n- Rama with a\n  [Mojo Udemy course](https://www.udemy.com/course/mojo-programming-basic-to-expert-with-ai-use-cases)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/mojo-matrix-slice.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: code ---\nfrom memory import memset_zero\nfrom algorithm import vectorize, parallelize\nfrom sys.intrinsics import strided_load\nfrom math import trunc, mod\nfrom random import rand\n\nstruct Matrix[dtype: DType = DType.float32]:\n    var dim0: Int\n    var dim1: Int\n    var _data: DTypePointer[dtype]\n    alias simd_width: Int = simdwidthof[dtype]()\n\n    fn __init__(inout self, *dims: Int):\n        self.dim0 = dims[0]\n        self.dim1 = dims[1]\n        self._data = DTypePointer[dtype].alloc(dims[0] * dims[1])\n        rand(self._data, dims[0] * dims[1])\n        \n    fn __copyinit__(inout self, other: Self):\n        self._data = other._data\n        self.dim0 = other.dim0\n        self.dim1 = other.dim1\n\n    fn _adjust_slice_(self, inout span: slice, dim: Int):\n        if span.start < 0:\n            span.start = dim + span.start\n        if not span._has_end():\n            span.end = dim\n        elif span.end < 0:\n            span.end = dim + span.end\n        if span.end > dim:\n            span.end = dim\n        if span.end < span.start:\n            span.start = 0\n            span.end = 0\n\n    fn __getitem__(self, x: Int, y: Int) -> SIMD[dtype,1]:\n        return self._data.simd_load[1](x * self.dim1 + y)\n\n    fn __getitem__(self, owned row_slice: slice, col: Int) -> Self:\n        return self.__getitem__(row_slice, slice(col,col+1))\n\n    fn __getitem__(self, row: Int, owned col_slice: slice) -> Self:\n        return self.__getitem__(slice(row,row+1),col_slice)\n\n    fn __getitem__(self, owned row_slice: slice, owned col_slice: slice) -> Self:\n        self._adjust_slice_(row_slice, self.dim0)\n        self._adjust_slice_(col_slice, self.dim1)\n\n        var src_ptr = self._data\n        var sliced_mat = Self(row_slice.__len__(),col_slice.__len__())\n\n        @parameter\n        fn slice_column(idx_rows: Int):\n            src_ptr = self._data.offset(row_slice[idx_rows]*self.dim1+col_slice[0])\n            @parameter\n            fn slice_row[simd_width: Int](idx: Int) -> None:\n                sliced_mat._data.simd_store[simd_width](idx+idx_rows*col_slice.__len__(),\n                                                        strided_load[dtype,simd_width](src_ptr,col_slice.step))\n                src_ptr = src_ptr.offset(simd_width*col_slice.step)\n            vectorize[self.simd_width,slice_row](col_slice.__len__())\n        parallelize[slice_column](row_slice.__len__(),row_slice.__len__())\n        return sliced_mat\n\n    fn print(self, prec: Int=4)->None:\n        var rank:Int = 2\n        var dim0:Int = 0\n        var dim1:Int = 0\n        var val:SIMD[dtype, 1]=0.0\n        if self.dim0 == 1:\n            rank = 1\n            dim0 = 1\n            dim1 = self.dim1\n        else:\n            dim0 = self.dim0\n            dim1 = self.dim1\n        if dim0>0 and dim1>0:\n            for j in range(dim0):\n                if rank>1:\n                    if j==0:\n                        print_no_newline(\"  [\")\n                    else:\n                        print_no_newline(\"\\n   \")\n                print_no_newline(\"[\")\n                for k in range(dim1):\n                    if rank==1:\n                        val = self._data.simd_load[1](k)\n                    if rank==2:\n                        val = self[j,k]\n                    let int_str: String\n                    if val > 0 or val == 0:\n                        int_str = String(trunc(val).cast[DType.int32]())\n                    else:\n                        int_str = \"-\"+String(trunc(val).cast[DType.int32]())\n                        val = -val\n                    let float_str: String\n                    float_str = String(mod(val,1))\n                    let s = int_str+\".\"+float_str[2:prec+2]\n                    if k==0:\n                        print_no_newline(s)\n                    else:\n                        print_no_newline(\"  \",s)\n                print_no_newline(\"]\")\n            if rank>1:\n                print_no_newline(\"]\")\n            print()\n            if rank>2:\n                print(\"]\")\n        print(\"  Matrix:\",self.dim0,'x',self.dim1,\",\",\"DType:\", dtype.__str__())\n        print()\n\n\nfn main():\n    let mat = Matrix(8,5)\n    mat.print()\n\n    mat[2:4,-3:].print()\n    mat[1:3,:].print()\n    mat[0:3,0:3].print()\n    mat[1::2,::2].print()\n    mat[:,-1:2].print()\n    mat[-1:2,:].print()\n\nmain()\n--- cell type: code ---\nlet mat = Matrix(8,5)\nmat.print()\n--- cell type: code ---\nmat[2:4,-3:].print()\n--- cell type: code ---\nmat[1:3,:].print()\n--- cell type: code ---\nmat[0:3,0:3].print()\n--- cell type: code ---\nmat[1::2,::2].print()\n--- cell type: code ---\nmat[:,-1:2].print()\nmat[-1:2,:].print()"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/mojo-plotter/README.md",
        "content": "# Mojo Plotter\n\n## Installation\n\n### Conda\n\nIf you don't have `conda`, install [miniconda here](https://docs.conda.io/projects/miniconda/en/latest/#quick-command-line-install)\n\n### Create conda environment\n\nCreate and acivate conda enironment:\n\n#### General\n\n```bash\nconda env create -f environment.yaml\nconda activate mojo-plotter\n```\n\n### Auto Set Mojo Environment\n\nTo automatically set Mojo to use the python environment when you activate it:\n\n#### Macos/Linux\n\n```bash\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\nexport MOJO_PYTHON_LIBRARY=\"$(find $CONDA_PREFIX/lib -iname 'libpython*.[s,d]*' | sort -r | head -n 1)\"\necho \"export MOJO_PYTHON_LIBRARY=\\\"$MOJO_PYTHON_LIBRARY\\\"\" > $CONDA_PREFIX/etc/conda/activate.d/export-mojo.sh\n\nmkdir -p $CONDA_PREFIX/etc/conda/deactivate.d\necho \"unset MOJO_PYTHON_LIBRARY\" > $CONDA_PREFIX/etc/conda/deactivate.d/unset-mojo.sh\n```\n\n## Usage\n\nSimply activate the environment and run the program:\n\n```bash\nconda activate mojo-plotter\nmojo main.mojo\n```\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/mojo-plotter/main.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nfrom python import Python\n\n\nfn main() raises:\n    let torch = Python.import_module(\"torch\")\n    let x = torch.linspace(0, 10, 100)\n    let y = torch.sin(x)\n    plot(x, y)\n\n\ndef plot(x: PythonObject, y: PythonObject) -> None:\n    let plt = Python.import_module(\"matplotlib.pyplot\")\n    plt.plot(x.numpy(), y.numpy())\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Plot of y = sin(x)\")\n    plt.grid(True)\n    plt.show()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/tensorutils/__init__.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nfrom .tensorutils import *\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/tensorutils/tensorutils.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nfrom tensor import Tensor\nfrom math import trunc, mod\n\n\nfn tensorprint[type: DType](t: Tensor[type]) -> None:\n    let rank = t.rank()\n    var dim0: Int = 0\n    var dim1: Int = 0\n    var dim2: Int = 0\n    if rank == 0 or rank > 3:\n        print(\"Error: Tensor rank should be: 1,2, or 3. Tensor rank is \", rank)\n        return\n    if rank == 1:\n        dim0 = 1\n        dim1 = 1\n        dim2 = t.dim(0)\n    if rank == 2:\n        dim0 = 1\n        dim1 = t.dim(0)\n        dim2 = t.dim(1)\n    if rank == 3:\n        dim0 = t.dim(0)\n        dim1 = t.dim(1)\n        dim2 = t.dim(2)\n    var val: SIMD[type, 1] = 0.0\n    for i in range(dim0):\n        if i == 0 and rank == 3:\n            print(\"[\")\n        else:\n            if i > 0:\n                print()\n        for j in range(dim1):\n            if rank != 1:\n                if j == 0:\n                    print_no_newline(\"  [\")\n                else:\n                    print_no_newline(\"\\n   \")\n            print_no_newline(\"[\")\n            for k in range(dim2):\n                if rank == 1:\n                    val = t[k]\n                if rank == 2:\n                    val = t[j, k]\n                if rank == 3:\n                    val = t[i, j, k]\n                let int_str = String(trunc(val).cast[DType.int32]())\n                let float_str = String(mod(val, 1))\n                let s = int_str + \".\" + float_str[2:6]\n                if k == 0:\n                    print_no_newline(s)\n                else:\n                    print_no_newline(\"  \", s)\n            print_no_newline(\"]\")\n        if rank > 1:\n            print_no_newline(\"]\")\n        print()\n    if rank == 3:\n        print(\"]\")\n    print(\n        \"Tensor shape:\",\n        t.shape().__str__(),\n        \", Tensor rank:\",\n        rank,\n        \",\",\n        \"DType:\",\n        type.__str__(),\n    )\n    print()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/whats_new_v0.5.ipynb",
        "content": "--- cell type: markdown ---\n# What's new in Mojoüî• SDK v0.5\n--- cell type: markdown ---\n## Keyword parameters\n--- cell type: code ---\nfrom tensor import Tensor\nfrom algorithm import vectorize\n\nstruct SquareMatrix[dtype: DType = DType.float32, dim: Int = 4]():\n  var mat: Tensor[dtype]\n\n  fn __init__(inout self, val: SIMD[dtype,1] = 5):\n    self.mat = Tensor[dtype](self.dim,self.dim)\n    alias simd_width = simdwidthof[dtype]()\n    @parameter\n    fn fill_val[simd_width: Int](idx: Int) -> None:\n        self.mat.simd_store(idx, self.mat.simd_load[simd_width](idx).splat(val))\n    vectorize[simd_width, fill_val](self.mat.num_elements())\n\n  fn __getitem__(self,x:Int,y:Int)->SIMD[dtype,1]:\n    return self.mat[x,y]\n\n  fn print(self):\n    print(self.mat)\n--- cell type: code ---\nSquareMatrix().print()\n--- cell type: code ---\nSquareMatrix(val=12).print()\n--- cell type: code ---\nSquareMatrix[DType.float64](10).print()\n--- cell type: code ---\nSquareMatrix[DType.float64,dim=3](1).print()\n--- cell type: code ---\nSquareMatrix[dtype=DType.float64,dim=3](val=1.5).print()\n--- cell type: markdown ---\nKeyword argument in `__getitem__()`\n--- cell type: code ---\nlet sm = SquareMatrix()\nsm.print()\n\nprint()\nprint('Keyword argument in __getitem__()')\nprint(sm[x=0, y=3])\n--- cell type: markdown ---\n## Automatic parameterization of functions\n--- cell type: markdown ---\n* Parameters are automatically added as input parameters on the function\n* Function argument input parameters can now be referenced within the signature of the function\n--- cell type: code ---\nfrom math import mul\nfn multiply(sm: SquareMatrix, val: SIMD[sm.dtype,1]) -> Tensor[sm.dtype]:\n    alias simd_width: Int = simdwidthof[sm.dtype]()\n    let result_tensor = Tensor[sm.dtype](sm.mat.shape())\n\n    @parameter\n    fn vectorize_multiply[simd_width: Int](idx: Int) -> None:\n        result_tensor.simd_store[simd_width](idx, mul[sm.dtype,simd_width](sm.mat.simd_load[simd_width](idx),val))\n    vectorize[simd_width, vectorize_multiply](sm.mat.num_elements())\n    return result_tensor\n\nfn main():\n    let sm = SquareMatrix(5)\n    let res = multiply(sm,100.0)\n    print(res)\nmain()\n--- cell type: markdown ---\n## Load and save Tensors + String enhancements\n--- cell type: code ---\nfrom tensor import Tensor\nfrom algorithm import vectorize\nfrom time import now\nfrom memory import memcpy\n\nstruct SquareMatrix[dtype: DType = DType.float32, dim: Int = 4]():\n  var mat: Tensor[dtype]\n\n  fn __init__(inout self, val:SIMD[dtype,1] = 5):\n    self.mat = Tensor[dtype](self.dim,self.dim)\n    alias simd_width = simdwidthof[dtype]()\n    @parameter\n    fn fill_val[simd_width: Int](idx: Int) -> None:\n        self.mat.simd_store(idx, self.mat.simd_load[simd_width](idx).splat(val))\n    vectorize[simd_width, fill_val](self.mat.num_elements())\n\n  fn print(self):\n    print(self.mat)\n\n  fn prepare_filename(self, fname: String)->String:\n    var fpath = fname\n    if fpath.count('.') < 2:\n        fpath += '.data'\n    fpath = fpath.replace(\".\",\"_\"+self.mat.spec().__str__()+\".\")\n    if fpath.find('/'):\n        fpath = './'+fpath\n    return fpath\n\n  fn save(self, fname: String='saved_matrix') raises -> String:\n    let fpath = self.prepare_filename(fname)\n    self.mat.tofile(fpath)\n    print('File saved:',fpath)\n    return fpath\n\n  @staticmethod\n  fn load[dtype: DType,dim: Int](fpath:String) raises -> Tensor[dtype]:\n    let load_mat = Tensor[dtype].fromfile(fpath)\n    let new_tensor = Tensor[dtype](dim,dim)\n    memcpy(new_tensor.data(),load_mat.data(),load_mat.num_elements())\n    _ = load_mat\n    return new_tensor\n    \n--- cell type: code ---\nlet m = SquareMatrix()\nm.print()\nlet fpath = m.save('saved_matrix')\n--- cell type: code ---\nprint('Loading Tensor from file:',fpath)\nprint()\nlet load_mat = SquareMatrix.load[DType.float32,4](fpath)\nprint(load_mat)\n--- cell type: markdown ---\n## Benchmark enhancements\n--- cell type: markdown ---\nBenchmark row-wise `mean()` of a matrix by vectorizing across colums and parallelizing across rows\n--- cell type: code ---\nfrom random import rand\nlet tx = rand[DType.float32](5,7)\nprint(tx)\n--- cell type: code ---\nfrom tensor import Tensor\nfrom random import rand\nimport benchmark\nfrom time import sleep\nfrom algorithm import vectorize, parallelize\n\nalias dtype = DType.float32\nalias simd_width = simdwidthof[DType.float32]()\n\nfn row_mean_naive[dtype: DType](t: Tensor[dtype]) -> Tensor[dtype]:\n    var res = Tensor[dtype](t.dim(0),1)\n    for i in range(t.dim(0)):\n        for j in range(t.dim(1)):\n            res[i] += t[i,j]\n        res[i] /= t.dim(1)\n    return res\n\nfn row_mean_fast[dtype: DType](t: Tensor[dtype]) -> Tensor[dtype]:\n    var res = Tensor[dtype](t.dim(0),1)\n    @parameter\n    fn parallel_reduce_rows(idx1: Int)->None:\n        @parameter\n        fn vectorize_reduce_row[simd_width: Int](idx2: Int) -> None:\n            res[idx1] += t.simd_load[simd_width](idx1*t.dim(1)+idx2).reduce_add()\n        vectorize[2*simd_width,vectorize_reduce_row](t.dim(1))\n        res[idx1] /= t.dim(1)\n    parallelize[parallel_reduce_rows](t.dim(0),t.dim(0))\n    return res\n\nfn main():\n    let t = rand[dtype](1000,100000)\n    var result = Tensor[dtype](t.dim(0),1)\n\n    @parameter\n    fn bench_mean():\n        _ = row_mean_naive(t)\n    \n    @parameter\n    fn bench_mean_fast():\n        _ = row_mean_fast(t)\n\n    let report = benchmark.run[bench_mean]()\n    let report_fast = benchmark.run[bench_mean_fast]()\n    report.print()\n    report_fast.print()\n    print(\"Speed up:\",report.mean()/report_fast.mean())\n\nmain()\n--- cell type: markdown ---\n## SIMD enhancements\n--- cell type: code ---\ndef main():\n    alias dtype = DType.float32\n    alias simd_width = simdwidthof[DType.float32]()\n\n    let a = SIMD[dtype].splat(0.5)\n    let b = SIMD[dtype].splat(2.5) \n\n    print(\"SIMD a:\",a)\n    print(\"SIMD b:\",b)\n    print()\n    print(\"SIMD a.join(b):\",a.join(b))\nmain()"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/check_mod.py",
        "content": "import shutil\nimport subprocess\nfrom importlib.util import find_spec\n\nFIX = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which(\"python3\"):\n            python = \"python3\"\n        elif shutil.which(\"python\"):\n            python = \"python\"\n        else:\n            raise ImportError(\"python not on path\" + FIX)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n        return\n    except:\n        raise ImportError(f\"{name} not found\" + FIX)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/deviceinfo.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample prints the current host system information using APIs from the\n# sys module.\n\nfrom runtime.llcl import num_cores\nfrom sys.info import (\n    os_is_linux,\n    os_is_windows,\n    os_is_macos,\n    has_sse4,\n    has_avx,\n    has_avx2,\n    has_avx512f,\n    has_vnni,\n    has_neon,\n    is_apple_m1,\n    has_intel_amx,\n    _current_target,\n    _current_cpu,\n    _triple_attr,\n)\n\n\ndef main():\n    var os = \"\"\n    if os_is_linux():\n        os = \"linux\"\n    elif os_is_macos():\n        os = \"macOS\"\n    else:\n        os = \"windows\"\n    let cpu = String(_current_cpu())\n    let arch = String(_triple_attr())\n    var cpu_features = String(\"\")\n    if has_sse4():\n        cpu_features += \" sse4\"\n    if has_avx():\n        cpu_features += \" avx\"\n    if has_avx2():\n        cpu_features += \" avx2\"\n    if has_avx512f():\n        cpu_features += \" avx512f\"\n    if has_vnni():\n        if has_avx512f():\n            cpu_features += \" avx512_vnni\"\n        else:\n            cpu_features += \" avx_vnni\"\n    if has_intel_amx():\n        cpu_features += \" intel_amx\"\n    if has_neon():\n        cpu_features += \" neon\"\n    if is_apple_m1():\n        cpu_features += \" Apple M1\"\n\n    print(\"System information: \")\n    print(\"    OS          : \", os)\n    print(\"    CPU         : \", cpu)\n    print(\"    Arch        : \", arch)\n    print(\"    Num Cores   : \", num_cores())\n    print(\"    CPU Features:\", cpu_features)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/hello.üî•",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n# RUN: %mojo -debug-level full %s | FileCheck %s\n\n# This sample demonstrates some basic Mojo\n# Range and print functions available as builtins\n\n\ndef main():\n    # CHECK: Hello Mojo üî•!\n    print(\"Hello Mojo üî•!\")\n    for x in range(9, 0, -3):\n        print(x)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/hello_interop.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample demonstrates some basic Mojo\n# range() and print() functions available in the standard library.\n# It also demonstrates Python interop by importing the simple_interop.py file.\n\nfrom python.python import Python\n\n\ndef main():\n    print(\"Hello Mojo üî•!\")\n    for x in range(9, 0, -3):\n        print(x)\n    Python.add_to_path(\".\")\n    Python.add_to_path(\"./examples\")\n    let test_module = Python.import_module(\"simple_interop\")\n    test_module.test_interop_func()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/mandelbrot.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nimport benchmark\nfrom complex import ComplexSIMD, ComplexFloat64\nfrom math import iota\nfrom python import Python\nfrom runtime.llcl import num_cores\nfrom algorithm import parallelize, vectorize\nfrom tensor import Tensor\nfrom utils.index import Index\nfrom python import Python\n\nalias float_type = DType.float64\nalias simd_width = 2 * simdwidthof[float_type]()\n\nalias width = 960\nalias height = 960\nalias MAX_ITERS = 200\n\nalias min_x = -2.0\nalias max_x = 0.6\nalias min_y = -1.5\nalias max_y = 1.5\n\n\nfn mandelbrot_kernel_SIMD[\n    simd_width: Int\n](c: ComplexSIMD[float_type, simd_width]) -> SIMD[float_type, simd_width]:\n    \"\"\"A vectorized implementation of the inner mandelbrot computation.\"\"\"\n    let cx = c.re\n    let cy = c.im\n    var x = SIMD[float_type, simd_width](0)\n    var y = SIMD[float_type, simd_width](0)\n    var y2 = SIMD[float_type, simd_width](0)\n    var iters = SIMD[float_type, simd_width](0)\n\n    var t: SIMD[DType.bool, simd_width] = True\n    for i in range(MAX_ITERS):\n        if not t.reduce_or():\n            break\n        y2 = y * y\n        y = x.fma(y + y, cy)\n        t = x.fma(x, y2) <= 4\n        x = x.fma(x, cx - y2)\n        iters = t.select(iters + 1, iters)\n    return iters\n\n\nfn main() raises:\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we operate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](\n                row * width + col, mandelbrot_kernel_SIMD[simd_width](c)\n            )\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n    @parameter\n    fn bench[simd_width: Int]():\n        for row in range(height):\n            worker(row)\n\n    let vectorized = benchmark.run[bench[simd_width]](\n        max_runtime_secs=0.5\n    ).mean()\n    print(\"Number of threads:\", num_cores())\n    print(\"Vectorized:\", vectorized, \"s\")\n\n    # Parallelized\n    @parameter\n    fn bench_parallel[simd_width: Int]():\n        parallelize[worker](height, height)\n\n    let parallelized = benchmark.run[bench_parallel[simd_width]](\n        max_runtime_secs=0.5\n    ).mean()\n    print(\"Parallelized:\", parallelized, \"s\")\n    print(\"Parallel speedup:\", vectorized / parallelized)\n\n    _ = t  # Make sure tensor isn't destroyed before benchmark is finished\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/matmul.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample demonstrates how various systems optimizations can be\n# applied to a naive matmul implementation in Mojo to gain significant\n# performance speedups\n\nfrom benchmark import Unit\nfrom memory import memset_zero, stack_allocation\nfrom random import rand\nfrom algorithm import vectorize, parallelize, vectorize_unroll\nfrom algorithm import Static2DTileUnitFunc as Tile2DFunc\nfrom python import Python\nfrom tensor import Tensor\nfrom utils.index import Index\nfrom memory.buffer import NDBuffer\n\nalias M = 512\nalias N = 512\nalias K = 4096\nalias type = DType.float32\n\n\nstruct Matrix:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    # Initialize zeroeing all values\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        memset_zero(self.data, rows * cols)\n        self.rows = rows\n        self.cols = cols\n\n    # Initialize taking a pointer, don't set any elements\n    fn __init__(\n        inout self, rows: Int, cols: Int, data: DTypePointer[DType.float32]\n    ):\n        self.data = data\n        self.rows = rows\n        self.cols = cols\n\n    ## Initialize with random values\n    @staticmethod\n    fn rand(rows: Int, cols: Int) -> Self:\n        let data = DTypePointer[type].alloc(rows * cols)\n        rand(data, rows * cols)\n        return Self(rows, cols, data)\n\n    fn __getitem__(self, y: Int, x: Int) -> Float32:\n        return self.load[1](y, x)\n\n    fn __setitem__(inout self, y: Int, x: Int, val: Float32):\n        self.store[1](y, x, val)\n\n    fn load[nelts: Int](self, y: Int, x: Int) -> SIMD[DType.float32, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    fn store[nelts: Int](self, y: Int, x: Int, val: SIMD[DType.float32, nelts]):\n        return self.data.simd_store[nelts](y * self.cols + x, val)\n\n\ndef run_matmul_python() -> Float64:\n    Python.add_to_path(\".\")\n    let pymatmul: PythonObject = Python.import_module(\"pymatmul\")\n    let py = Python.import_module(\"builtins\")\n\n    let gflops = pymatmul.benchmark_matmul_python(128, 128, 128).to_float64()\n    py.print(py.str(\"{:<13}{:>8.3f} GFLOPS\").format(\"Python:\", gflops))\n\n    return gflops\n\n\ndef run_matmul_numpy() -> Float64:\n    let pymatmul: PythonObject = Python.import_module(\"pymatmul\")\n    let py = Python.import_module(\"builtins\")\n\n    let gflops = pymatmul.benchmark_matmul_numpy(M, N, K).to_float64()\n    py.print(py.str(\"{:<13}{:>8.3f} GFLOPS\").format(\"Numpy:\", gflops))\n\n    return gflops\n\n\nfn matmul_naive(inout C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n\n\n# Mojo has SIMD vector types, we can vectorize the Matmul code as follows.\nalias nelts = simdwidthof[type]()  # The SIMD vector width.\n\n\n# Using stdlib vectorize function\nfn matmul_vectorized(inout C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n\n            @parameter\n            fn dot[nelts: Int](n: Int):\n                C.store[nelts](\n                    m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n)\n                )\n\n            vectorize[nelts, dot](C.cols)\n\n\n# Parallelize the code by using the builtin parallelize function\nfn matmul_parallelized(inout C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        for k in range(A.cols):\n\n            @parameter\n            fn dot[nelts: Int](n: Int):\n                C.store[nelts](\n                    m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n)\n                )\n\n            vectorize[nelts, dot](C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n\n\n# Perform 2D tiling on the iteration space defined by end_x and end_y.\nfn tile[tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    for y in range(0, end_y, tile_y):\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n\n\n# Use the above tile function to perform tiled matmul.\nfn matmul_tiled(inout C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n\n                @parameter\n                fn dot[\n                    nelts: Int,\n                ](n: Int):\n                    C.store[nelts](\n                        m,\n                        n + x,\n                        C.load[nelts](m, n + x)\n                        + A[m, k] * B.load[nelts](k, n + x),\n                    )\n\n                vectorize[nelts, dot](tile_x)\n\n        # We hardcode the tile factor to be 4.\n        alias tile_size = 4\n        tile[calc_tile, nelts * tile_size, tile_size](C.cols, B.rows)\n\n    parallelize[calc_row](C.rows, C.rows)\n\n\n# Unroll the vectorized loop by a constant factor.\n# from Functional import vectorize_unroll\nfn matmul_unrolled(inout C: Matrix, A: Matrix, B: Matrix):\n    alias tile_size = 4\n\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n\n                @parameter\n                fn dot[\n                    nelts: Int,\n                ](n: Int):\n                    C.store[nelts](\n                        m,\n                        n + x,\n                        C.load[nelts](m, n + x)\n                        + A[m, k] * B.load[nelts](k, n + x),\n                    )\n\n                # Vectorize by nelts and unroll by tile_x/nelts\n                # Here unroll factor is 4\n                vectorize_unroll[nelts, tile_x // nelts, dot](tile_x)\n\n        alias tile_size = 4\n        tile[calc_tile, nelts * tile_size, tile_size](C.cols, B.rows)\n\n    parallelize[calc_row](C.rows, C.rows)\n\n\n# Perform 2D tiling on the iteration space defined by end_x and end_y, parallelizing over y.\nfn tile_parallel[\n    tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int\n](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    @parameter\n    fn row(yo: Int):\n        let y = tile_y * yo\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n\n    parallelize[row](end_y // tile_y, M)\n\n\n# Use stack allocation for tiles to accumulate values efficiently,\n# avoiding repeated reads and writes to memory. Also reorder the loops\n# and do not fully unroll the loop over the reduction dimension.\nfn matmul_accumulated(inout C: Matrix, A: Matrix, B: Matrix):\n    alias tile_k = 8\n    alias tile_k_unroll = 8\n    alias tile_i = 32\n    alias tile_j = nelts * 4\n\n    @parameter\n    fn calc_tile[tile_j: Int, tile_i: Int](jo: Int, io: Int):\n        # Allocate the tile of accumulators on the stack.\n        var accumulators = Matrix(\n            tile_i, tile_j, stack_allocation[tile_i * tile_j, DType.float32]()\n        )\n\n        for ko in range(0, A.cols, tile_k * tile_k_unroll):\n            for _ in range(tile_i):\n                for i in range(tile_k):\n\n                    @unroll\n                    for k in range(tile_k_unroll):\n\n                        @parameter\n                        fn calc_tile_cols[nelts: Int](j: Int):\n                            accumulators.store[nelts](\n                                i,\n                                j,\n                                accumulators.load[nelts](i, j)\n                                + A[io + i, ko + k]\n                                * B.load[nelts](ko + k, jo + j),\n                            )\n\n                        vectorize_unroll[\n                            nelts, tile_j // nelts, calc_tile_cols\n                        ](tile_j)\n\n        # Copy the local tile to the output\n        for i in range(tile_i):\n            for j in range(tile_j):\n                C[io + i, jo + j] = accumulators[i, j]\n\n    tile_parallel[calc_tile, tile_j, tile_i](C.cols, C.rows)\n\n\n@always_inline\nfn bench[\n    func: fn (inout Matrix, Matrix, Matrix) -> None, name: StringLiteral\n](base_gflops: Float64, numpy_gflops: Float64) raises:\n    var A = Matrix.rand(M, K)\n    var B = Matrix.rand(K, N)\n    var C = Matrix(M, N)\n\n    @always_inline\n    @parameter\n    fn test_fn():\n        _ = func(C, A, B)\n\n    let secs = benchmark.run[test_fn](max_runtime_secs=0.5).mean()\n    # Prevent the matrices from being freed before the benchmark run\n    A.data.free()\n    B.data.free()\n    C.data.free()\n    let gflops = ((2 * M * N * K) / secs) / 1e9\n    let speedup: Float64 = gflops / base_gflops\n    let numpy_speedup: Float64 = gflops / numpy_gflops\n\n    let py = Python.import_module(\"builtins\")\n    _ = py.print(\n        py.str(\"{:<13}{:>8.3f} GFLOPS {:>9.2f}x Python {:>5.2f}x Numpy\").format(\n            name, gflops, speedup, numpy_speedup\n        )\n    )\n\n\n@always_inline\nfn test[\n    func: fn (inout Matrix, Matrix, Matrix) -> None\n](A: Matrix, B: Matrix) raises -> SIMD[type, 1]:\n    var C = Matrix(M, N)\n    _ = func(C, A, B)\n    var result = SIMD[type, 1]()\n    for i in range(C.rows):\n        for j in range(C.cols):\n            result += C[i, j]\n    return result\n\n\nfn test_all() raises:\n    constrained[M == N, \"M and N must be equal for matrix multiplication\"]()\n\n    let A = Matrix.rand(M, K)\n    let B = Matrix.rand(K, N)\n\n    let result = test[matmul_naive](A, B)\n\n    if test[matmul_vectorized](A, B) != result:\n        raise Error(\"Vectorize output does not match\")\n    if test[matmul_parallelized](A, B) != result:\n        raise Error(\"Parallelize output incorrect\")\n    if test[matmul_tiled](A, B) != result:\n        raise Error(\"Tiled output incorrect\")\n    if test[matmul_unrolled](A, B) != result:\n        raise Error(\"Unroll output incorrect\")\n    if test[matmul_accumulated](A, B) != result:\n        raise Error(\"Loop reorder output incorrect\")\n\n    A.data.free()\n    B.data.free()\n\n\nfn main() raises:\n    # Uncomment below to test correctness of Matmuls\n    # test_all()\n    print(\"CPU Results\\n\")\n    let python_gflops = run_matmul_python()\n    let numpy_gflops = run_matmul_numpy()\n\n    bench[matmul_naive, \"Naive:\"](python_gflops, numpy_gflops)\n    bench[matmul_vectorized, \"Vectorized: \"](python_gflops, numpy_gflops)\n    bench[matmul_parallelized, \"Parallelized:\"](python_gflops, numpy_gflops)\n    bench[matmul_tiled, \"Tiled:\"](python_gflops, numpy_gflops)\n    bench[matmul_unrolled, \"Unrolled:\"](python_gflops, numpy_gflops)\n    bench[matmul_accumulated, \"Accumulated:\"](python_gflops, numpy_gflops)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/memset.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample implements various memset algorithms and optimizations\n\nfrom autotune import autotune_fork\nfrom math import min, max\nfrom time import time_function\nfrom memory import memset as stdlib_memset\nfrom benchmark import keep\n\nalias type = UInt8\nalias ptr_type = DTypePointer[DType.uint8]\nalias fn_type = fn (ptr_type, type, Int) -> None\n\n\nfn measure_time(func: fn_type, size: Int, iters: Int, samples: Int) -> Int:\n    alias alloc_size = 1024 * 1024\n    let ptr = ptr_type.alloc(alloc_size)\n\n    var best = -1\n    for sample in range(samples):\n\n        @parameter\n        fn runner():\n            for iter in range(iters):\n                # Offset pointer to shake up cache a bit\n                let offset_ptr = ptr.offset((iter * 128) & 1024)\n\n                # memset, change the value we're filling with\n                let v = type(iter&255)\n\n                # Actually call the memset function\n                func(offset_ptr, v.value, size)\n\n                # Avoid compiler optimizing things away\n                keep(v)\n                keep(size)\n                keep(offset_ptr)\n\n        let ns = time_function[runner]()\n        if best < 0 or ns < best:\n            best = ns\n\n    ptr.free()\n    return best\n\n\nalias MULT = 2_000\n\n\nfn visualize_result(size: Int, result: Int):\n    print_no_newline(\"Size: \")\n    if size < 10:\n        print_no_newline(\" \")\n    print_no_newline(size, \"  |\")\n    for _ in range(result // MULT):\n        print_no_newline(\"*\")\n    print()\n\n\nfn benchmark(func: fn_type, title: StringRef):\n    print(\"\\n=====================\")\n    print(title)\n    print(\"---------------------\\n\")\n\n    alias benchmark_iterations = 30 * MULT\n    alias warmup_samples = 10\n    alias benchmark_samples = 1000\n\n    # Warmup\n    for size in range(35):\n        _ = measure_time(func, size, benchmark_iterations, warmup_samples)\n\n    # Actual run\n    for size in range(35):\n        let result = measure_time(\n            func, size, benchmark_iterations, benchmark_samples\n        )\n\n        visualize_result(size, result)\n\n\n@always_inline\nfn overlapped_store[width: Int](ptr: ptr_type, value: type, count: Int):\n    let v = SIMD[DType.uint8, width].splat(value)\n    ptr.simd_store[width](v)\n    ptr.simd_store[width](count - width, v)\n\n\nfn memset_manual(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count <= 16:\n            if count >= 8:\n                # 8 <= count < 16\n                overlapped_store[8](ptr, value, count)\n                return\n            # 4 < count < 8\n            overlapped_store[4](ptr, value, count)\n            return\n\n        # 16 <= count < 32\n        overlapped_store[16](ptr, value, count)\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n\n\nfn memset_system(ptr: ptr_type, value: type, count: Int):\n    stdlib_memset(ptr, value.value, count)\n\n\nfn memset_manual_2(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count >= 16:\n            # 16 <= count < 32\n            overlapped_store[16](ptr, value, count)\n            return\n\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count >= 8:\n            # 8 <= count < 16\n            overlapped_store[8](ptr, value, count)\n            return\n        # 4 < count < 8\n        overlapped_store[4](ptr, value, count)\n\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n\n\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    @parameter\n    if lower == -100 and upper == 0:\n        pass\n    elif lower == 0 and upper == 4:\n        ptr.store(0, value)\n        ptr.store(count - 1, value)\n        if count <= 2:\n            return\n        ptr.store(1, value)\n        ptr.store(count - 2, value)\n    elif lower == 4 and upper == 8:\n        overlapped_store[4](ptr, value, count)\n    elif lower == 8 and upper == 16:\n        overlapped_store[8](ptr, value, count)\n    elif lower == 16 and upper == 32:\n        overlapped_store[16](ptr, value, count)\n    elif lower == 32 and upper == 100:\n        memset_system(ptr, value, count)\n    else:\n        constrained[False]()\n\n\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count > cur:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n    else:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n\n\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count <= cur:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n    else:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n\n\nfn memset_evaluator(funcs: Pointer[fn_type], size: Int) -> Int:\n    # This size is picked at random, in real code we could use a real size\n    # distribution here.\n    let size_to_optimize_for = 17\n    print(\"Optimizing for size: \", size_to_optimize_for)\n\n    var best_idx: Int = -1\n    var best_time: Int = -1\n\n    alias eval_iterations = MULT\n    alias eval_samples = 500\n\n    # Find the function that's the fastest on the size we're optimizing for\n    for f_idx in range(size):\n        let func = funcs.load(f_idx)\n        let cur_time = measure_time(\n            func, size_to_optimize_for, eval_iterations, eval_samples\n        )\n        if best_idx < 0:\n            best_idx = f_idx\n            best_time = cur_time\n        if best_time > cur_time:\n            best_idx = f_idx\n            best_time = cur_time\n\n    return best_idx\n\n\nfn main():\n    benchmark(memset_manual, \"Manual memset\")\n    benchmark(memset_system, \"System memset\")\n    benchmark(memset_manual_2, \"Manual memset v2\")\n    benchmark(memset_system, \"Mojo system memset\")\n    benchmark(memset_manual, \"Mojo manual memset\")\n    benchmark(memset_manual_2, \"Mojo manual memset v2\")\n    benchmark(memset_system, \"Mojo system memset\")\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/nbody.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample implements the nbody benchmarking in\n# https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/nbody.html\n\nfrom utils.index import StaticTuple\nfrom math import sqrt\nfrom benchmark import run\n\nalias PI = 3.141592653589793\nalias SOLAR_MASS = 4 * PI * PI\nalias DAYS_PER_YEAR = 365.24\n\n\n@register_passable(\"trivial\")\nstruct Planet:\n    var pos: SIMD[DType.float64, 4]\n    var velocity: SIMD[DType.float64, 4]\n    var mass: Float64\n\n    fn __init__(\n        pos: SIMD[DType.float64, 4],\n        velocity: SIMD[DType.float64, 4],\n        mass: Float64,\n    ) -> Self:\n        return Self {\n            pos: pos,\n            velocity: velocity,\n            mass: mass,\n        }\n\n\nalias NUM_BODIES = 5\n\n\nfn offset_momentum(inout bodies: StaticTuple[NUM_BODIES, Planet]):\n    var p = SIMD[DType.float64, 4]()\n\n    @unroll\n    for i in range(NUM_BODIES):\n        p += bodies[i].velocity * bodies[i].mass\n\n    var body = bodies[0]\n    body.velocity = -p / SOLAR_MASS\n\n    bodies[0] = body\n\n\nfn advance(inout bodies: StaticTuple[NUM_BODIES, Planet], dt: Float64):\n    @unroll\n    for i in range(NUM_BODIES):\n        for j in range(NUM_BODIES - i - 1):\n            var body_i = bodies[i]\n            var body_j = bodies[j + i + 1]\n            let diff = body_i.pos - body_j.pos\n            let diff_sqr = (diff * diff).reduce_add()\n            let mag = dt / (diff_sqr * sqrt(diff_sqr))\n\n            body_i.velocity -= diff * body_j.mass * mag\n            body_j.velocity += diff * body_i.mass * mag\n\n            bodies[i] = body_i\n            bodies[j + i + 1] = body_j\n\n    @unroll\n    for i in range(NUM_BODIES):\n        var body = bodies[i]\n        body.pos += dt * body.velocity\n        bodies[i] = body\n\n\nfn energy(bodies: StaticTuple[NUM_BODIES, Planet]) -> Float64:\n    var e: Float64 = 0\n\n    @unroll\n    for i in range(NUM_BODIES):\n        let body_i = bodies[i]\n        e += (\n            0.5\n            * body_i.mass\n            * ((body_i.velocity * body_i.velocity).reduce_add())\n        )\n\n        for j in range(NUM_BODIES - i - 1):\n            let body_j = bodies[j + i + 1]\n            let diff = body_i.pos - body_j.pos\n            let distance = sqrt((diff * diff).reduce_add())\n            e -= (body_i.mass * body_j.mass) / distance\n\n    return e\n\n\nfn bench():\n    let Sun = Planet(\n        0,\n        0,\n        SOLAR_MASS,\n    )\n\n    let Jupiter = Planet(\n        SIMD[DType.float64, 4](\n            4.84143144246472090e00,\n            -1.16032004402742839e00,\n            -1.03622044471123109e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            1.66007664274403694e-03 * DAYS_PER_YEAR,\n            7.69901118419740425e-03 * DAYS_PER_YEAR,\n            -6.90460016972063023e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        9.54791938424326609e-04 * SOLAR_MASS,\n    )\n\n    let Saturn = Planet(\n        SIMD[DType.float64, 4](\n            8.34336671824457987e00,\n            4.12479856412430479e00,\n            -4.03523417114321381e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            -2.76742510726862411e-03 * DAYS_PER_YEAR,\n            4.99852801234917238e-03 * DAYS_PER_YEAR,\n            2.30417297573763929e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        2.85885980666130812e-04 * SOLAR_MASS,\n    )\n\n    let Uranus = Planet(\n        SIMD[DType.float64, 4](\n            1.28943695621391310e01,\n            -1.51111514016986312e01,\n            -2.23307578892655734e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            2.96460137564761618e-03 * DAYS_PER_YEAR,\n            2.37847173959480950e-03 * DAYS_PER_YEAR,\n            -2.96589568540237556e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        4.36624404335156298e-05 * SOLAR_MASS,\n    )\n\n    let Neptune = Planet(\n        SIMD[DType.float64, 4](\n            1.53796971148509165e01,\n            -2.59193146099879641e01,\n            1.79258772950371181e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            2.68067772490389322e-03 * DAYS_PER_YEAR,\n            1.62824170038242295e-03 * DAYS_PER_YEAR,\n            -9.51592254519715870e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        5.15138902046611451e-05 * SOLAR_MASS,\n    )\n    var system = StaticTuple[NUM_BODIES, Planet](\n        Sun, Jupiter, Saturn, Uranus, Neptune\n    )\n    offset_momentum(system)\n\n    print(\"Energy of System:\", energy(system))\n\n    for i in range(50_000_000):\n        advance(system, 0.01)\n\n    print(\"Energy of System:\", energy(system))\n\n\nfn benchmark():\n    print(run[bench](max_runtime_secs=0.5).mean())\n\n\nfn main():\n    print(\"Starting nbody...\")\n    bench()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/BoolMLIR.ipynb",
        "content": "--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n# Low-level IR in Mojo\n--- cell type: markdown ---\nMojo is a high-level programming language with an extensive set of modern features. Mojo also provides you, the programmer, access to all of the low-level primitives that you need to write powerful -- yet zero-cost -- abstractions.\n\nThese primitives are implemented in [MLIR](https://mlir.llvm.org), an extensible intermediate representation (IR) format for compiler design. Many different programming languages and compilers translate their source programs into MLIR, and because Mojo provides direct access to MLIR features, this means Mojo programs can enjoy the benefits of each of these tools.\n\nGoing one step further, Mojo's unique combination of zero-cost abstractions with MLIR interoperability means that Mojo programs can take full advantage of *anything* that interfaces with MLIR. While this isn't something normal Mojo programmers may ever need to do, it's an extremely powerful capability when extending a system to interface with a new datatype, or an esoteric new accelerator feature.\n\nTo illustrate these ideas, we'll implement a boolean type in Mojo below, which we'll call `OurBool`. We'll make extensive use of MLIR, so let's begin with a short primer.\n\n--- cell type: markdown ---\n## What is MLIR?\n\nMLIR is an intermediate representation of a program, not unlike an assembly language, in which a sequential set of instructions operate on in-memory values.\n\nMore importantly, MLIR is modular and extensible. MLIR is composed of an ever-growing number of \"dialects.\" Each dialect defines operations and optimizations: for example, the ['math' dialect](https://mlir.llvm.org/docs/Dialects/MathOps/) provides mathematical operations such as sine and cosine, the ['amdgpu' dialect](https://mlir.llvm.org/docs/Dialects/AMDGPU/) provides operations specific to AMD processors, and so on.\n\nEach of MLIR's dialects can interoperate with the others. This is why MLIR¬†is said to unlock heterogeneous compute: as newer, faster processors and architectures are developed, new MLIR dialects are implemented to generate optimal code for those environments. Any new MLIR dialect can be translated seamlessly into other dialects, so as more get added, all existing MLIR becomes more powerful.\n\nThis means that our own custom types, such as the `OurBool` type we'll create below, can be used to provide programmers with a high-level, Python-like interface. But \"under the covers,\" Mojo and MLIR will optimize our convenient, high-level types for each new processor that appears in the future.\n\nThere's much more to write about why MLIR is such a revolutionary technology, but let's get back to Mojo and defining the `OurBool` type. There will be opportunities to learn more about MLIR along the way.\n--- cell type: markdown ---\n## Defining the `OurBool` type\n\nWe can use Mojo's `struct` keyword to define a new type `OurBool`:\n--- cell type: code ---\nstruct OurBool:\n    var value: __mlir_type.i1\n--- cell type: markdown ---\nA boolean can represent 0 or 1, \"true\" or \"false.\" To store this information, `OurBool` has a single member, called `value`. Its type is represented *directly in MLIR*, using the MLIR builtin type [`i1`](https://mlir.llvm.org/docs/Dialects/Builtin/#integertype). In fact, you can use any MLIR type in Mojo, by prefixing the type name with `__mlir_type`.\n\nAs we'll see below, representing our boolean value with `i1` will allow us to utilize all of the MLIR operations and optimizations that interface with the `i1` type -- and there are many of them!\n\nHaving defined `OurBool`, we can now declare a variable of this type:\n--- cell type: code ---\nlet a: OurBool\n--- cell type: markdown ---\n## Leveraging MLIR\n\nNaturally, we might next try to create an instance of `OurBool`. Attempting to do so at this point, however, results in an error:\n\n```mojo\nlet a = OurBool() # error: 'OurBool' does not implement an '__init__' method\n```\n\nAs in Python, `__init__` is a [special method](https://docs.python.org/3/reference/datamodel.html#specialnames) that can be defined to customize the behavior of a type. We can implement an `__init__` method that takes no arguments, and returns an `OurBool` with a \"false\" value.\n--- cell type: code ---\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(inout self):\n        self.value = __mlir_op.`index.bool.constant`[\n            value=__mlir_attr.`false`,\n        ]()\n--- cell type: markdown ---\nTo initialize the underlying `i1` value, we use an MLIR operation from its ['index' dialect](https://mlir.llvm.org/docs/Dialects/IndexOps/), called [`index.bool.constant`](https://mlir.llvm.org/docs/Dialects/IndexOps/#indexboolconstant-mlirindexboolconstantop).\n\nMLIR's 'index' dialect provides us with operations for manipulating builtin MLIR types, such as the `i1` we use to store the value of `OurBool`. The `index.bool.constant` operation takes a `true` or `false` compile-time constant as input, and produces a runtime output of type `i1` with the given value.\n\nSo, as shown above, in addition to any MLIR type, Mojo also provides direct access to any MLIR operation via the `__mlir_op` prefix, and to any attribute via the `__mlir_attr` prefix. MLIR attributes are used to represent compile-time constants.\n\nAs you can see above, the syntax for interacting with MLIR isn't always pretty: MLIR attributes are passed in between square brackets `[...]`, and the operation is executed via a parentheses suffix `(...)`, which can take runtime argument values. However, most Mojo programmers will not need to access MLIR directly, and for the few that do, this \"ugly\" syntax gives them superpowers: they can define high-level types that are easy to use, but that internally plug into MLIR and its powerful system of dialects.\n\nWe think this is very exciting, but let's bring things back down to earth: having defined an `__init__` method, we can now create an instance of our `OurBool` type:\n--- cell type: code ---\nlet b = OurBool()\n--- cell type: markdown ---\n## Value semantics in Mojo\n\nWe can now instantiate `OurBool`, but using it is another story:\n\n```mojo\nlet a = OurBool()\nlet b = a # error: 'OurBool' does not implement the '__copyinit__' method\n```\n\nMojo uses \"value semantics\" by default, meaning that it expects to create a copy of `a` when assigning to `b`. However, Mojo doesn't make any assumptions about *how* to copy `OurBool`, or its underlying `i1` value. The error indicates that we should implement a `__copyinit__` method, which would implement the copying logic.\n\nIn our case, however, `OurBool` is a very simple type, with only one \"trivially copyable\" member. We can use a decorator to tell the Mojo compiler that, saving us the trouble of defining our own `__copyinit__` boilerplate. Trivially copyable types must implement an `__init__` method that returns an instance of themselves, so we must also rewrite our initializer slightly.\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__() -> Self:\n        return Self {\n            value: __mlir_op.`index.bool.constant`[\n                value=__mlir_attr.`false`,\n            ]()\n        }\n--- cell type: markdown ---\nWe can now copy `OurBool` as we please:\n--- cell type: code ---\nlet c = OurBool()\nlet d = c\n--- cell type: markdown ---\n## Compile-time constants\n\nIt's not very useful to have a boolean type that can only represent \"false.\" Let's define compile-time constants that represent true and false `OurBool` values.\n\nFirst, let's define another `__init__` constructor for `OurBool` that takes its `i1` value as an argument:\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    # ...\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n--- cell type: markdown ---\nThis allows us to define compile-time constant `OurBool` values, using the `alias` keyword. First, let's define `OurTrue`:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\n--- cell type: markdown ---\nHere we're passing in an MLIR compile-time constant value of `true`, which has the `i1` type that our new `__init__` constructor expects. We can use a slightly different syntax for `OurFalse`:\n--- cell type: code ---\nalias OurFalse: OurBool = __mlir_attr.`false`\n--- cell type: markdown ---\n`OurFalse` is declared to be of type `OurBool`, and then assigned an `i1` type -- in this case, the `OurBool` constructor we added is called implicitly.\n\nWith true and false constants, we can also simplify our original `__init__` constructor for `OurBool`. Instead of constructing an MLIR value, we can simply return our `OurFalse` constant:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    # We can simplify our no-argument constructor:\n    fn __init__() -> Self:\n        return OurFalse\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n--- cell type: markdown ---\nNote also that we can define `OurTrue` before we define `OurBool`. The Mojo compiler is smart enough to figure this out.\n\nWith these constants, we can now define variables with both true and false values of `OurBool`:\n--- cell type: code ---\nlet e = OurTrue\nlet f = OurFalse\n--- cell type: markdown ---\n## Implementing `__bool__`\n\nOf course, the reason booleans are ubiquitous in programming is because they can be used for program control flow. However, if we attempt to use `OurBool` in this way, we get an error:\n\n```mojo\nlet a = OurTrue\nif a: print(\"It's true!\") # error: 'OurBool' does not implement the '__bool__' method\n```\n\nWhen Mojo attempts to execute our program, it needs to be able to determine whether to print \"It's true!\" or not. It doesn't yet know that `OurBool` represents a boolean value -- Mojo just sees a struct that is 1 bit in size. However, Mojo also provides interfaces that convey boolean¬†qualities, which are the same as those used by Mojo's standard library types, like `Bool`. In practice, this means Mojo gives you full control: any type that's packaged with the language's standard library is one for which you could define your own version.\n\nIn the case of our error message, Mojo is telling us that implementing a `__bool__` method on `OurBool` would signify that it has boolean qualities.\n\nThankfully, `__bool__` is simple to implement: Mojo's standard library and builtin types are all implemented on top of MLIR, and so the builtin `Bool` type also defines a constructor that takes an `i1`, just like `OurBool`:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    # ...\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n\n    # Our new method converts `OurBool` to `Bool`:\n    fn __bool__(self) -> Bool:\n        return Bool(self.value)\n--- cell type: markdown ---\nNow we can use `OurBool` anywhere we can use the builtin `Bool` type:\n--- cell type: code ---\n#| CHECK: It's true!\nlet g = OurTrue\nif g: print(\"It's true!\")\n--- cell type: markdown ---\n## Avoiding type conversion with `__mlir_i1__`\n\nThe `OurBool` type is looking great, and by providing a conversion to `Bool`,\nit can be used anywhere the builtin `Bool` type can. But we promised you \"full\ncontrol,\" and the ability to define your own version of any type built into\nMojo or its standard library. So, why do we depend on `__bool__` to convert our\ntype into `Bool` (the standard library type)? This is just the formal way for\nMojo to evaluate a type as a boolean, which is useful for real-world scenarios.\nHowever, to define a boolean type from scratch, you have a more low-level\noption.\n\nWhen Mojo evaluates a conditional expression, it actually attempts to convert\nthe expression to an MLIR `i1` value, by searching for the special interface\nmethod `__mlir_i1__`. (The automatic conversion to `Bool` occurs because `Bool`\nis known to implement the `__mlir_i1__` method.)\n\nThus, by implementing the `__mlir_i1__` special methods in `OurBool`, we can\ncreate a type that can replaces `Bool` entirely:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n\n    # Our new method converts `OurBool` to `i1`:\n    fn __mlir_i1__(self) -> __mlir_type.i1:\n        return self.value\n--- cell type: markdown ---\nWe can still use `OurBool` in conditionals just as we did before:\n--- cell type: code ---\n#| CHECK: No more Bool conversion!\nlet h = OurTrue\nif h: print(\"No more Bool conversion!\")\n--- cell type: markdown ---\nBut this time, no conversion to `Bool` occurs. You can try adding `print` statements to the `__bool__` and `__mlir_i1__` methods to see for yourself.\n--- cell type: markdown ---\n## Adding functionality with MLIR\n\nThere are many more ways we can improve `OurBool`. Many of those involve implementing special methods, some of which you may recognize from Python, and some which are specific to Mojo. For example, we can implement inversion of a `OurBool` value by adding a `__invert__` method. We can also add an `__eq__` method, which allows two `OurBool` to be compared with the `==` operator.\n\nWhat sets Mojo apart is the fact that we can implement each of these using MLIR. To implement `__eq__`, for example, we use the [`index.casts`](https://mlir.llvm.org/docs/Dialects/IndexOps/#indexcasts-mlirindexcastsop) operation to cast our `i1` values to the MLIR index dialect's `index` type, and then the [`index.cmp`](https://mlir.llvm.org/docs/Dialects/IndexOps/#indexcmp-mlirindexcmpop) operation to compare them for equality. And with the `__eq__` method implemented, we can then implement `__invert__` in terms of `__eq__`:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n\n    # ...\n\n    fn __mlir_i1__(self) -> __mlir_type.i1:\n        return self.value\n\n    fn __eq__(self, rhs: OurBool) -> Self:\n        let lhsIndex = __mlir_op.`index.casts`[_type=__mlir_type.index](\n            self.value\n        )\n        let rhsIndex = __mlir_op.`index.casts`[_type=__mlir_type.index](\n            rhs.value\n        )\n        return Self(\n            __mlir_op.`index.cmp`[\n                pred=__mlir_attr.`#index<cmp_predicate eq>`\n            ](lhsIndex, rhsIndex)\n        )\n\n    fn __invert__(self) -> Self:\n        return OurFalse if self == OurTrue else OurTrue\n--- cell type: markdown ---\nThis allows us to use the `~` operator with `OurBool`:\n--- cell type: code ---\n#| CHECK: It's false!\nlet i = OurFalse\nif ~i: print(\"It's false!\")\n--- cell type: markdown ---\nThis extensible design is what allows even \"built in\" Mojo types like `Bool`, `Int`, and even `Tuple` to be implemented in the Mojo standard library in terms of MLIR, rather than hard-coded into the Mojo language. This also means that there's almost nothing that those types can achieve that user-defined types cannot.\n\nBy extension, this means that the incredible performance that Mojo unlocks for machine learning workflows isn't due to some magic being performed behind a curtain -- you can define your own high-level types that, in their implementation, use low-level MLIR to achieve unprecedented speed and control.\n--- cell type: markdown ---\n## The promise of modularity\n\nAs we've seen, Mojo's integration with MLIR allows Mojo programmers to implement zero-cost abstractons on par with Mojo's own builtin and standard library types.\n\nMLIR is open-source and extensible: new dialects are being added all the time, and those dialects then become available to use in Mojo. All the while, Mojo code gets more powerful and more optimized for new hardware -- with no additional work necessary by Mojo programmers.\n\nWhat this means is that your own custom types, whether those be `OurBool` or `OurTensor`, can be used to provide programmers with an easy-to-use and unchanging interface. But behind the scenes, MLIR will optimize those convenient, high-level types for the computing environments of tomorrow.\n\nIn other words: Mojo isn't magic, it's modular."
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/HelloMojo.ipynb",
        "content": "--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n# Mojo language basics\n--- cell type: markdown ---\nMojo is a powerful programming language that's primarily designed for\nhigh-performance systems programming, so it has a lot in common with other\nsystems languages like Rust and C++. Yet, Mojo is also designed to become a\nsuperset of Python, so a lot of language features and concepts you might know\nfrom Python translate nicely to Mojo. \n\nFor example, if you're in a REPL environment or Jupyter notebook (like this\ndocument), you can run top-level code just like Python:\n--- cell type: code ---\n#| CHECK: Hello Mojo!\nprint(\"Hello Mojo!\")\n--- cell type: markdown ---\nYou don't normally see that with other systems programming languages.\n\nMojo preserves Python's dynamic features and language syntax, and it even\nallows you to import and run code from Python packages. However, it's important\nto know that Mojo is an entirely new language, not just a new implementation of\nPython with syntax sugar. Mojo takes the Python language to a whole new level,\nwith systems programming features, strong type-checking, memory safety,\nnext-generation compiler technologies, and more. Yet, it's still designed to be\na simple language that's useful for general-purpose programming.\n\nThis page provides a gentle introduction to the Mojo language, and requires\nonly a little programming experience. So let's get started!\n\nFor more details about everything covered here, check out the\n[Mojo Manual](https://docs.modular.com/mojo/manual/).\n--- cell type: markdown ---\n## Language basics\n\nFirst and foremost, Mojo is a compiled language and a lot of its performance\nand memory-safety features are derived from that fact. Mojo code can be\nahead-of-time (AOT) or just-in-time (JIT) compiled.\n\nLike other compiled languages, Mojo programs (`.mojo` or `.üî•` files) require a\n`main()` function as the entry point to the program. For example:\n--- cell type: code ---\nfn main():\n    var x: Int = 1\n    x += 1\n    print(x)\n--- cell type: markdown ---\nIf you know Python, you might have expected the function name to be `def\nmain()` instead of `fn main()`. Both actually work in Mojo, but using `fn`\nbehaves a bit differently, as we'll discuss below.\n\nOf course, if you're building a Mojo module (an API library), not a Mojo\nprogram, then your file doesn't need a `main()` function (because it will be\nimported by other programs that do have one).\n\n<div class=\"alert alert-block alert-info alert--secondary\">\n\n**Note:** When you're writing code in a `.mojo`/`.üî•` file, you can't run\ntop-level code as shown on this page‚Äîall code in a Mojo program or module\nmust be encased in a function or struct. However, top-level code does work in a\nREPL or Jupyter notebook (such as the [notebook for this\npage](https://github.com/modularml/mojo/blob/main/examples/notebooks/HelloMojo.ipynb)).\n\n</div>\n\nNow let's explain the code in this `main()` function.\n--- cell type: markdown ---\n### Syntax and semantics\n\nThis is simple: Mojo supports (or will support) all of Python's syntax and\nsemantics. If you're not familiar with Python syntax, there are a ton of great\nresources online that can teach you.\n\nFor example, like Python, Mojo uses line breaks and indentation to define code\nblocks (not curly braces), and Mojo supports all of Python's control-flow syntax\nsuch as `if` conditions and `for` loops.\n\nHowever, Mojo is still a work in progress, so there are some things from Python\nthat aren't implemented in Mojo yet (see the [Mojo\nroadmap](https://docs.modular.com/mojo/roadmap.html)). All the missing Python\nfeatures will arrive in time, but Mojo already includes many features and\ncapabilities beyond what's available in Python.\n\nAs such, the following sections will focus on some of the language features that\nare unique to Mojo (compared to Python).\n--- cell type: markdown ---\n\n### Functions\n\nMojo functions can be declared with either `fn` (shown above) or `def` (as\nin Python). The `fn` declaration enforces strongly-typed and memory-safe\nbehaviors, while `def` provides Python-style dynamic behaviors.\n\nBoth `fn` and `def` functions have their value, and it's important that you\nlearn them both. However, for the purposes of this introduction, we're going to\nfocus on `fn` functions only. For more detail about both, see the [functions\npage in the manual](https://docs.modular.com/mojo/manual/functions.html).\n\nIn the following sections, you'll learn how `fn` functions enforce\nstrongly-typed and memory-safe behaviors in your code.\n--- cell type: markdown ---\n### Variables\n\nYou can declare variables (such as `x` in the above `main()` function) with\n`var` to create a mutable value, or with `let` to create an immutable value.\n\nIf you change `var` to `let` in the `main()` function above and run it, you'll\nget a compiler error like this:\n\n```text\nerror: Expression [15]:7:5: expression must be mutable for in-place operator destination\n    x += 1\n    ^\n```\n\nThat's because `let` makes the value immutable, so you can't increment it.\n\nAnd if you delete `var` completely, you'll get an error because `fn` functions\nrequire explicit variable declarations (unlike Python-style `def` functions).\n\nFinally, notice that the `x` variable has an explicit `Int` type specification.\nDeclaring the type is not required for variables in `fn`, but it is desirable\nsometimes. If you omit it, Mojo infers the type, as shown here:\n--- cell type: code ---\nfn do_math():\n    let x: Int = 1\n    let y = 2\n    print(x + y)\n\ndo_math()\n--- cell type: markdown ---\n### Function arguments and returns\n\nAlthough types aren't required for variables declared in the function body,\nthey are required for arguments and return values for an `fn` function.\n\nFor example, here's how to declare `Int` as the type for function arguments and\nthe return value:\n--- cell type: code ---\nfn add(x: Int, y: Int) -> Int:\n    return x + y\n\nz = add(1, 2)\nprint(z)\n--- cell type: markdown ---\n#### Optional arguments and keyword arguments\n\nYou can also specify argument default values (also known as optional\narguments), and pass values with keyword argument names. For example:\n--- cell type: code ---\nfn pow(base: Int, exp: Int = 2) -> Int:\n    return base ** exp\n\n# Uses default value for `exp`\nz = pow(3)\nprint(z)\n\n# Uses keyword argument names (with order reversed)\nz = pow(exp=3, base=2)\nprint(z)\n--- cell type: markdown ---\n\n<div class=\"alert alert-block alert-info alert--secondary\">\n\n**Note:** Mojo currently includes only partial support for keyword arguments, so\nsome features such as keyword-only arguments and variadic keyword arguments (e.g. `**kwargs`)\nare not supported yet.\n\n</div>\n--- cell type: markdown ---\n#### Argument mutability and ownership\n\nMojo supports full [value\nsemantics](https://en.wikipedia.org/wiki/Value_semantics) and enforces memory\nsafety with a robust value ownership model (similar to the Rust borrow\nchecker). Essentially, that means Mojo allows you to share references to values\n(instead of making a copy every time you pass a value to a function), but doing\nso requires that you follow Mojo's ownership rules (to ensure memory safety) as\ndescribed in this section.\n\nNotice that, above, `add()` doesn't modify `x` or `y`, it only reads the\nvalues. In fact, as written, the function *cannot* modify them because `fn`\narguments are **immutable references** by default. This ensures memory safety\n(no surprise changes to the data) while also avoiding a copy (which could be\na performance hit).\n\nIn terms of argument conventions, this is called \"borrowing,\" and although it's\nthe default for `fn` functions, you can make it explicit with the `borrowed`\ndeclaration like this (this behaves exactly the same as the `add()` above):\n--- cell type: code ---\nfn add(borrowed x: Int, borrowed y: Int) -> Int:\n    return x + y\n--- cell type: markdown ---\nIf you want the arguments to be mutable, you need to declare each argument\nconvention as `inout`. This means that changes made to the arguments *in*side\nthe function are visible *out*side the function. \n\nFor example, this function is able to modify the original variables:\n--- cell type: code ---\nfn add_inout(inout x: Int, inout y: Int) -> Int:\n    x += 1\n    y += 1\n    return x + y\n\nvar a = 1\nvar b = 2\nc = add_inout(a, b)\nprint(a)\nprint(b)\nprint(c)\n--- cell type: markdown ---\nAnother option is to declare the argument as `owned`, which provides\nthe function full ownership of the value (it's mutable and guaranteed unique).\nThis way, the function can modify the value and not worry about affecting\nvariables outside the function. For example:\n--- cell type: code ---\nfn set_fire(owned text: String) -> String:\n    text += \"üî•\"\n    return text\n\nfn mojo():\n    let a: String = \"mojo\"\n    let b = set_fire(a)\n    print(a)\n    print(b)\n\nmojo()\n--- cell type: markdown ---\nIn this case, Mojo makes a copy of `a` and passes it as the `text` argument.\nThe original `a` string is still alive and well.\n\nHowever, if you want to give the function ownership of the value and **do not**\nwant to make a copy (which can be an expensive operation for some types), then\nyou can add the `^` \"transfer\" operator when you pass `a` to the function. The\ntransfer operator effectively destroys the local variable name‚Äîany attempt to\ncall upon it later causes a compiler error.\n\nTry it above by changing the call to `set_fire()` to look like this:\n\n```mojo\n    let b = set_fire(a^)\n```\n\nYou'll now get an error because the transfer operator effectively destroys the\n`a` variable, so when the following `print()` function tries to use `a`, that\nvariable isn't initialized anymore.\n\nIf you delete `print(a)`, then it works fine.\n\nThese argument conventions are designed to provide systems programmers with\ntotal control for memory optimizations while ensuring safe access and timely\ndeallocations‚Äîthe Mojo compiler ensures that no two variables have mutable\naccess to the same value at the same time, and the lifetime of each value is\nwell-defined to strictly prevent any memory errors such as \"use-after-free\" and\n\"double-free.\"\n\n<div class=\"alert alert-block alert-info alert--secondary\">\n\n**Note:** Currently, Mojo always makes a copy when a function returns a value.\n\n</div>\n--- cell type: markdown ---\n## Structures\n\nYou can build high-level abstractions for types (or \"objects\") in a `struct`. A\n`struct` in Mojo is similar to a `class` in Python: they both support methods,\nfields, operator overloading, decorators for metaprogramming, etc. However,\nMojo structs are completely static‚Äîthey are bound at compile-time, so they do\nnot allow dynamic dispatch or any runtime changes to the structure. (Mojo will\nalso support classes in the future.)\n\nFor example, here's a basic struct:\n--- cell type: code ---\nstruct MyPair:\n    var first: Int\n    var second: Int\n\n    fn __init__(inout self, first: Int, second: Int):\n        self.first = first\n        self.second = second\n\n    fn dump(self):\n        print(self.first, self.second)\n--- cell type: markdown ---\nAnd here's how you can use it:\n--- cell type: code ---\nlet mine = MyPair(2, 4)\nmine.dump()\n--- cell type: markdown ---\nIf you're familiar with Python, then the `__init__()` method and the `self`\nargument should be familiar to you. If you're _not_ familiar with Python, then\nnotice that, when we call `dump()`, we don't actually pass a value for the\n`self` argument. The value for `self` is automatically provided with the\ncurrent instance of the struct (it's used similar to the `this` name used in\nsome other languages to refer to the current instance of the object/type).\n\nFor more detail, see the sections of the Mojo Manual about\n[structs](https://docs.modular.com/mojo/manual/basics/structs.html) and [value\nlifecycle](https://docs.modular.com/mojo/manual/lifeclcye/).\n--- cell type: markdown ---\n## Python integration\n\nAlthough Mojo is still a work in progress and is not a full superset of Python\nyet, we've built a mechanism to import Python modules as-is, so you can\nleverage existing Python code right away. Under the hood, this mechanism uses\nthe CPython interpreter to run Python code, and thus it works seamlessly with\nall Python modules today.\n\nFor example, here's how you can import and use NumPy (you must have Python\n`numpy` installed):\n--- cell type: code ---\nfrom python import Python\n\nlet np = Python.import_module(\"numpy\")\n\nar = np.arange(15).reshape(3, 5)\nprint(ar)\nprint(ar.shape)\n--- cell type: markdown ---\n\n<div class=\"alert alert-block alert-info alert--secondary\">\n\n**Note:** Mojo is not a feature-complete superset of Python yet. So, you can't\nalways copy Python code and run it in Mojo. For more details on our plans,\nplease refer to the [Mojo roadmap and sharp edges](/mojo/roadmap.html).\n\n</div>\n--- cell type: markdown ---\n\n<div class=\"alert alert-block alert-info alert--warning\">\n\n**Caution:** When you install Mojo, the installer searches your system for a\nversion of Python to use with Mojo, and adds the path to the `modular.cfg`\nconfig file. If you change your Python version or switch virtual environments,\nMojo will then be looking at the wrong Python library, which can cause problems\nsuch as errors when you import Python packages (Mojo says only `An error\noccurred in Python`‚Äîthis is a separate [known\nissue](https://github.com/modularml/mojo/issues/536)). The current solution is\nto override Mojo's path to the Python library, using the `MOJO_PYTHON_LIBRARY`\nenvironment variable. For instructions on how to find and set this path, see\n[this related issue](https://github.com/modularml/mojo/issues/551).\n\n</div>\n--- cell type: markdown ---\n## Next steps\n\nWe hope this page covered enough of the basics to get you started. It's\nintentionally brief, so if you want more detail about any of the topics touched\nupon here, check out the [Mojo\nprogramming manual](https://docs.modular.com/mojo/programming-manual.html).\n\n- If you want to package your code as a library, read about\n  [Mojo modules and packages](/mojo/manual/get-started/packages.html).\n\n- If you want to explore some Mojo code, check out our\n  [code examples on GitHub](https://github.com/modularml/mojo/tree/main/examples#mojo-code-examples).\n\n- To see all the available Mojo APIs, check out the [Mojo standard library\n  reference](/mojo/lib.html).\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-info alert--secondary\">\n\n**Note:** The Mojo SDK is still in early development. Some things are still\nrough, but you can expect constant changes and improvements to both the\nlanguage and tools. Please see the [known\nissues](/mojo/roadmap.html#mojo-sdk-known-issues) and [report any other\nissues on GitHub](https://github.com/modularml/mojo/issues/new/choose).\n\n</div>"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/Mandelbrot.ipynb",
        "content": "--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n# Mandelbrot in Mojo with Python plots\n\n--- cell type: markdown ---\nNot only is Mojo great for writing high-performance code, but it also allows us to leverage the huge Python ecosystem of libraries and tools. With seamless Python interoperability, Mojo can use Python for what it's good at, especially GUIs, without sacrificing performance in critical code. Let's take the classic Mandelbrot set algorithm and implement it in Mojo.\n\nThis tutorial shows two aspects of Mojo. First, it shows that Mojo can be used to develop fast programs for irregular applications. It also shows how we can leverage Python for visualizing the results.\n--- cell type: code ---\n#|code-fold: true\nfrom benchmark import Unit, run\nfrom complex import ComplexSIMD, ComplexFloat64\nfrom math import iota\nfrom python import Python\nfrom runtime.llcl import num_cores\nfrom algorithm import parallelize, vectorize\nfrom tensor import Tensor\nfrom utils.index import Index\n\nalias float_type = DType.float64\nalias simd_width = 2 * simdwidthof[float_type]()\n--- cell type: markdown ---\nFirst set some parameters, you can try changing these to see different results:\n--- cell type: code ---\nalias width = 960\nalias height = 960\nalias MAX_ITERS = 200\n\nalias min_x = -2.0\nalias max_x = 0.6\nalias min_y = -1.5\nalias max_y = 1.5\n--- cell type: markdown ---\nThe core [Mandelbrot](https://en.wikipedia.org/wiki/Mandelbrot_set) algorithm involves computing an iterative complex function for each pixel until it \"escapes\" the complex circle of radius 2, counting the number of iterations to escape:\n\n$$z_{i+1} = z_i^2 + c$$\n--- cell type: code ---\n# Compute the number of steps to escape.\ndef mandelbrot_kernel(c: ComplexFloat64) -> Int:\n    z = c\n    for i in range(MAX_ITERS):\n        z = z * z + c\n        if z.squared_norm() > 4:\n            return i\n    return MAX_ITERS\n\n\ndef compute_mandelbrot() -> Tensor[float_type]:\n    # create a matrix. Each element of the matrix corresponds to a pixel\n    t = Tensor[float_type](height, width)\n\n    dx = (max_x - min_x) / width\n    dy = (max_y - min_y) / height\n\n    y = min_y\n    for row in range(height):\n        x = min_x\n        for col in range(width):\n            t[Index(row, col)] = mandelbrot_kernel(ComplexFloat64(x, y))\n            x += dx\n        y += dy\n    return t\n--- cell type: markdown ---\nPlotting the number of iterations to escape with some color gives us the canonical Mandelbrot set plot. To render it we can directly leverage Python's `matplotlib` right from Mojo!\n\nFirst install the required libraries:\n--- cell type: code ---\n%%python\nfrom importlib.util import find_spec\nimport shutil\nimport subprocess\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which('python3'): python = \"python3\"\n        elif shutil.which('python'): python = \"python\"\n        else: raise (\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n\ninstall_if_missing(\"numpy\")\ninstall_if_missing(\"matplotlib\")\n--- cell type: code ---\ndef show_plot(tensor: Tensor[float_type]):\n    alias scale = 10\n    alias dpi = 64\n\n    np = Python.import_module(\"numpy\")\n    plt = Python.import_module(\"matplotlib.pyplot\")\n    colors = Python.import_module(\"matplotlib.colors\")\n\n    numpy_array = np.zeros((height, width), np.float64)\n\n    for row in range(height):\n        for col in range(width):\n            numpy_array.itemset((col, row), tensor[col, row])\n\n    fig = plt.figure(1, [scale, scale * height // width], dpi)\n    ax = fig.add_axes([0.0, 0.0, 1.0, 1.0], False, 1)\n    light = colors.LightSource(315, 10, 0, 1, 1, 0)\n\n    image = light.shade(numpy_array, plt.cm.hot, colors.PowerNorm(0.3), \"hsv\", 0, 0, 1.5)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()\n\nshow_plot(compute_mandelbrot())\n--- cell type: markdown ---\n## Vectorizing Mandelbrot\nWe showed a naive implementation of the Mandelbrot algorithm, but there are two things we can do to speed it up. We can early-stop the loop iteration when a pixel is known to have escaped, and we can leverage Mojo's access to hardware by vectorizing the loop, computing multiple pixels simultaneously. To do that we will use the `vectorize` higher order generator.\n\nWe start by defining our main iteration loop in a vectorized fashion\n--- cell type: code ---\nfn mandelbrot_kernel_SIMD[\n    simd_width: Int\n](c: ComplexSIMD[float_type, simd_width]) -> SIMD[float_type, simd_width]:\n    \"\"\"A vectorized implementation of the inner mandelbrot computation.\"\"\"\n    let cx = c.re\n    let cy = c.im\n    var x = SIMD[float_type, simd_width](0)\n    var y = SIMD[float_type, simd_width](0)\n    var y2 = SIMD[float_type, simd_width](0)\n    var iters = SIMD[float_type, simd_width](0)\n\n    var t: SIMD[DType.bool, simd_width] = True\n    for i in range(MAX_ITERS):\n        if not t.reduce_or():\n            break\n        y2 = y*y\n        y = x.fma(y + y, cy)\n        t = x.fma(x, y2) <= 4\n        x = x.fma(x, cx - y2)\n        iters = t.select(iters + 1, iters)\n    return iters\n--- cell type: markdown ---\nThe above function is parameterized on the `simd_width` and processes simd_width pixels. It only escapes once all pixels within the vector lane are done. We can use the same iteration loop as above, but this time we vectorize within each row instead. We use the `vectorize` generator to make this a simple function call.\n--- cell type: code ---\nfn vectorized():\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we oeprate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](\n                row * width + col, mandelbrot_kernel_SIMD[simd_width](c)\n            )\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n    @parameter\n    fn bench[simd_width: Int]():\n        for row in range(height):\n            worker(row)\n\n    let vectorized = benchmark.run[bench[simd_width]](\n        max_runtime_secs=0.5\n    ).mean(Unit.ms)\n\n    print(\"Vectorized\", \":\", vectorized, \"ms\")\n\n    try:\n        _ = show_plot(t)\n    except e:\n        print(\"failed to show plot:\", e)\n\nvectorized()\n--- cell type: markdown ---\n## Parallelizing Mandelbrot\nWhile the vectorized implementation above is efficient, we can get better performance by parallelizing on the cols. This again is simple in Mojo using the `parallelize` higher order function. Only the function that performs the invocation needs to change.\n--- cell type: code ---\nfn parallelized():\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we oeprate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](row * width + col, mandelbrot_kernel_SIMD[simd_width](c))\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n\n    @parameter\n    fn bench_parallel[simd_width: Int]():\n        parallelize[worker](height, height)\n\n    let parallelized = benchmark.run[bench_parallel[simd_width]](\n        max_runtime_secs=0.5\n    ).mean(Unit.ms)\n\n    print(\"Parallelized:\", parallelized, Unit.ms)\n\n    try:\n        _ = show_plot(t)\n    except e:\n        print(\"failed to show plot:\", e)\n\nparallelized()\n--- cell type: markdown ---\n## Benchmarking\n\nIn this section we increase the size to 4096x4096 and run 1000 iterations for a larger test to stress the CPU \n--- cell type: code ---\nfn compare():\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we oeprate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](\n                row * width + col, mandelbrot_kernel_SIMD[simd_width](c)\n            )\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n    @parameter\n    fn bench[simd_width: Int]():\n        for row in range(height):\n            worker(row)\n\n    let vectorized = benchmark.run[bench[simd_width]](\n        max_runtime_secs=0.5\n    ).mean(Unit.ms)\n    print(\"Number of threads:\", num_cores())\n    print(\"Vectorized:\", vectorized, \"ms\")\n\n    # Parallelized\n    @parameter\n    fn bench_parallel[simd_width: Int]():\n        parallelize[worker](height, height)\n\n    let parallelized = benchmark.run[bench_parallel[simd_width]](\n        max_runtime_secs=0.5\n    ).mean(Unit.ms)\n    print(\"Parallelized:\", parallelized, \"ms\")\n    print(\"Parallel speedup:\", vectorized / parallelized)\n\n    _ = t # Make sure tensor isn't destroyed before benchmark is finished\n--- cell type: code ---\n#| CHECK: speedup\ncompare()"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/Matmul.ipynb",
        "content": "--- cell type: markdown ---\n---\ntitle: Matrix multiplication in Mojo\ndescription: Learn how to leverage Mojo's various functions to write a high-performance matmul.\nwebsite:\n  open-graph:\n    image: /static/images/mojo-social-card.png\n  twitter-card:\n    image: /static/images/mojo-social-card.png\n---\n\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n# Matrix multiplication in Mojo\n--- cell type: markdown ---\nThis notebook describes how to write a matrix multiplication (matmul) algorithm in Mojo. We will start with a pure Python implementation, transition to a naive implementation that is essentially a copy of the Python one, then add types, then continue the optimizations by vectorizing, tiling, and parallelizing the implementation.\n--- cell type: markdown ---\nFirst, let's define matrix multiplication. Given two dense matrices $A$ and $B$ of dimensions $M\\times K$ and $K\\times N$ respectively, we want to compute their dot product $C = A . B$ (also known as matmul). The dot product $C += A . B$ is defined by\n--- cell type: markdown ---\n$$C_{i,j} += \\sum_{k \\in [0 \\cdots K)} A_{i,k} B_{k,j}$$\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-success alert--secondary\">\n\nPlease take look at our [blog](https://www.modular.com/blog/ais-compute-fragmentation-what-matrix-multiplication-teaches-us) post on matmul and why it is important for ML and DL workloads.\n\n</div>\n--- cell type: markdown ---\nThe format of this notebook is to start with an implementation which is identical to that of Python (effectively renaming the file extension), then look at how adding types to the implementation helps performance before extending the implementation by leveraging the vectorization and parallelization capabilities available on modern hardware. Throughout the execution, we report the GFlops achieved.\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n<div class=\"alert alert-block alert-info alert--secondary\">\n<b>Note:</b> Mojo Playground is designed only for testing the Mojo language.\nThe cloud environment is not always stable and performance varies, so it is not\nan appropriate environment for performance benchmarking. However, we believe it\ncan still demonstrate the magnitude of performance gains provided by Mojo. For\nmore information about the compute power in the Mojo Playground, see the <a\nhref=\"https://docs.modular.com/mojo/faq.html#mojo-playground\">Mojo FAQ</a>.\n</div>\n--- cell type: markdown ---\n## Python Implementation\n--- cell type: markdown ---\nLet's first implement matmul in Python directly from the definition.\n--- cell type: code ---\n%%python\ndef matmul_python(C, A, B):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n--- cell type: markdown ---\nLet's benchmark our implementation using 128 by 128 square matrices and report the achieved GFLops.\n--- cell type: markdown ---\nInstall numpy if it's not already:\n--- cell type: code ---\n%%python\nfrom importlib.util import find_spec\nimport shutil\nimport subprocess\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which('python3'): python = \"python3\"\n        elif shutil.which('python'): python = \"python\"\n        else: raise (\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n\ninstall_if_missing(\"numpy\")\n--- cell type: code ---\n%%python\nfrom timeit import timeit\nimport numpy as np\n\nclass Matrix:\n    def __init__(self, value, rows, cols):\n        self.value = value\n        self.rows = rows\n        self.cols = cols\n\n    def __getitem__(self, idxs):\n        return self.value[idxs[0]][idxs[1]]\n\n    def __setitem__(self, idxs, value):\n        self.value[idxs[0]][idxs[1]] = value\n\ndef benchmark_matmul_python(M, N, K):\n    A = Matrix(list(np.random.rand(M, K)), M, K)\n    B = Matrix(list(np.random.rand(K, N)), K, N)\n    C = Matrix(list(np.zeros((M, N))), M, N)\n    secs = timeit(lambda: matmul_python(C, A, B), number=2)/2\n    gflops = ((2*M*N*K)/secs) / 1e9\n    print(gflops, \"GFLOP/s\")\n    return gflops\n--- cell type: code ---\npython_gflops = benchmark_matmul_python(128, 128, 128).to_float64()\n--- cell type: markdown ---\n## Importing the Python implementation to Mojo\n--- cell type: markdown ---\nUsing Mojo is as simple as Python. First, let's include that modules from the Mojo stdlib that we are going to use:\n--- cell type: code ---\n#|code-fold: true\n#|code-summary: \"Import utilities and define `Matrix` (click to show/hide)\"\n\nfrom benchmark import Unit\nfrom sys.intrinsics import strided_load\nfrom math import div_ceil, min\nfrom memory import memset_zero\nfrom memory.unsafe import DTypePointer\nfrom random import rand, random_float64\nfrom sys.info import simdwidthof\nfrom runtime.llcl import Runtime\n--- cell type: markdown ---\nThen, we can copy and paste our Python code. Mojo is a superset of Python, so the same Python code will run as Mojo code\n--- cell type: code ---\n# This exactly the same Python implementation,\n# but is infact Mojo code!\ndef matmul_untyped(C, A, B):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n--- cell type: markdown ---\nWe can then benchmark the implementation. As before we use a 128 by 128 matrix\n--- cell type: code ---\nfn matrix_getitem(self: object, i: object) raises -> object:\n    return self.value[i]\n\n\nfn matrix_setitem(self: object, i: object, value: object) raises -> object:\n    self.value[i] = value\n    return None\n\n\nfn matrix_append(self: object, value: object) raises -> object:\n    self.value.append(value)\n    return None\n\n\nfn matrix_init(rows: Int, cols: Int) raises -> object:\n    let value = object([])\n    return object(\n        Attr(\"value\", value), Attr(\"__getitem__\", matrix_getitem), Attr(\"__setitem__\", matrix_setitem),\n        Attr(\"rows\", rows), Attr(\"cols\", cols), Attr(\"append\", matrix_append),\n    )\n\ndef benchmark_matmul_untyped(M: Int, N: Int, K: Int, python_gflops: Float64):\n    C = matrix_init(M, N)\n    A = matrix_init(M, K)\n    B = matrix_init(K, N)\n    for i in range(M):\n        c_row = object([])\n        b_row = object([])\n        a_row = object([])\n        for j in range(N):\n            c_row.append(0.0)\n            b_row.append(random_float64(-5, 5))\n            a_row.append(random_float64(-5, 5))\n        C.append(c_row)\n        B.append(b_row)\n        A.append(a_row)\n\n    @parameter\n    fn test_fn():\n        try:\n            _ = matmul_untyped(C, A, B)\n        except:\n            pass\n\n    let secs = benchmark.run[test_fn](max_runtime_secs=0.5).mean()\n    _ = (A, B, C)\n    let gflops = ((2*M*N*K)/secs) / 1e9\n    let speedup : Float64 = gflops / python_gflops\n    print(gflops, \"GFLOP/s, a\", speedup, \"x speedup over Python\")\n--- cell type: code ---\nbenchmark_matmul_untyped(128, 128, 128, python_gflops)\n--- cell type: markdown ---\nNote the huge speedup with no effort that we have gotten.\n--- cell type: markdown ---\n## Adding types to the Python implementation\n--- cell type: markdown ---\nThe above program, while achieving better performance than Python, is still not the best we can get from Mojo. If we tell Mojo the types of the inputs, it can optimize much of the code away and reduce dispatching costs (unlike Python, which only uses types for type checking, Mojo exploits type info for performance optimizations as well).\n--- cell type: markdown ---\nTo do that, let's first define a `Matrix` struct. The `Matrix` struct contains a data pointer along with size fields. While the `Matrix` struct can be parametrized on any data type, here we set the data type to be Float32 for conciseness.\n--- cell type: code ---\nalias type = DType.float32\n\nstruct Matrix:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    # Initialize zeroeing all values\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        memset_zero(self.data, rows * cols)\n        self.rows = rows\n        self.cols = cols\n\n    # Initialize taking a pointer, don't set any elements\n    fn __init__(inout self, rows: Int, cols: Int, data: DTypePointer[DType.float32]):\n        self.data = data\n        self.rows = rows\n        self.cols = cols\n\n    ## Initialize with random values\n    @staticmethod\n    fn rand(rows: Int, cols: Int) -> Self:\n        let data = DTypePointer[type].alloc(rows * cols)\n        rand(data, rows * cols)\n        return Self(rows, cols, data)\n\n    fn __getitem__(self, y: Int, x: Int) -> Float32:\n        return self.load[1](y, x)\n\n    fn __setitem__(self, y: Int, x: Int, val: Float32):\n        return self.store[1](y, x, val)\n\n    fn load[nelts: Int](self, y: Int, x: Int) -> SIMD[DType.float32, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    fn store[nelts: Int](self, y: Int, x: Int, val: SIMD[DType.float32, nelts]):\n        return self.data.simd_store[nelts](y * self.cols + x, val)\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-success alert--secondary\">\n\nNote that we implement `getitem` and `setitem` in terms of `load` and `store`. For the naive implementation of matmul it does not make a difference, but we will utilize this later in a more optimized vectorized version of matmul.\n\n</div>\n--- cell type: markdown ---\nWith the above `Matrix` type we can effectively copy and paste the Python implementation and just add type annotations:\n--- cell type: code ---\n# Note that C, A, and B have types.\nfn matmul_naive(C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n--- cell type: markdown ---\nWe are going to benchmark the implementations as we improve, so let's write a helper function that will do that for us: \n--- cell type: code ---\nalias M = 1024\nalias N = 1024\nalias K = 1024\n\n@always_inline\nfn bench[\n    func: fn (Matrix, Matrix, Matrix) -> None](base_gflops: Float64):\n    var C = Matrix(M, N)\n    var A = Matrix.rand(M, K)\n    var B = Matrix.rand(K, N)\n\n    @always_inline\n    @parameter\n    fn test_fn():\n        _ = func(C, A, B)\n\n    let secs = benchmark.run[test_fn](max_runtime_secs=1).mean()\n    # Prevent the matrices from being freed before the benchmark run\n    A.data.free()\n    B.data.free()\n    C.data.free()\n    let gflops = ((2 * M * N * K) / secs) / 1e9\n    let speedup: Float64 = gflops / base_gflops\n    # print(gflops, \"GFLOP/s\", speedup, \" speedup\")\n    print(gflops, \"GFLOP/s, a\", speedup, \"x speedup over Python\")\n--- cell type: markdown ---\nBenchmarking shows significant speedups. We increase the size of the matrix to 512 by 512, since Mojo is much faster than Python.\n--- cell type: code ---\nbench[matmul_naive](python_gflops)\n--- cell type: markdown ---\nAdding type annotations gives a huge improvement compared to the original untyped version.\n--- cell type: markdown ---\n## Vectorizing the inner most loop\n--- cell type: markdown ---\nWe can do better than the above implementation by utilizing the vector instructions. Rather than assuming a vector width, we query the simd width of the specified dtype using `simd_width`. This makes our code portable as we transition to other hardware. Leverage SIMD instructions is as easy as:\n--- cell type: code ---\n# Mojo has SIMD vector types, we can vectorize the Matmul code as follows.\n# nelts = number of float32 elements that can fit in SIMD register\nalias nelts = simdwidthof[DType.float32]()\nfn matmul_vectorized_0(C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for nv in range(0, C.cols, nelts):\n                C.store[nelts](m,nv, C.load[nelts](m,nv) + A[m,k] * B.load[nelts](k,nv))\n\n            # Handle remaining elements with scalars.\n            for n in range(nelts*(C.cols//nelts), C.cols):\n                C[m,n] += A[m,k] * B[k,n]\n--- cell type: markdown ---\nWe can benchmark the above implementation. Note that many compilers can detect naive loops and perform optimizations on them. Mojo, however, allows you to be explicit and precisely control what optimizations are applied.\n--- cell type: code ---\nbench[matmul_vectorized_0](python_gflops)\n--- cell type: markdown ---\nVectorization is a common optimization, and Mojo provides a higher-order function that performs vectorization for you. The `vectorize` function takes a vector width and a function which is parametric on the vector width and is going to be evaluated in a vectorized manner.\n--- cell type: code ---\n# Simplify the code by using the builtin vectorize function\nfrom algorithm import vectorize\nfn matmul_vectorized_1(C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            @parameter\n            fn dot[nelts : Int](n : Int):\n                C.store[nelts](m,n, C.load[nelts](m,n) + A[m,k] * B.load[nelts](k,n))\n            vectorize[nelts, dot](C.cols)\n--- cell type: markdown ---\nThere is only a slight difference in terms of performance between the two implementations:\n--- cell type: code ---\nbench[matmul_vectorized_1](python_gflops)\n--- cell type: markdown ---\n## Parallelizing Matmul\n--- cell type: markdown ---\nWith Mojo we can easily run code in parallel with the `parallelize` function.\n\nLet's modify our matmul implementation and make it multi-threaded (for simplicity, we only `parallelize` on the M dimension).\n\nIn `parallelize` below we're overpartitioning by distributing the work more evenly among processors. This ensures they all have something to work on even if some tasks finish before others, or some processors are stragglers. Intel and Apple now have separate performance and efficiency cores and this mitigates the problems that can cause.\n--- cell type: code ---\n# Parallelize the code by using the builtin parallelize function\nfrom algorithm import parallelize\nfn matmul_parallelized(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        for k in range(A.cols):\n            @parameter\n            fn dot[nelts : Int](n : Int):\n                C.store[nelts](m,n, C.load[nelts](m,n) + A[m,k] * B.load[nelts](k,n))\n            vectorize[nelts, dot](C.cols)\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nWe can benchmark the parallel matmul implementation.\n--- cell type: code ---\nbench[matmul_parallelized](python_gflops)\n--- cell type: markdown ---\n## Tiling Matmul\n--- cell type: markdown ---\nTiling is an optimization performed for matmul to increase cache locality. The idea is to keep sub-matrices resident in the cache and increase the reuse. The tile function itself can be written in Mojo as:\n--- cell type: code ---\nfrom algorithm import Static2DTileUnitFunc as Tile2DFunc\n--- cell type: code ---\n# Perform 2D tiling on the iteration space defined by end_x and end_y.\nfn tile[tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    for y in range(0, end_y, tile_y):\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n--- cell type: markdown ---\nThe above will perform 2 dimensional tiling over a 2D iteration space defined to be between $([0, end_x], [0, end_y])$. Once we define it above, we can use it within our matmul kernel. For simplicity we choose `4` as the tile height and since we also want to vectorize we use `4 * nelts` as the tile width (since we vectorize on the columns).\n--- cell type: code ---\n# Use the above tile function to perform tiled matmul.\nfn matmul_tiled_parallelized(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n                @parameter\n                fn dot[nelts : Int,](n : Int):\n                    C.store[nelts](m,n + x, C.load[nelts](m,n+x) + A[m,k] * B.load[nelts](k,n+x))\n                vectorize[nelts, dot](tile_x)\n\n        # We hardcode the tile factor to be 4.\n        alias tile_size = 4\n        tile[calc_tile, nelts * tile_size, tile_size](A.cols, C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nAgain, we can benchmark the tiled parallel matmul implementation:\n--- cell type: code ---\nbench[matmul_tiled_parallelized](python_gflops)\n--- cell type: markdown ---\nOne source of overhead in the above implementation is the fact that the we are not unrolling the loops introduced by vectorize of the dot function. We can do that via the `vectorize_unroll` higher-order function in Mojo:\n--- cell type: code ---\n# Unroll the vectorized loop by a constant factor.\nfrom algorithm import vectorize_unroll\nfn matmul_tiled_unrolled_parallelized(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n                @parameter\n                fn dot[nelts : Int,](n : Int):\n                    C.store[nelts](m,n+x, C.load[nelts](m,n+x) + A[m,k] * B.load[nelts](k,n+x))\n\n                # Vectorize by nelts and unroll by tile_x/nelts\n                # Here unroll factor is 4\n                vectorize_unroll[nelts, tile_x//nelts, dot](tile_x)\n\n        alias tile_size = 4\n        tile[calc_tile, nelts*tile_size, tile_size](A.cols, C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nAgain, we can benchmark the new tiled parallel matmul implementation with unrolled and vectorized inner loop:\n--- cell type: code ---\nbench[matmul_tiled_unrolled_parallelized](python_gflops)\n--- cell type: markdown ---\n## Searching for the `tile_factor`\n--- cell type: code ---\nfrom autotune import autotune, search\nfrom time import now\nfrom memory.unsafe import Pointer\n\nalias matmul_fn_sig_type = fn(C: Matrix, A: Matrix, B: Matrix, /) -> None\n--- cell type: markdown ---\nThe choice of the tile factor can greatly impact the performance of the full matmul,\nbut the optimal tile factor is highly hardware-dependent, and is influenced by the\ncache configuration and other hard-to-model effects. We want to write portable code\nwithout having to know everything about the hardware, so we can ask Mojo to automatically\nselect the best tile factor using autotuning.\n--- cell type: code ---\n# Autotune the tile size used in the matmul.\n@adaptive\nfn matmul_autotune_impl(C: Matrix, A: Matrix, B: Matrix, /):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n                @parameter\n                fn dot[nelts : Int](n : Int):\n                    C.store[nelts](m,n+x, C.load[nelts](m,n+x) + A[m,k] * B.load[nelts](k,n+x))\n                vectorize_unroll[nelts, tile_x // nelts, dot](tile_x)\n\n        # Instead of hardcoding to tile_size = 4, search for the fastest\n        # tile size by evaluating this function as tile size varies.\n        alias tile_size = autotune(1, 2, 4, 8, 16, 32)\n        tile[calc_tile, nelts * tile_size, tile_size](A.cols, C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nThis will generate multiple candidates for the matmul function. To teach Mojo how\nto find the best tile factor, we provide an evaluator function Mojo can use to\nassess each candidate.\n--- cell type: code ---\nfn matmul_evaluator(funcs: Pointer[matmul_fn_sig_type], size: Int) -> Int:\n    print(\"matmul_evaluator, number of candidates: \", size)\n\n    let eval_begin: Int = now()\n\n    # This size is picked at random, in real code we could use a real size\n    # distribution here.\n    let M = 512\n    let N = 512\n    let K = 512\n    print(\"Optimizing for size:\", M, \"x\", N, \"x\", K)\n\n    var best_idx: Int = -1\n    var best_time: Int = -1\n\n    alias eval_iterations = 10\n    alias eval_samples = 10\n\n    var C = Matrix(M, N)\n    var A = Matrix(M, K)\n    var B = Matrix(K, N)\n    let Cptr = Pointer[Matrix].address_of(C).address\n    let Aptr = Pointer[Matrix].address_of(A).address\n    let Bptr = Pointer[Matrix].address_of(B).address\n\n    # Find the function that's the fastest on the size we're optimizing for\n    for f_idx in range(size):\n        let func = funcs.load(f_idx)\n\n        @always_inline\n        @parameter\n        fn wrapper():\n            func(C, A, B)\n        let cur_time = benchmark.run[wrapper](max_runtime_secs=0.5).mean(Unit.ns).to_int()\n\n        if best_idx < 0:\n            best_idx = f_idx\n            best_time = cur_time\n        if best_time > cur_time:\n            best_idx = f_idx\n            best_time = cur_time\n\n    let eval_end: Int = now()\n    # Prevent matrices from being destroyed before we finished benchmarking them.\n    A.data.free()\n    B.data.free()\n    C.data.free()\n    print(\"Time spent in matmul_evaluator, ms:\", (eval_end - eval_begin) // 1000000)\n    print(\"Best candidate idx:\", best_idx)\n    return best_idx\n--- cell type: markdown ---\nFinally, we need to define an entry function that would simply call the best candidate.\n--- cell type: code ---\nfn matmul_autotune(C: Matrix, A: Matrix, B: Matrix):\n    alias best_impl: matmul_fn_sig_type\n    search[\n        matmul_fn_sig_type,\n        VariadicList(matmul_autotune_impl.__adaptive_set),\n        matmul_evaluator -> best_impl\n    ]()\n    # Run the best candidate\n    return best_impl(C, A, B)\n--- cell type: markdown ---\nLet's benchmark our new implementation:\n--- cell type: code ---\nbench[matmul_autotune](python_gflops)\n--- cell type: markdown ---\n# Tile and accumulate in registers and reorder loop\n--- cell type: markdown ---\nPerform 2D tiling on the iteration space defined by end_x and end_y, parallelizing over y.\n--- cell type: code ---\nfn tile_parallel[\n    tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int\n](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    @parameter\n    fn row(yo: Int):\n        let y = tile_y * yo\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n\n    parallelize[row](end_y // tile_y, M)\n--- cell type: markdown ---\nUse stack allocation for tiles to accumulate values efficiently, avoiding repeated reads and writes to memory. Also reorder the loops and do not fully unroll the loop over the reduction dimension.\n--- cell type: code ---\nfrom memory import stack_allocation\n\nfn accumulate_registers(C: Matrix, A: Matrix, B: Matrix):\n    alias tile_k = 8\n    alias tile_k_unroll = 8\n    alias tile_i = 32\n    alias tile_j = nelts * 4\n\n    @parameter\n    fn calc_tile[tile_j: Int, tile_i: Int](jo: Int, io: Int):\n        # Allocate the tile of accumulators on the stack.\n        var accumulators = Matrix(\n            tile_i, tile_j, stack_allocation[tile_i * tile_j, DType.float32]()\n        )\n\n        for ko in range(0, A.cols, tile_k * tile_k_unroll):\n            for _ in range(tile_i):\n                for i in range(tile_k):\n                    @unroll\n                    for k in range(tile_k_unroll):\n                        @parameter\n                        fn calc_tile_cols[nelts: Int](j: Int):\n                            accumulators.store[nelts](\n                                i,\n                                j,\n                                accumulators.load[nelts](i, j)\n                                + A[io + i, ko + k]\n                                * B.load[nelts](ko + k, jo + j),\n                            )\n\n                        vectorize_unroll[\n                            nelts, tile_j // nelts, calc_tile_cols\n                        ](tile_j)\n\n        # Copy the local tile to the output\n        for i in range(tile_i):\n            for j in range(tile_j):\n                C[io + i, jo + j] = accumulators[i, j]\n\n    tile_parallel[calc_tile, tile_j, tile_i](C.cols, C.rows)\n--- cell type: code ---\nbench[accumulate_registers](python_gflops)"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/Memset.ipynb",
        "content": "--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n# Fast memset in Mojo\n--- cell type: markdown ---\nIn this tutorial we will implement a memset version optimized for small sizes\nusing Mojo's autotuning feature.\n\nThe idea behind the implementation is based on Nadav Rotem's work [[1](https://github.com/nadavrot/memset_benchmark)], and is also well-described in [[2](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4f7c3da72d557ed418828823a8e59942859d677f.pdf)].\n\nWe briefly summarize the approach below.\n--- cell type: markdown ---\n## High-level overview\n\nFor the best memset performance we want to use the widest possible register\nwidth for the memory access. For instance, if we want to store 19 bytes, we\nwant to use vector width 16 and use two overlapping stores. To store 9 bytes,\nwe would want to use two 8-byte stores.\n\nHowever, before we get to actually doing stores, we need to perform size\nchecks to make sure that we're in the right range. I.e. we want to use 8\nbytes stores for sizes 8-16, 16 bytes stores for sizes 16-32, etc.\n\nThe order in which we do the size checks significantly affects performance\nand ideally we would like to run as few checks as possible for the sizes\nthat occur most often. I.e. if most of the sizes we see are 16-32, then we\nwant to first check if it's within that range before we check if it's in\n8-16 or some other range.\n\nThis results in a number of different comparison \"trees\" that can be used to\nperform the size checks, and in this tutorial we use Mojo's autotuning to pick\nthe most optimal one given the distribution of input data.\n--- cell type: markdown ---\n## Implementation\n\nWe will start as we always start - with imports and type aliases.\n--- cell type: code ---\nfrom autotune import autotune_fork, search\nfrom math import min, max\nfrom memory.unsafe import DTypePointer, Pointer\nfrom time import time_function\nfrom benchmark import keep\nfrom memory import memset as stdlib_memset\n\nalias type = UInt8\nalias ptr_type = DTypePointer[DType.uint8]\n\nalias fn_type = fn(ptr: ptr_type, value: type, count: Int, /) -> None\n--- cell type: markdown ---\nNow let's add some auxiliary functions. We will use them to benchmark various\nmemset implementations and visualize results.\n--- cell type: code ---\nfn measure_time(func: fn_type, size: Int, iters: Int, samples: Int) -> Int:\n    alias alloc_size = 1024 * 1024\n    let ptr = ptr_type.alloc(alloc_size)\n\n    var best = -1\n    for sample in range(samples):\n\n        @parameter\n        fn runner():\n            for iter in range(iters):\n                # Offset pointer to shake up cache a bit\n                let offset_ptr = ptr.offset((iter * 128) & 1024)\n\n                # memset, change the value we're filling with\n                let v = type(iter&255)\n\n                # Actually call the memset function\n                func(offset_ptr, v.value, size)\n\n                # Avoid compiler optimizing things away\n                keep(v)\n                keep(size)\n                keep(offset_ptr)\n\n        let ns = time_function[runner]()\n        if best < 0 or ns < best:\n            best = ns\n\n    ptr.free()\n    return best\n\nalias MULT = 2_000\n\nfn visualize_result(size: Int, result: Int):\n    print_no_newline(\"Size: \")\n    if size < 10:\n        print_no_newline(\" \")\n    print_no_newline(size, \"  |\")\n    for _ in range(result // MULT):\n        print_no_newline(\"*\")\n    print()\n\n\nfn benchmark(func: fn_type, title: StringRef):\n    print(\"\\n=====================\")\n    print(title)\n    print(\"---------------------\\n\")\n\n    alias benchmark_iterations = 30 * MULT\n    alias warmup_samples = 10\n    alias benchmark_samples = 1000\n\n    # Warmup\n    for size in range(35):\n        _ = measure_time(\n            func, size, benchmark_iterations, warmup_samples\n        )\n\n    # Actual run\n    for size in range(35):\n        let result = measure_time(\n            func, size, benchmark_iterations, benchmark_samples\n        )\n\n        visualize_result(size, result)\n--- cell type: markdown ---\n### Reproducing results from the paper\n\nLet's implement a memset version from the paper in Mojo and compare it against\nthe system memset.\n\n--- cell type: code ---\n@always_inline\nfn overlapped_store[\n    width: Int\n](ptr: ptr_type, value: type, count: Int):\n    let v = SIMD[DType.uint8, width].splat(value)\n    ptr.simd_store[width](v)\n    ptr.simd_store[width](count - width, v)\n\n\nfn memset_manual(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count <= 16:\n            if count >= 8:\n                # 8 <= count < 16\n                overlapped_store[8](ptr, value, count)\n                return\n            # 4 < count < 8\n            overlapped_store[4](ptr, value, count)\n            return\n\n        # 16 <= count < 32\n        overlapped_store[16](ptr, value, count)\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n\n\nfn memset_system(ptr: ptr_type, value: type, count: Int):\n    stdlib_memset(ptr, value.value, count)\n--- cell type: markdown ---\nLet's benchmark our version of memset vs the standard memset.\n\n<div class=\"alert alert-block alert-success alert--secondary\">\n\n**Note**: We're optimizing memset for tiniest sizes and benchmarking that properly is tricky. The notebook environment makes it even harder, and while we tried our best to tune the notebook to demonstrate the performance difference, it is hard to guarantee that the results will be stable from run to run.\n\n</div>\n--- cell type: code ---\n#| CHECK: Manual memset\n#| CHECK: System memset\nbenchmark(memset_manual, \"Manual memset\")\nbenchmark(memset_system, \"System memset\")\n--- cell type: markdown ---\n### Tweaking the implementation for different sizes\n\nWe can see that it's already much faster for small sizes.\nThat version was specifically optimized for a certain input size distribution,\ne.g. we can see that sizes 8-16 and 0-4 work fastest.\n\nBut what if in **our use case** the distribution is different? Let's imagine that\nin our case the most common sizes are 16-32 - is this version the most optimal\nversion we can use then? The answer is obviously \"no\", and we can easily tweak\nthe implementation to work better for these sizes - we just need to move the\ncorresponding check closer to the beginning of the function. E.g. like so:\n--- cell type: code ---\nfn memset_manual_2(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count >= 16:\n            # 16 <= count < 32\n            overlapped_store[16](ptr, value, count)\n            return\n\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count >= 8:\n            # 8 <= count < 16\n            overlapped_store[8](ptr, value, count)\n            return\n        # 4 < count < 8\n        overlapped_store[4](ptr, value, count)\n\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n--- cell type: markdown ---\nLet's check the performance of this version.\n--- cell type: code ---\n#| CHECK: Manual memset v2\nbenchmark(memset_manual_2, \"Manual memset v2\")\nbenchmark(memset_system, \"Mojo system memset\")\n--- cell type: markdown ---\nThe performance is now much better on the 16-32 sizes!\n\nThe problem is that we had to manually re-write the code. Wouldn't it be nice\nif it was done automatically?\n\nIn Mojo this is possible (and quite easy) - we can generate multiple\nimplementations and let the compiler pick the fastest one for us evaluating\nthem on sizes we want!\n--- cell type: markdown ---\n### Mojo implementation\n\nLet's dive into that.\n\nThe first thing we need to do is to generate all possible candidates. To do\nthat we will need to iteratively generate size checks to understand what size\nfor the overlapping store we can use. Once we localize the size interval, we\njust call the overlapping store of the corresponding size.\n\nTo express this we will implement an adaptive function `memset_impl_layer` two\nparameters designating the current interval of possible size values. When we\ngenerate a new size check, we split that interval into two parts and\nrecursively call the same functions on those two parts. Once we reach the\nminimal intervals, we will call the corresponding overlapped_store function.\n\nThis first implementation covers minimal interval cases:\n--- cell type: code ---\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    @parameter\n    if lower == -100 and upper == 0:\n        pass\n    elif lower == 0 and upper == 4:\n        ptr.store(0, value)\n        ptr.store(count - 1, value)\n        if count <= 2:\n            return\n        ptr.store(1, value)\n        ptr.store(count - 2, value)\n    elif lower == 4 and upper == 8:\n        overlapped_store[4](ptr, value, count)\n    elif lower == 8 and upper == 16:\n        overlapped_store[8](ptr, value, count)\n    elif lower == 16 and upper == 32:\n        overlapped_store[16](ptr, value, count)\n    elif lower == 32 and upper == 100:\n        memset_system(ptr, value, count)\n    else:\n        constrained[False]()\n--- cell type: markdown ---\nLet's now add an implementation for the other case, where we need to generate a\nsize check.\n--- cell type: code ---\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count > cur:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n    else:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n--- cell type: markdown ---\nHere we use `autotune_fork` to generate all possible at that point checks.\n\nWe will discard values beyond the current interval, and for the values within\nwe will recursively call this function on the interval splits.\n\nThis is sufficient to generate multiple correct versions of memset, but to\nachieve the best performance we need to take into account one more factor: when\nwe're dealing with such small sizes, even the code location matters a lot. E.g.\nif we swap Then and Else branches and invert the condition, we might get a\ndifferent performance of the final function.\n\nTo account for that, let's add one more implementation of our function, but now\nwith branches swapped:\n--- cell type: code ---\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count <= cur:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n    else:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n--- cell type: markdown ---\nWe defined building blocks for our implementation, now we need to add a top\nlevel entry-point that will kick off the recursion we've just defined.\n\nWe will simply call our function with [-100,100] interval - -100 and 100 simply\ndesignate that no checks have been performed yet. This interval will be refined\nas we generate more and more check until we have enough to emit actual stores.\n--- cell type: code ---\n@adaptive\nfn memset_autotune_impl(ptr: ptr_type, value: type, count: Int, /):\n    memset_impl_layer[-100, 100](ptr, value, count)\n--- cell type: markdown ---\nOk, we're done with our memset implementation, now we just need to plug it to\nautotuning infrastructure to let the Mojo compiler do the search and pick the\nbest implementation.\n\nTo do that, we need to define an evaluator - this is a function that will take\nan array of function pointers to all implementations of our function and will\nneed to return an index of the best candidate.\n\nThere are no limitations in how this function can be implemented - it can\nreturn the first or a random candidate, or it can actually benchmark all of\nthem and pick the fastest - this is what we're going to do for this example.\n--- cell type: code ---\nfn memset_evaluator(funcs: Pointer[fn_type], size: Int) -> Int:\n    # This size is picked at random, in real code we could use a real size\n    # distribution here.\n    let size_to_optimize_for = 17\n\n    var best_idx: Int = -1\n    var best_time: Int = -1\n\n    alias eval_iterations = MULT\n    alias eval_samples = 500\n\n    # Find the function that's the fastest on the size we're optimizing for\n    for f_idx in range(size):\n        let func = funcs.load(f_idx)\n        let cur_time = measure_time(\n            func, size_to_optimize_for, eval_iterations, eval_samples\n        )\n        if best_idx < 0:\n            best_idx = f_idx\n            best_time = cur_time\n        if best_time > cur_time:\n            best_idx = f_idx\n            best_time = cur_time\n\n    return best_idx\n--- cell type: markdown ---\nThe evaluator is ready, the last brush stroke is to add a function that will\ncall the best candidate.\n\nThe search will be performed at compile time, and at runtime we will go\ndirectly to the best implementation.\n--- cell type: code ---\nfn memset_autotune(ptr: ptr_type, value: type, count: Int):\n    # Get the set of all candidates\n    alias candidates = memset_autotune_impl.__adaptive_set\n\n    # Use the evaluator to select the best candidate.\n    alias best_impl: fn_type\n    search[fn_type, VariadicList(candidates), memset_evaluator -> best_impl]()\n\n    # Run the best candidate\n    return best_impl(ptr, value, count)\n--- cell type: markdown ---\nWe are now ready to benchmark our function, let's see how its performance looks!\n--- cell type: code ---\n#| CHECK: Mojo autotune memset\nbenchmark(memset_manual, \"Mojo manual memset\")\nbenchmark(memset_manual_2, \"Mojo manual memset v2\")\nbenchmark(memset_system, \"Mojo system memset\")\nbenchmark(memset_autotune, \"Mojo autotune memset\")"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/README.md",
        "content": "# Mojo Jupyter notebooks\n\nMojo supports programming in [Jupyter notebooks](https://jupyter.org/), just\nlike Python.\n\nThis page explains how to get started with Mojo notebooks, and this repo\ndirectory contains notebooks that demonstrate some of Mojo's features\n(most of which we originally published on the [Mojo\nPlayground](https://playground.modular.com/)).\n\nIf you're not familiar with Jupyter notebooks, they're files that allow you to\ncreate documents with live code, equations, visualizations, and explanatory\ntext. They're basically documents with executable code blocks, making them\ngreat for sharing code experiments and programming tutorials. We actually wrote\nthe [Mojo programming\nmanual](https://docs.modular.com/mojo/programming-manual.html) as a Jupyter\nnotebook, so we can easily test all the code samples.\n\nAnd because Mojo allows you to import Python modules, you can use visualization\nlibraries in your notebooks to draw graphs and charts, or display images. For\nan example, check out the `Mandelbrot.ipynb` notebook, which uses `matplotlib`\nto draw the Mandelbrot set calculated in Mojo, and the `RayTracing.ipynb`\nnotebook, which draws images using `numpy`.\n\n## Get started in VS Code\n\nVisual Studio Code is a great environment for programming with Jupyter\nnotebooks. Especially if you're developing with Mojo on a remote system, using\nVS Code is ideal because it allows you to edit and interact with notebooks on\nthe remote machine where you've installed Mojo.\n\nAll you need is the Mojo SDK and the Jupyter VS Code extension:\n\n1. Install the [Mojo SDK](https://developer.modular.com/download).\n\n2. Install [Visual Studio Code](https://code.visualstudio.com/) and the\n   [Jupyter\n   extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter).\n\n3. Then open any `.ipynb` file with Mojo code, click **Select Kernel** in the\n   top-right corner of the document, and then select **Jupyter Kernel > Mojo**.\n\n   The Mojo kernel should have been installed automatically when you installed\n   the Mojo SDK. If the Mojo kernel is not listed, make sure that your\n   `$MODULAR_HOME` environment variable is set on the system where you\n   installed the Mojo SDK (specified in the `~/.profile` or `~/.bashrc` file).\n\n   Now run some Mojo code!\n\n## Get started with JupyterLab\n\nYou can also select the Mojo kernel when running notebooks in a local instance\nof JupyterLab. The following is just a quick setup guide for Linux users with\nthe Mojo SDK installed locally, and it might not work with your system (these\ninstructions don't support remote access to the JupyterLab). For more details\nabout using JupyterLab, see the complete [JupyterLab installation\nguide](https://jupyterlab.readthedocs.io/en/latest/getting_started/installation.html).\n\n**Note:** You must run this setup on the same machine where you've installed\nthe [Mojo SDK](https://developer.modular.com/download). However, syntax\nhighlighting for Mojo code is not currently enabled in JupyterLab (coming soon).\n\n1. Install JupyterLab:\n\n    ```sh\n    python3 -m pip install jupyterlab\n    ```\n\n2. Make sure the user-level `bin` is in your `$PATH`:\n\n    ```sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n3. Launch JupyterLab:\n\n    ```sh\n    jupyter lab\n    ```\n\n4. When you open any of the `.ipynb` notebooks from this repository, JupyterLab\n   should automatically select the Mojo kernel (which was installed with the\n   Mojo SDK).\n\n   Now run some Mojo code!\n\n## Notes and tips\n\n- Code in a Jupyter notebook cell behaves like code in a Mojo REPL environment:\n  The `main()` function is not required, but there are some caveats:\n\n  - Top-level variables (variables declared outside a function) are not visible\n    inside functions.\n\n  - Redefining undeclared variables is not supported (variables without a `let`\n    or `var` in front). If you‚Äôd like to redefine a variable across notebook\n    cells, you must declare the variable with either `let` or `var`.\n\n- You can use `%%python` at the top of a code cell and write normal Python\n  code. Variables, functions, and imports defined in a Python cell are available\n  from subsequent Mojo code cells.\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/RayTracing.ipynb",
        "content": "--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n[//]: # REMOVE_FOR_WEBSITE\n# Ray tracing in Mojo\n--- cell type: markdown ---\nThis tutorial about [ray tracing](https://en.wikipedia.org/wiki/Ray_tracing_(graphics)) is based on the popular tutorial [Understandable RayTracing in C++](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing). The mathematical explanations are well described in that tutorial, so we'll just point you to the appropriate sections for reference as we implement a basic ray tracer in Mojo.\n--- cell type: markdown ---\n## Step 1: Basic definitions\n\nWe'll start by defining a `Vec3f` struct, which will use to represent a vector in 3D space as well as RGB pixels. We'll use a `SIMD` representation for our vector to enable vectorized operations. Note that since the SIMD type only allows a power of 2, we always pad the underlying storage with a 0.\n--- cell type: code ---\nfrom math import rsqrt\n\n\n@register_passable(\"trivial\")\nstruct Vec3f:\n    var data: SIMD[DType.float32, 4]\n\n    @always_inline\n    fn __init__(x: Float32, y: Float32, z: Float32) -> Self:\n        return Vec3f {data: SIMD[DType.float32, 4](x, y, z, 0)}\n\n    @always_inline\n    fn __init__(data: SIMD[DType.float32, 4]) -> Self:\n        return Vec3f {data: data}\n\n    @always_inline\n    @staticmethod\n    fn zero() -> Vec3f:\n        return Vec3f(0, 0, 0)\n\n    @always_inline\n    fn __sub__(self, other: Vec3f) -> Vec3f:\n        return self.data - other.data\n\n    @always_inline\n    fn __add__(self, other: Vec3f) -> Vec3f:\n        return self.data + other.data\n\n    @always_inline\n    fn __matmul__(self, other: Vec3f) -> Float32:\n        return (self.data * other.data).reduce_add()\n\n    @always_inline\n    fn __mul__(self, k: Float32) -> Vec3f:\n        return self.data * k\n\n    @always_inline\n    fn __neg__(self) -> Vec3f:\n        return self.data * -1.0\n\n    @always_inline\n    fn __getitem__(self, idx: Int) -> SIMD[DType.float32, 1]:\n        return self.data[idx]\n\n    @always_inline\n    fn cross(self, other: Vec3f) -> Vec3f:\n        let self_zxy = self.data.shuffle[2, 0, 1, 3]()\n        let other_zxy = other.data.shuffle[2, 0, 1, 3]()\n        return (self_zxy * other.data - self.data * other_zxy).shuffle[\n            2, 0, 1, 3\n        ]()\n\n    @always_inline\n    fn normalize(self) -> Vec3f:\n        return self.data * rsqrt(self @ self)\n\n--- cell type: markdown ---\nWe now define our `Image` struct, which will store the RGB pixels of our images. It also contains a method to conver this Mojo struct into a numpy image, which will be used for implementing a straightforward displaying mechanism. We will also implement a function for loading PNG files from disk.\n--- cell type: markdown ---\nFirst install the required libraries:\n--- cell type: code ---\n%%python\nfrom importlib.util import find_spec\nimport shutil\nimport subprocess\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which('python3'): python = \"python3\"\n        elif shutil.which('python'): python = \"python\"\n        else: raise (\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n\ninstall_if_missing(\"numpy\")\ninstall_if_missing(\"matplotlib\")\n\n--- cell type: code ---\nfrom python import Python\nfrom python.object import PythonObject\n\nstruct Image:\n    # reference count used to make the object efficiently copyable\n    var rc: Pointer[Int]\n    # the two dimensional image is represented as a flat array\n    var pixels: Pointer[Vec3f]\n    var height: Int\n    var width: Int\n\n    fn __init__(inout self, height: Int, width: Int):\n        self.height = height\n        self.width = width\n        self.pixels = Pointer[Vec3f].alloc(self.height * self.width)\n        self.rc = Pointer[Int].alloc(1)\n        self.rc.store(1)\n\n    fn __copyinit__(inout self, other: Self):\n        other._inc_rc()\n        self.pixels = other.pixels\n        self.rc = other.rc\n        self.height = other.height\n        self.width = other.width\n\n    fn __del__(owned self):\n        self._dec_rc()\n\n    fn _get_rc(self) -> Int:\n        return self.rc.load()\n\n    fn _dec_rc(self):\n        let rc = self._get_rc()\n        if rc > 1:\n            self.rc.store(rc - 1)\n            return\n        self._free()\n\n    fn _inc_rc(self):\n        let rc = self._get_rc()\n        self.rc.store(rc + 1)\n\n    fn _free(self):\n        self.rc.free()\n        self.pixels.free()\n\n    @always_inline\n    fn set(self, row: Int, col: Int, value: Vec3f) -> None:\n        self.pixels.store(self._pos_to_index(row, col), value)\n\n    @always_inline\n    fn _pos_to_index(self, row: Int, col: Int) -> Int:\n        # Convert a (rol, col) position into an index in the underlying linear storage\n        return row * self.width + col\n\n    def to_numpy_image(self) -> PythonObject:\n        let np = Python.import_module(\"numpy\")\n        let plt = Python.import_module(\"matplotlib.pyplot\")\n\n        let np_image = np.zeros((self.height, self.width, 3), np.float32)\n\n        # We use raw pointers to efficiently copy the pixels to the numpy array\n        let out_pointer = Pointer(\n            __mlir_op.`pop.index_to_pointer`[\n                _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n            ](\n                SIMD[DType.index, 1](\n                    np_image.__array_interface__[\"data\"][0].__index__()\n                ).value\n            )\n        )\n        let in_pointer = Pointer(\n            __mlir_op.`pop.index_to_pointer`[\n                _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n            ](SIMD[DType.index, 1](self.pixels.__as_index()).value)\n        )\n\n        for row in range(self.height):\n            for col in range(self.width):\n                let index = self._pos_to_index(row, col)\n                for dim in range(3):\n                    out_pointer.store(\n                        index * 3 + dim, in_pointer[index * 4 + dim]\n                    )\n\n        return np_image\n\n\ndef load_image(fname: String) -> Image:\n    let np = Python.import_module(\"numpy\")\n    let plt = Python.import_module(\"matplotlib.pyplot\")\n\n    let np_image = plt.imread(fname)\n    let rows = np_image.shape[0].__index__()\n    let cols = np_image.shape[1].__index__()\n    let image = Image(rows, cols)\n\n    let in_pointer = Pointer(\n        __mlir_op.`pop.index_to_pointer`[\n            _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n        ](\n            SIMD[DType.index, 1](\n                np_image.__array_interface__[\"data\"][0].__index__()\n            ).value\n        )\n    )\n    let out_pointer = Pointer(\n        __mlir_op.`pop.index_to_pointer`[\n            _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n        ](SIMD[DType.index, 1](image.pixels.__as_index()).value)\n    )\n    for row in range(rows):\n        for col in range(cols):\n            let index = image._pos_to_index(row, col)\n            for dim in range(3):\n                out_pointer.store(\n                    index * 4 + dim, in_pointer[index * 3 + dim]\n                )\n    return image\n\n--- cell type: markdown ---\nWe then add a function for quickly displaying an `Image` into the notebook. Our Python interop comes in quite handy.\n--- cell type: code ---\ndef render(image: Image):\n    np = Python.import_module(\"numpy\")\n    plt = Python.import_module(\"matplotlib.pyplot\")\n    colors = Python.import_module(\"matplotlib.colors\")\n    dpi = 32\n    fig = plt.figure(1, [image.height // 10, image.width // 10], dpi)\n\n    plt.imshow(image.to_numpy_image())\n    plt.axis(\"off\")\n    plt.show()\n\n--- cell type: markdown ---\nFinally, we test all our code so far with a simple image, which is the one rendered in the [Step 1 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-1-write-an-image-to-the-disk).\n--- cell type: code ---\nlet image = Image(192, 256)\n\nfor row in range(image.height):\n    for col in range(image.width):\n        image.set(\n            row,\n            col,\n            Vec3f(Float32(row) / image.height, Float32(col) / image.width, 0),\n        )\n\nrender(image)\n\n--- cell type: markdown ---\n## Step 2: Ray tracing\n\nNow we'll perform ray tracing from a camera into a scene with a sphere. Before reading the code below, we suggest you read more about how this works conceptually from [Step 2 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-2-the-crucial-one-ray-tracing).\n--- cell type: markdown ---\nWe first define the `Material` and `Sphere` structs, which are the new data structures we'll need.\n--- cell type: code ---\nfrom math import sqrt\n\n\n@register_passable(\"trivial\")\nstruct Material:\n    var color: Vec3f\n    var albedo: Vec3f\n    var specular_component: Float32\n\n    fn __init__(color: Vec3f) -> Material:\n        return Material {\n            color: color, albedo: Vec3f(0, 0, 0), specular_component: 0\n        }\n\n    fn __init__(\n        color: Vec3f, albedo: Vec3f, specular_component: Float32\n    ) -> Material:\n        return Material {\n            color: color, albedo: albedo, specular_component: specular_component\n        }\n\n\nalias W = 1024\nalias H = 768\nalias bg_color = Vec3f(0.02, 0.02, 0.02)\nlet shiny_yellow = Material(Vec3f(0.95, 0.95, 0.4), Vec3f(0.7, 0.6, 0), 30.0)\nlet green_rubber = Material(Vec3f( 0.3,  0.7, 0.3), Vec3f(0.9, 0.1, 0), 1.0)\n\n\n@register_passable(\"trivial\")\nstruct Sphere(CollectionElement):\n    var center: Vec3f\n    var radius: Float32\n    var material: Material\n\n    fn __init__(c: Vec3f, r: Float32, material: Material) -> Self:\n        return Sphere {center: c, radius: r, material: material}\n\n    @always_inline\n    fn intersects(self, orig: Vec3f, dir: Vec3f, inout dist: Float32) -> Bool:\n        \"\"\"This method returns True if a given ray intersects this sphere.\n        And if it does, it writes in the `dist` parameter the distance to the\n        origin of the ray.\n        \"\"\"\n        let L = orig - self.center\n        let a = dir @ dir\n        let b = 2 * (dir @ L)\n        let c = L @ L - self.radius * self.radius\n        let discriminant = b * b - 4 * a * c\n        if discriminant < 0:\n            return False\n        if discriminant == 0:\n            dist = -b / 2 * a\n            return True\n        let q = -0.5 * (b + sqrt(discriminant)) if b > 0 else -0.5 * (\n            b - sqrt(discriminant)\n        )\n        var t0 = q / a\n        let t1 = c / q\n        if t0 > t1:\n            t0 = t1\n        if t0 < 0:\n            t0 = t1\n            if t0 < 0:\n                return False\n\n        dist = t0\n        return True\n\n--- cell type: markdown ---\nWe then define a `cast_ray` method, which will be used to figure out the color of a particular pixel in the image we'll produce. It basically works by identifying whether this ray intersects the sphere or not.\n--- cell type: code ---\nfn cast_ray(orig: Vec3f, dir: Vec3f, sphere: Sphere) -> Vec3f:\n    var dist: Float32 = 0\n    if not sphere.intersects(orig, dir, dist):\n        return bg_color\n\n    return sphere.material.color\n\n--- cell type: markdown ---\nLastly, we parallelize the ray tracing for every pixel row-wise.\n--- cell type: code ---\nfrom math import tan, acos\nfrom algorithm import parallelize\n\n\nfn create_image_with_sphere(sphere: Sphere, height: Int, width: Int) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(row, col, cast_ray(Vec3f.zero(), dir, sphere))\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nrender(\n    create_image_with_sphere(Sphere(Vec3f(-3, 0, -16), 2, shiny_yellow), H, W)\n)\n\n--- cell type: markdown ---\n## Step 3: More spheres\n\nThis section corresponds to the [Step 3 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-3-add-more-spheres). \n\nWe include here all the necessary changes:\n\n- We add 3 more spheres to the scene, 2 of them being of ivory material.\n- When we intersect the ray with the sphere, we render the color of the closest sphere.\n--- cell type: code ---\nfrom algorithm import parallelize\nfrom math.limit import inf\n# DynamicVector doesn't work in jupyter environment yet\nfrom utils.vector import _OldDynamicVector as DynamicVector\n\n\nfn scene_intersect(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    background: Material,\n) -> Material:\n    var spheres_dist = inf[DType.float32]()\n    var material = background\n\n    for i in range(spheres.size):\n        var dist = inf[DType.float32]()\n        if spheres[i].intersects(orig, dir, dist) and dist < spheres_dist:\n            spheres_dist = dist\n            material = spheres[i].material\n\n    return material\n\n\nfn cast_ray(\n    orig: Vec3f, dir: Vec3f, spheres: DynamicVector[Sphere]\n) -> Material:\n    let background = Material(Vec3f(0.02, 0.02, 0.02))\n    return scene_intersect(orig, dir, spheres, background)\n\n\nfn create_image_with_spheres(\n    spheres: DynamicVector[Sphere], height: Int, width: Int\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(row, col, cast_ray(Vec3f.zero(), dir, spheres).color)\n\n    parallelize[_process_row](height)\n\n    return image\n\nlet spheres = DynamicVector[Sphere]()\nspheres.push_back(Sphere(Vec3f(-3,      0, -16),   2, shiny_yellow))\nspheres.push_back(Sphere(Vec3f(-1.0, -1.5, -12), 1.8, green_rubber))\nspheres.push_back(Sphere(Vec3f( 1.5, -0.5, -18),   3, green_rubber))\nspheres.push_back(Sphere(Vec3f( 7,      5, -18),   4, shiny_yellow))\n\nrender(create_image_with_spheres(spheres, H, W))\n\n--- cell type: markdown ---\n## Step 4: Add lighting\n\nThis section corresponds to the [Step 4 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-4-lighting). Please read that section for an explanation of the trick used to estimate the light intensity of pixel based on the angle of intersection between each ray and the spheres. The changes are minimal and are primarily about handling this intersection angle.\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct Light(CollectionElement):\n    var position: Vec3f\n    var intensity: Float32\n\n    fn __init__(p: Vec3f, i: Float32) -> Self:\n        return Light {position: p, intensity: i}\n\n--- cell type: code ---\nfrom math import max\n\n\nfn scene_intersect(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    inout material: Material,\n    inout hit: Vec3f,\n    inout N: Vec3f,\n) -> Bool:\n    var spheres_dist = inf[DType.float32]()\n\n    for i in range(0, spheres.size):\n        var dist: Float32 = 0\n        if spheres[i].intersects(orig, dir, dist) and dist < spheres_dist:\n            spheres_dist = dist\n            hit = orig + dir * dist\n            N = (hit - spheres[i].center).normalize()\n            material = spheres[i].material\n\n    return (spheres_dist != inf[DType.float32]()).__bool__()\n\n\nfn cast_ray(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n) -> Material:\n    var point = Vec3f.zero()\n    var material = Material(Vec3f.zero())\n    var N = Vec3f.zero()\n    if not scene_intersect(orig, dir, spheres, material, point, N):\n        return bg_color\n\n    var diffuse_light_intensity: Float32 = 0\n    for i in range(lights.size):\n        let light_dir = (lights[i].position - point).normalize()\n        diffuse_light_intensity += lights[i].intensity * max(0, light_dir @ N)\n\n    return material.color * diffuse_light_intensity\n\n\nfn create_image_with_spheres_and_lights(\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    height: Int,\n    width: Int,\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(\n                row, col, cast_ray(Vec3f.zero(), dir, spheres, lights).color\n            )\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nlet lights = DynamicVector[Light]()\nlights.push_back(Light(Vec3f(-20, 20, 20), 1.0))\nlights.push_back(Light(Vec3f(20, -20, 20), 0.5))\n\nrender(create_image_with_spheres_and_lights(spheres, lights, H, W))\n\n--- cell type: markdown ---\n## Step 5: Add specular lighting\n\nThis section corresponds to the [Step 5 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-5-specular-lighting). The changes to the code are quite minimal, but the rendered picture looks much more realistic!\n--- cell type: code ---\nfrom math import pow\n\n\nfn reflect(I: Vec3f, N: Vec3f) -> Vec3f:\n    return I - N * (I @ N) * 2.0\n\n\nfn cast_ray(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n) -> Material:\n    var point = Vec3f.zero()\n    var material = Material(Vec3f.zero())\n    var N = Vec3f.zero()\n    if not scene_intersect(orig, dir, spheres, material, point, N):\n        return bg_color\n\n    var diffuse_light_intensity: Float32 = 0\n    var specular_light_intensity: Float32 = 0\n    for i in range(lights.size):\n        let light_dir = (lights[i].position - point).normalize()\n        diffuse_light_intensity += lights[i].intensity * max(0, light_dir @ N)\n        specular_light_intensity += (\n            pow(\n                max(0.0, -reflect(-light_dir, N) @ dir),\n                material.specular_component,\n            )\n            * lights[i].intensity\n        )\n\n    let result = material.color * diffuse_light_intensity * material.albedo.data[\n        0\n    ] + Vec3f(\n        1.0, 1.0, 1.0\n    ) * specular_light_intensity * material.albedo.data[\n        1\n    ]\n    let result_max = max(result[0], max(result[1], result[2]))\n    # Cap the resulting vector\n    if result_max > 1:\n        return result * (1.0 / result_max)\n    return result\n\n\nfn create_image_with_spheres_and_specular_lights(\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    height: Int,\n    width: Int,\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(\n                row, col, cast_ray(Vec3f.zero(), dir, spheres, lights).color\n            )\n\n    parallelize[_process_row](height)\n\n    return image\n\nrender(create_image_with_spheres_and_specular_lights(spheres, lights, H, W))\n\n--- cell type: markdown ---\n## Step 6: Add background\n\nAs a last step, let's use an image for the background instead of a uniform fill. The only code that we need to change is the code where we used to return `bg_color`. Now we will determine a point in the background image to which the ray is directed and draw that.\n--- cell type: code ---\nfrom math import abs\n\n\nfn cast_ray(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    bg: Image,\n) -> Material:\n    var point = Vec3f.zero()\n    var material = Material(Vec3f.zero())\n    var N = Vec3f.zero()\n    if not scene_intersect(orig, dir, spheres, material, point, N):\n        # Background\n        # Given a direction vector `dir` we need to find a pixel in the image\n        let x = dir[0]\n        let y = dir[1]\n\n        # Now map x from [-1,1] to [0,w-1] and do the same for y.\n        let w = bg.width\n        let h = bg.height\n        let col = int((1.0 + x) * 0.5 * (w - 1))\n        let row = int((1.0 + y) * 0.5 * (h - 1))\n        return Material(bg.pixels[bg._pos_to_index(row, col)])\n\n    var diffuse_light_intensity: Float32 = 0\n    var specular_light_intensity: Float32 = 0\n    for i in range(lights.size):\n        let light_dir = (lights[i].position - point).normalize()\n        diffuse_light_intensity += lights[i].intensity * max(0, light_dir @ N)\n        specular_light_intensity += (\n            pow(\n                max(0.0, -reflect(-light_dir, N) @ dir),\n                material.specular_component,\n            )\n            * lights[i].intensity\n        )\n\n    let result = material.color * diffuse_light_intensity * material.albedo.data[\n        0\n    ] + Vec3f(\n        1.0, 1.0, 1.0\n    ) * specular_light_intensity * material.albedo.data[\n        1\n    ]\n    let result_max = max(result[0], max(result[1], result[2]))\n    # Cap the resulting vector\n    if result_max > 1:\n        return result * (1.0 / result_max)\n    return result\n\n\nfn create_image_with_spheres_and_specular_lights(\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    height: Int,\n    width: Int,\n    bg: Image,\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(\n                row, col, cast_ray(Vec3f.zero(), dir, spheres, lights, bg).color\n            )\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nlet bg = load_image(\"images/background.png\")\nrender(\n    create_image_with_spheres_and_specular_lights(spheres, lights, H, W, bg)\n)\n\n--- cell type: markdown ---\n## Next steps\n\nWe've only explored the basics of ray tracing here, but you can add shadows, reflections and so much more! Fortunately these are explained in [the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing), and we leave the corresponding Mojo implementations as an exercise for you."
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/help/sharing-notebooks.ipynb",
        "content": "--- cell type: markdown ---\n# Sharing notebooks\n\nYou can share your Mojo notebooks with other Mojo Playground users.\n\n## Share a Mojo notebook\n\n1. Save a notebook in the `shared` directory. \n2. Right-click on the file and select **Copy Sharable link**.\n3. Share the link.\n4. To stop sharing, remove the file from the `shared` directory.\n\n![](https://docs.modular.com/static/images/playground/copy-sharable-link.png)\n\n\n## Copy a shared Mojo notebook\n\n1. Open the shared link in a browser. \n2. By default, Mojo Playground always opens the `HelloMojo` notebook when\nit opens, so you might need to click the **Preview** tab to see the\nshared notebook.\n3. Click **Import** at the top of the notebook window. This makes a copy\nof the file on your Mojo Playground volume. If the original author makes\nany changes (including removing the file), it will not affect your imported\ncopy.\n\n![](https://docs.modular.com/static/images/playground/save-shared-notebook.png)\n--- cell type: markdown ---\n## Caveats\n\n- Only files ending with `.ipynb` can be shared.\n- The maximum sharable notebook size is 256 KiB.\n- You can click **Copy Sharable link** on any file, but links for files that are\n  not in the `shared` directory will not work (they will 404)."
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/pymatmul.py",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# Simple program demonstrating a naive matrix multiplication in Python\nfrom timeit import timeit\n\nimport check_mod\n\ncheck_mod.install_if_missing(\"numpy\")\nimport numpy as np\n\n\nclass PyMatrix:\n    def __init__(self, value, rows, cols):\n        self.value = value\n        self.rows = rows\n        self.cols = cols\n\n    def __getitem__(self, idxs):\n        return self.value[idxs[0]][idxs[1]]\n\n    def __setitem__(self, idxs, value):\n        self.value[idxs[0]][idxs[1]] = value\n\n\ndef matmul_python(C, A, B):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n\n\ndef benchmark_matmul_python(M, N, K):\n    A = PyMatrix(list(np.random.rand(M, K)), M, K)\n    B = PyMatrix(list(np.random.rand(K, N)), K, N)\n    C = PyMatrix(list(np.zeros((M, N))), M, N)\n    secs = timeit(lambda: matmul_python(C, A, B), number=2) / 2\n    gflops = ((2 * M * N * K) / secs) / 1e9\n    return gflops\n\n\ndef benchmark_matmul_numpy(M, N, K):\n    A = np.random.rand(M, K).astype(np.float32)\n    B = np.random.rand(K, N).astype(np.float32)\n    secs = timeit(lambda: A @ B, number=10) / 10\n    gflops = ((2 * M * N * K) / secs) / 1e9\n    return gflops\n\n\nif __name__ == \"__main__\":\n    print(\"Throughput of a 128x128 matrix multiplication in Python:\")\n    print(benchmark_matmul_python(128, 128, 128))\n    print(\"Throughput of a 128x128 matrix multiplication in Python numpy:\")\n    print(benchmark_matmul_numpy(128, 128, 128))\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/reduce.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample implements a simple reduction operation on a\n# large array of values to produce a single result.\n# Reductions and scans are common algorithm patterns in parallel computing.\n\nfrom benchmark import Unit, keep\nfrom time import now\nfrom algorithm import sum\nfrom random import rand\nfrom memory.buffer import Buffer\nfrom python import Python\n\n# Change these numbers to reduce on different sizes\nalias size_small: Int = 1 << 21\nalias size_large: Int = 1 << 27\n\n# Datatype for Tensor/Array\nalias type = DType.float32\n\n\n# Use the https://en.wikipedia.org/wiki/Kahan_summation_algorithm\n# Simple summation of the array elements\nfn naive_reduce_sum[size: Int](array: Tensor[type]) -> Float32:\n    let A = array\n    var my_sum = array[0]\n    var c: Float32 = 0.0\n    for i in range(array.dim(0)):\n        let y = array[i] - c\n        let t = my_sum + y\n        c = (t - my_sum) - y\n        my_sum = t\n    return my_sum\n\n\nfn stdlib_reduce_sum[size: Int](array: Tensor[type]) -> Float32:\n    let my_sum = sum(array._to_buffer())\n    return my_sum\n\n\nfn pretty_print(name: StringLiteral, elements: Int, time: Float64) raises:\n    let py = Python.import_module(\"builtins\")\n    _ = py.print(\n        py.str(\"{:<16} {:>11,} {:>8.2f}ms\").format(\n            String(name) + \" elements:\", elements, time\n        )\n    )\n\n\nfn bench[\n    func: fn[size: Int] (array: Tensor[type]) -> Float32,\n    size: Int,\n    name: StringLiteral,\n](array: Tensor[type]) raises:\n    @parameter\n    fn runner():\n        let result = func[size](array)\n        keep(result)\n\n    let ms = benchmark.run[runner](max_runtime_secs=0.5).mean(Unit.ms)\n    pretty_print(name, size, ms)\n\n\nfn main() raises:\n    print(\n        \"Sum all values in a small array and large array\\n\"\n        \"Shows algorithm.sum from stdlib with much better performance\\n\"\n    )\n    # Create two 1-dimensional tensors i.e. arrays\n    let small_array = rand[type](size_small)\n    let large_array = rand[type](size_large)\n\n    bench[naive_reduce_sum, size_small, \"naive\"](small_array)\n    bench[naive_reduce_sum, size_large, \"naive\"](large_array)\n\n    bench[stdlib_reduce_sum, size_small, \"stdlib\"](small_array)\n    bench[stdlib_reduce_sum, size_large, \"stdlib\"](large_array)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/simple_interop.py",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n\n# Simple python program to test interop with Mojo.\n# This file is imported from hello_interop.mojo.\n\nimport check_mod\n\ncheck_mod.install_if_missing(\"numpy\")\nimport numpy as np\n\n\ndef test_interop_func():\n    print(\"Hello from Python!\")\n    a = np.array([1, 2, 3])\n    print(\"I can even print a numpy array: \", a)\n\n\nif __name__ == \"__main__\":\n    from timeit import timeit\n\n    print(timeit(lambda: test_interop_func(), number=1))\n"
    }
]
[
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/LICENSE",
        "content": "==============================================================================================\nThe Mojo examples repository is licensed under the Apache License v2.0 with LLVM Exceptions:\n==============================================================================================\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n    1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n    2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n    3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n    4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n    5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n    6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n    7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n    8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n    9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n    END OF TERMS AND CONDITIONS\n\n    APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n    Copyright [yyyy] [name of copyright owner]\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n\n---- LLVM Exceptions to the Apache 2.0 License ----\n\nAs an exception, if, as a result of your compiling your source code, portions\nof this Software are embedded into an Object form of such source code, you\nmay redistribute such embedded portions in such Object form without complying\nwith the conditions of Sections 4(a), 4(b) and 4(d) of the License.\n\nIn addition, if you combine or link compiled forms of this Software with\nsoftware that is licensed under the GPLv2 (\"Combined Software\") and if a\ncourt of competent jurisdiction determines that the patent provision (Section\n3), the indemnity provision (Section 9) or other Section of the License\nconflicts with the conditions of the GPLv2, you may retroactively and\nprospectively choose to deem waived or otherwise exclude such Section(s) of\nthe License, but only in their entirety and only with respect to the Combined\nSoftware.\n\n==============================================================================\nSoftware from third parties included in the LLVM Project:\n==============================================================================\nThe LLVM Project contains third party software which is under different license\nterms. All such code will be identified clearly using at least one of two\nmechanisms:\n1) It will be in a separate directory tree with its own `LICENSE.txt` or\n   `LICENSE` file at the top containing the specific license and restrictions\n   which apply to that software, or\n2) It will contain specific license and restriction terms at the top of every\n   file.\n\n==============================================================================\nLegacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy):\n==============================================================================\nUniversity of Illinois/NCSA\nOpen Source License\n\nCopyright (c) 2003-2019 University of Illinois at Urbana-Champaign.\nAll rights reserved.\n\nDeveloped by:\n\n    LLVM Team\n\n    University of Illinois at Urbana-Champaign\n\n    http://llvm.org\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal with\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\n    * Redistributions of source code must retain the above copyright notice,\n      this list of conditions and the following disclaimers.\n\n    * Redistributions in binary form must reproduce the above copyright notice,\n      this list of conditions and the following disclaimers in the\n      documentation and/or other materials provided with the distribution.\n\n    * Neither the names of the LLVM Team, University of Illinois at\n      Urbana-Champaign, nor the names of its contributors may be used to\n      endorse or promote products derived from this Software without specific\n      prior written permission.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nCONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE\nSOFTWARE.\n\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/README.md",
        "content": "# Mojo code examples\n\nA collection of sample programs and Mojo notebooks written in the  \n[Mojo](https://docs.modular.com/mojo/programming-manual.html) programming language.\n\n## Getting Started\n\nAccess a Mojo programming environment available from the  \nMojo product [page](https://www.modular.com/mojo).\n\nGit clone the repository of Mojo samples using the command below:\n\n```bash\ngit clone https://github.com/modularml/mojo.git\n```\n\n## Running\n\nUse the following sample command-line to run the programs:\n\n```bash\nmojo matmul.mojo\n```\n\nYou can run the Mojo notebooks using [JupyterLab or Visual Studio  \nCode](notebooks/README.md) with the Mojo extension available on the Marketplace.\n\n### Mojo SDK Container\n\nThe repo also contains a Dockerfile that can be used to create a  \nMojo SDK container for developing and running Mojo programs. Use the  \ncontainer in conjunction with the Visual Studio Code devcontainers  \nextension to develop directly inside the container.\n\nThe Dockerfile also sets up a `conda` environment and by default,  \nstarts a `jupyter` server (which you can access via the browser).\n\nTo build a Mojo container, either use\n[docker-compose](https://docs.docker.com/compose/) in `mojo/examples/docker`:\n\n```bash\ndocker compose up -d\n```\n\nOr the convenience script provided:\n\n```bash\n./build-image.sh --auth-key <your-modular-auth-key> \\\n   --mojo-version 0.3\n```\n\nThe script also supports building with `podman` instead of `docker`:\n\n```bash\n./build-image.sh --auth-key <your-modular-auth-key> \\\n   --use-podman \\\n   --mojo-version 0.3\n   \n```\n\nYou can then run with either `docker` or `podman`. In the example below,  \nwe map the ports, bind mount the current directory and open a shell into  \nthe container:\n\n```bash\ndocker run \\\n   -it --rm \\\n   -p 8888:8888 \\\n   --net host \\\n   -v ${PWD}:${PWD} \\\n   modular/mojo-v0.3-20232109-1205 bash\n```\n\n`podman` requires an additional argument to add the `SYS_PTRACE` capabilities:\n\n```bash\npodman run \\\n   --cap-add SYS_PTRACE \\\n   -it --rm \\\n   -p 8888:8888 \\\n   --net host \\\n   -v ${PWD}:${PWD} \\\n   modular/mojo-v0.3-20232109-1205 bash\n```\n\n## License\n\nThe Mojo examples and notebooks in this repository are licensed  \nunder the Apache License v2.0 with LLVM Exceptions  \n(see the LLVM [License](https://llvm.org/LICENSE.txt)).\n\n## Contributing\n\nThanks for your interest in contributing to this repository!  \nWe are not accepting pull requests at this time, but are actively  \nworking on a process to accept contributions. Please stay tuned.\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/mojo-matrix-slice.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: code ---\nfrom memory import memset_zero\nfrom algorithm import vectorize, parallelize\nfrom sys.intrinsics import strided_load\nfrom math import trunc, mod\nfrom random import rand\n\nstruct Matrix[dtype: DType = DType.float32]:\n    var dim0: Int\n    var dim1: Int\n    var _data: DTypePointer[dtype]\n    alias simd_width: Int = simdwidthof[dtype]()\n\n    fn __init__(inout self, *dims: Int):\n        self.dim0 = dims[0]\n        self.dim1 = dims[1]\n        self._data = DTypePointer[dtype].alloc(dims[0] * dims[1])\n        rand(self._data, dims[0] * dims[1])\n        \n    fn __copyinit__(inout self, other: Self):\n        self._data = other._data\n        self.dim0 = other.dim0\n        self.dim1 = other.dim1\n\n    fn _adjust_slice_(self, inout span: slice, dim: Int):\n        if span.start < 0:\n            span.start = dim + span.start\n        if not span._has_end():\n            span.end = dim\n        elif span.end < 0:\n            span.end = dim + span.end\n        if span.end > dim:\n            span.end = dim\n        if span.end < span.start:\n            span.start = 0\n            span.end = 0\n\n    fn __getitem__(self, x: Int, y: Int) -> SIMD[dtype,1]:\n        return self._data.simd_load[1](x * self.dim1 + y)\n\n    fn __getitem__(self, owned row_slice: slice, col: Int) -> Self:\n        return self.__getitem__(row_slice, slice(col,col+1))\n\n    fn __getitem__(self, row: Int, owned col_slice: slice) -> Self:\n        return self.__getitem__(slice(row,row+1),col_slice)\n\n    fn __getitem__(self, owned row_slice: slice, owned col_slice: slice) -> Self:\n        self._adjust_slice_(row_slice, self.dim0)\n        self._adjust_slice_(col_slice, self.dim1)\n\n        var src_ptr = self._data\n        var sliced_mat = Self(row_slice.__len__(),col_slice.__len__())\n\n        @parameter\n        fn slice_column(idx_rows: Int):\n            src_ptr = self._data.offset(row_slice[idx_rows]*self.dim1+col_slice[0])\n            @parameter\n            fn slice_row[simd_width: Int](idx: Int) -> None:\n                sliced_mat._data.simd_store[simd_width](idx+idx_rows*col_slice.__len__(),\n                                                        strided_load[dtype,simd_width](src_ptr,col_slice.step))\n                src_ptr = src_ptr.offset(simd_width*col_slice.step)\n            vectorize[self.simd_width,slice_row](col_slice.__len__())\n        parallelize[slice_column](row_slice.__len__(),row_slice.__len__())\n        return sliced_mat\n\n    fn print(self, prec: Int=4)->None:\n        var rank:Int = 2\n        var dim0:Int = 0\n        var dim1:Int = 0\n        var val:SIMD[dtype, 1]=0.0\n        if self.dim0 == 1:\n            rank = 1\n            dim0 = 1\n            dim1 = self.dim1\n        else:\n            dim0 = self.dim0\n            dim1 = self.dim1\n        if dim0>0 and dim1>0:\n            for j in range(dim0):\n                if rank>1:\n                    if j==0:\n                        print_no_newline(\"  [\")\n                    else:\n                        print_no_newline(\"\\n   \")\n                print_no_newline(\"[\")\n                for k in range(dim1):\n                    if rank==1:\n                        val = self._data.simd_load[1](k)\n                    if rank==2:\n                        val = self[j,k]\n                    let int_str: String\n                    if val > 0 or val == 0:\n                        int_str = String(trunc(val).cast[DType.int32]())\n                    else:\n                        int_str = \"-\"+String(trunc(val).cast[DType.int32]())\n                        val = -val\n                    let float_str: String\n                    float_str = String(mod(val,1))\n                    let s = int_str+\".\"+float_str[2:prec+2]\n                    if k==0:\n                        print_no_newline(s)\n                    else:\n                        print_no_newline(\"  \",s)\n                print_no_newline(\"]\")\n            if rank>1:\n                print_no_newline(\"]\")\n            print()\n            if rank>2:\n                print(\"]\")\n        print(\"  Matrix:\",self.dim0,'x',self.dim1,\",\",\"DType:\", dtype.__str__())\n        print()\n\n\nfn main():\n    let mat = Matrix(8,5)\n    mat.print()\n\n    mat[2:4,-3:].print()\n    mat[1:3,:].print()\n    mat[0:3,0:3].print()\n    mat[1::2,::2].print()\n    mat[:,-1:2].print()\n    mat[-1:2,:].print()\n\nmain()\n--- cell type: code ---\nlet mat = Matrix(8,5)\nmat.print()\n--- cell type: code ---\nmat[2:4,-3:].print()\n--- cell type: code ---\nmat[1:3,:].print()\n--- cell type: code ---\nmat[0:3,0:3].print()\n--- cell type: code ---\nmat[1::2,::2].print()\n--- cell type: code ---\nmat[:,-1:2].print()\nmat[-1:2,:].print()"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/mojo-plotter/README.md",
        "content": "# Mojo Plotter\n\n## Installation\n\n### Conda\n\nIf you don't have `conda`, install [miniconda here](https://docs.conda.io/projects/miniconda/en/latest/#quick-command-line-install)\n\n### Create conda environment\n\nCreate and acivate conda enironment:\n\n#### General\n\n```bash\nconda env create -f environment.yaml\nconda activate mojo-plotter\n```\n\n### Auto Set Mojo Environment\n\nTo automatically set Mojo to use the python environment when you activate it:\n\n#### Macos/Linux\n\n```bash\nmkdir -p $CONDA_PREFIX/etc/conda/activate.d\nexport MOJO_PYTHON_LIBRARY=\"$(find $CONDA_PREFIX/lib -iname 'libpython*.[s,d]*' | sort -r | head -n 1)\"\necho \"export MOJO_PYTHON_LIBRARY=\\\"$MOJO_PYTHON_LIBRARY\\\"\" > $CONDA_PREFIX/etc/conda/activate.d/export-mojo.sh\n\nmkdir -p $CONDA_PREFIX/etc/conda/deactivate.d\necho \"unset MOJO_PYTHON_LIBRARY\" > $CONDA_PREFIX/etc/conda/deactivate.d/unset-mojo.sh\n```\n\n## Usage\n\nSimply activate the environment and run the program:\n\n```bash\nconda activate mojo-plotter\nmojo main.mojo\n```\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/mojo-plotter/main.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nfrom python import Python\n\n\nfn main() raises:\n    let torch = Python.import_module(\"torch\")\n    let x = torch.linspace(0, 10, 100)\n    let y = torch.sin(x)\n    plot(x, y)\n\n\ndef plot(x: PythonObject, y: PythonObject) -> None:\n    let plt = Python.import_module(\"matplotlib.pyplot\")\n    plt.plot(x.numpy(), y.numpy())\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Plot of y = sin(x)\")\n    plt.grid(True)\n    plt.show()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/tensorutils/__init__.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nfrom .tensorutils import *\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/tensorutils/tensorutils.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nfrom tensor import Tensor\nfrom math import trunc, mod\n\n\nfn tensorprint[type: DType](t: Tensor[type]) -> None:\n    let rank = t.rank()\n    var dim0: Int = 0\n    var dim1: Int = 0\n    var dim2: Int = 0\n    if rank == 0 or rank > 3:\n        print(\"Error: Tensor rank should be: 1,2, or 3. Tensor rank is \", rank)\n        return\n    if rank == 1:\n        dim0 = 1\n        dim1 = 1\n        dim2 = t.dim(0)\n    if rank == 2:\n        dim0 = 1\n        dim1 = t.dim(0)\n        dim2 = t.dim(1)\n    if rank == 3:\n        dim0 = t.dim(0)\n        dim1 = t.dim(1)\n        dim2 = t.dim(2)\n    var val: SIMD[type, 1] = 0.0\n    for i in range(dim0):\n        if i == 0 and rank == 3:\n            print(\"[\")\n        else:\n            if i > 0:\n                print()\n        for j in range(dim1):\n            if rank != 1:\n                if j == 0:\n                    print_no_newline(\"  [\")\n                else:\n                    print_no_newline(\"\\n   \")\n            print_no_newline(\"[\")\n            for k in range(dim2):\n                if rank == 1:\n                    val = t[k]\n                if rank == 2:\n                    val = t[j, k]\n                if rank == 3:\n                    val = t[i, j, k]\n                let int_str = String(trunc(val).cast[DType.int32]())\n                let float_str = String(mod(val, 1))\n                let s = int_str + \".\" + float_str[2:6]\n                if k == 0:\n                    print_no_newline(s)\n                else:\n                    print_no_newline(\"  \", s)\n            print_no_newline(\"]\")\n        if rank > 1:\n            print_no_newline(\"]\")\n        print()\n    if rank == 3:\n        print(\"]\")\n    print(\n        \"Tensor shape:\",\n        t.shape().__str__(),\n        \", Tensor rank:\",\n        rank,\n        \",\",\n        \"DType:\",\n        type.__str__(),\n    )\n    print()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/blogs-videos/whats_new_v0.5.ipynb",
        "content": "--- cell type: markdown ---\n# What's new in MojoðŸ”¥ SDK v0.5\n--- cell type: markdown ---\n## Keyword parameters\n--- cell type: code ---\nfrom tensor import Tensor\nfrom algorithm import vectorize\n\nstruct SquareMatrix[dtype: DType = DType.float32, dim: Int = 4]():\n  var mat: Tensor[dtype]\n\n  fn __init__(inout self, val: SIMD[dtype,1] = 5):\n    self.mat = Tensor[dtype](self.dim,self.dim)\n    alias simd_width = simdwidthof[dtype]()\n    @parameter\n    fn fill_val[simd_width: Int](idx: Int) -> None:\n        self.mat.simd_store(idx, self.mat.simd_load[simd_width](idx).splat(val))\n    vectorize[simd_width, fill_val](self.mat.num_elements())\n\n  fn __getitem__(self,x:Int,y:Int)->SIMD[dtype,1]:\n    return self.mat[x,y]\n\n  fn print(self):\n    print(self.mat)\n--- cell type: code ---\nSquareMatrix().print()\n--- cell type: code ---\nSquareMatrix(val=12).print()\n--- cell type: code ---\nSquareMatrix[DType.float64](10).print()\n--- cell type: code ---\nSquareMatrix[DType.float64,dim=3](1).print()\n--- cell type: code ---\nSquareMatrix[dtype=DType.float64,dim=3](val=1.5).print()\n--- cell type: markdown ---\nKeyword argument in `__getitem__()`\n--- cell type: code ---\nlet sm = SquareMatrix()\nsm.print()\n\nprint()\nprint('Keyword argument in __getitem__()')\nprint(sm[x=0, y=3])\n--- cell type: markdown ---\n## Automatic parameterization of functions\n--- cell type: markdown ---\n* Parameters are automatically added as input parameters on the function\n* Function argument input parameters can now be referenced within the signature of the function\n--- cell type: code ---\nfrom math import mul\nfn multiply(sm: SquareMatrix, val: SIMD[sm.dtype,1]) -> Tensor[sm.dtype]:\n    alias simd_width: Int = simdwidthof[sm.dtype]()\n    let result_tensor = Tensor[sm.dtype](sm.mat.shape())\n\n    @parameter\n    fn vectorize_multiply[simd_width: Int](idx: Int) -> None:\n        result_tensor.simd_store[simd_width](idx, mul[sm.dtype,simd_width](sm.mat.simd_load[simd_width](idx),val))\n    vectorize[simd_width, vectorize_multiply](sm.mat.num_elements())\n    return result_tensor\n\nfn main():\n    let sm = SquareMatrix(5)\n    let res = multiply(sm,100.0)\n    print(res)\nmain()\n--- cell type: markdown ---\n## Load and save Tensors + String enhancements\n--- cell type: code ---\nfrom tensor import Tensor\nfrom algorithm import vectorize\nfrom time import now\nfrom memory import memcpy\n\nstruct SquareMatrix[dtype: DType = DType.float32, dim: Int = 4]():\n  var mat: Tensor[dtype]\n\n  fn __init__(inout self, val:SIMD[dtype,1] = 5):\n    self.mat = Tensor[dtype](self.dim,self.dim)\n    alias simd_width = simdwidthof[dtype]()\n    @parameter\n    fn fill_val[simd_width: Int](idx: Int) -> None:\n        self.mat.simd_store(idx, self.mat.simd_load[simd_width](idx).splat(val))\n    vectorize[simd_width, fill_val](self.mat.num_elements())\n\n  fn print(self):\n    print(self.mat)\n\n  fn prepare_filename(self, fname: String)->String:\n    var fpath = fname\n    if fpath.count('.') < 2:\n        fpath += '.data'\n    fpath = fpath.replace(\".\",\"_\"+self.mat.spec().__str__()+\".\")\n    if fpath.find('/'):\n        fpath = './'+fpath\n    return fpath\n\n  fn save(self, fname: String='saved_matrix') raises -> String:\n    let fpath = self.prepare_filename(fname)\n    self.mat.tofile(fpath)\n    print('File saved:',fpath)\n    return fpath\n\n  @staticmethod\n  fn load[dtype: DType,dim: Int](fpath:String) raises -> Tensor[dtype]:\n    let load_mat = Tensor[dtype].fromfile(fpath)\n    let new_tensor = Tensor[dtype](dim,dim)\n    memcpy(new_tensor.data(),load_mat.data(),load_mat.num_elements())\n    _ = load_mat\n    return new_tensor\n    \n--- cell type: code ---\nlet m = SquareMatrix()\nm.print()\nlet fpath = m.save('saved_matrix')\n--- cell type: code ---\nprint('Loading Tensor from file:',fpath)\nprint()\nlet load_mat = SquareMatrix.load[DType.float32,4](fpath)\nprint(load_mat)\n--- cell type: markdown ---\n## Benchmark enhancements\n--- cell type: markdown ---\nBenchmark row-wise `mean()` of a matrix by vectorizing across colums and parallelizing across rows\n--- cell type: code ---\nfrom random import rand\nlet tx = rand[DType.float32](5,7)\nprint(tx)\n--- cell type: code ---\nfrom tensor import Tensor\nfrom random import rand\nimport benchmark\nfrom time import sleep\nfrom algorithm import vectorize, parallelize\n\nalias dtype = DType.float32\nalias simd_width = simdwidthof[DType.float32]()\n\nfn row_mean_naive[dtype: DType](t: Tensor[dtype]) -> Tensor[dtype]:\n    var res = Tensor[dtype](t.dim(0),1)\n    for i in range(t.dim(0)):\n        for j in range(t.dim(1)):\n            res[i] += t[i,j]\n        res[i] /= t.dim(1)\n    return res\n\nfn row_mean_fast[dtype: DType](t: Tensor[dtype]) -> Tensor[dtype]:\n    var res = Tensor[dtype](t.dim(0),1)\n    @parameter\n    fn parallel_reduce_rows(idx1: Int)->None:\n        @parameter\n        fn vectorize_reduce_row[simd_width: Int](idx2: Int) -> None:\n            res[idx1] += t.simd_load[simd_width](idx1*t.dim(1)+idx2).reduce_add()\n        vectorize[2*simd_width,vectorize_reduce_row](t.dim(1))\n        res[idx1] /= t.dim(1)\n    parallelize[parallel_reduce_rows](t.dim(0),t.dim(0))\n    return res\n\nfn main():\n    let t = rand[dtype](1000,100000)\n    var result = Tensor[dtype](t.dim(0),1)\n\n    @parameter\n    fn bench_mean():\n        _ = row_mean_naive(t)\n    \n    @parameter\n    fn bench_mean_fast():\n        _ = row_mean_fast(t)\n\n    let report = benchmark.run[bench_mean]()\n    let report_fast = benchmark.run[bench_mean_fast]()\n    report.print()\n    report_fast.print()\n    print(\"Speed up:\",report.mean()/report_fast.mean())\n\nmain()\n--- cell type: markdown ---\n## SIMD enhancements\n--- cell type: code ---\ndef main():\n    alias dtype = DType.float32\n    alias simd_width = simdwidthof[DType.float32]()\n\n    let a = SIMD[dtype].splat(0.5)\n    let b = SIMD[dtype].splat(2.5) \n\n    print(\"SIMD a:\",a)\n    print(\"SIMD b:\",b)\n    print()\n    print(\"SIMD a.join(b):\",a.join(b))\nmain()"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/check_mod.py",
        "content": "import shutil\nimport subprocess\nfrom importlib.util import find_spec\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which(\"python3\"):\n            python = \"python3\"\n        elif shutil.which(\"python\"):\n            python = \"python\"\n        else:\n            raise ImportError(\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/deviceinfo.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample prints the current host system information using APIs from the\n# sys module.\n\nfrom runtime.llcl import num_cores\nfrom sys.info import (\n    os_is_linux,\n    os_is_windows,\n    os_is_macos,\n    has_sse4,\n    has_avx,\n    has_avx2,\n    has_avx512f,\n    has_vnni,\n    has_neon,\n    is_apple_m1,\n    has_intel_amx,\n    _current_target,\n    _current_cpu,\n    _triple_attr,\n)\n\n\ndef main():\n    var os = \"\"\n    if os_is_linux():\n        os = \"linux\"\n    elif os_is_macos():\n        os = \"macOS\"\n    else:\n        os = \"windows\"\n    let cpu = String(_current_cpu())\n    let arch = String(_triple_attr())\n    var cpu_features = String(\"\")\n    if has_sse4():\n        cpu_features += \" sse4\"\n    if has_avx():\n        cpu_features += \" avx\"\n    if has_avx2():\n        cpu_features += \" avx2\"\n    if has_avx512f():\n        cpu_features += \" avx512f\"\n    if has_vnni():\n        if has_avx512f():\n            cpu_features += \" avx512_vnni\"\n        else:\n            cpu_features += \" avx_vnni\"\n    if has_intel_amx():\n        cpu_features += \" intel_amx\"\n    if has_neon():\n        cpu_features += \" neon\"\n    if is_apple_m1():\n        cpu_features += \" Apple M1\"\n\n    print(\"System information: \")\n    print(\"    OS          : \", os)\n    print(\"    CPU         : \", cpu)\n    print(\"    Arch        : \", arch)\n    print(\"    Num Cores   : \", num_cores())\n    print(\"    CPU Features:\", cpu_features)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/hello.ðŸ”¥",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample demonstrates some basic Mojo\n# Range and print functions available as builtins\n\n\ndef main():\n    print(\"Hello Mojo ðŸ”¥!\")\n    for x in range(9, 0, -3):\n        print(x)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/hello_interop.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample demonstrates some basic Mojo\n# Range and print functions available in the standard library\n# It also demonstrates importing a simple Python program into Mojo\n\nfrom python.python import Python\n\n\ndef main():\n    print(\"Hello Mojo ðŸ”¥!\")\n    for x in range(9, 0, -3):\n        print(x)\n    Python.add_to_path(\".\")\n    Python.add_to_path(\"./examples\")\n    let test_module = Python.import_module(\"simple_interop\")\n    test_module.test_interop_func()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/mandelbrot.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\nimport benchmark\nfrom complex import ComplexSIMD, ComplexFloat64\nfrom math import iota\nfrom python import Python\nfrom runtime.llcl import num_cores\nfrom algorithm import parallelize, vectorize\nfrom tensor import Tensor\nfrom utils.index import Index\nfrom python import Python\n\nalias float_type = DType.float64\nalias simd_width = 2 * simdwidthof[float_type]()\n\nalias width = 960\nalias height = 960\nalias MAX_ITERS = 200\n\nalias min_x = -2.0\nalias max_x = 0.6\nalias min_y = -1.5\nalias max_y = 1.5\n\n\nfn mandelbrot_kernel_SIMD[\n    simd_width: Int\n](c: ComplexSIMD[float_type, simd_width]) -> SIMD[float_type, simd_width]:\n    \"\"\"A vectorized implementation of the inner mandelbrot computation.\"\"\"\n    let cx = c.re\n    let cy = c.im\n    var x = SIMD[float_type, simd_width](0)\n    var y = SIMD[float_type, simd_width](0)\n    var y2 = SIMD[float_type, simd_width](0)\n    var iters = SIMD[float_type, simd_width](0)\n\n    var t: SIMD[DType.bool, simd_width] = True\n    for i in range(MAX_ITERS):\n        if not t.reduce_or():\n            break\n        y2 = y * y\n        y = x.fma(y + y, cy)\n        t = x.fma(x, y2) <= 4\n        x = x.fma(x, cx - y2)\n        iters = t.select(iters + 1, iters)\n    return iters\n\n\nfn main() raises:\n    let p = Python.import_module(\"numpy\")\n    let b = Python.import_module(\"numpy\")\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we operate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](\n                row * width + col, mandelbrot_kernel_SIMD[simd_width](c)\n            )\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n    @parameter\n    fn bench[simd_width: Int]():\n        for row in range(height):\n            worker(row)\n\n    let vectorized = benchmark.run[bench[simd_width]]().mean()\n    print(\"Number of threads:\", num_cores())\n    print(\"Vectorized:\", vectorized, \"s\")\n\n    # Parallelized\n    @parameter\n    fn bench_parallel[simd_width: Int]():\n        parallelize[worker](height, height)\n\n    let parallelized = benchmark.run[bench_parallel[simd_width]]().mean()\n    print(\"Parallelized:\", parallelized, \"s\")\n    print(\"Parallel speedup:\", vectorized / parallelized)\n\n    _ = t  # Make sure tensor isn't destroyed before benchmark is finished\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/matmul.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample demonstrates how various systems optimizations can be\n# applied to a naive matmul implementation in Mojo to gain significant\n# performance speedups\n\nimport benchmark\nfrom memory import memset_zero, stack_allocation\nfrom random import rand\nfrom algorithm import vectorize, parallelize, vectorize_unroll\nfrom algorithm import Static2DTileUnitFunc as Tile2DFunc\nfrom python import Python\nfrom tensor import Tensor\nfrom utils.index import Index\nfrom memory.buffer import NDBuffer\n\nalias M = 512\nalias N = 512\nalias K = 4096\nalias type = DType.float32\n\n\nstruct Matrix:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    # Initialize zeroeing all values\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        memset_zero(self.data, rows * cols)\n        self.rows = rows\n        self.cols = cols\n\n    # Initialize taking a pointer, don't set any elements\n    fn __init__(inout self, rows: Int, cols: Int, data: DTypePointer[DType.float32]):\n        self.data = data\n        self.rows = rows\n        self.cols = cols\n\n    ## Initialize with random values\n    @staticmethod\n    fn rand(rows: Int, cols: Int) -> Self:\n        let data = DTypePointer[type].alloc(rows * cols)\n        rand(data, rows * cols)\n        return Self(rows, cols, data)\n\n    fn __getitem__(self, y: Int, x: Int) -> Float32:\n        return self.load[1](y, x)\n\n    fn __setitem__(self, y: Int, x: Int, val: Float32):\n        return self.store[1](y, x, val)\n\n    fn load[nelts: Int](self, y: Int, x: Int) -> SIMD[DType.float32, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    fn store[nelts: Int](self, y: Int, x: Int, val: SIMD[DType.float32, nelts]):\n        return self.data.simd_store[nelts](y * self.cols + x, val)\n\n\ndef run_matmul_python() -> Float64:\n    Python.add_to_path(\".\")\n    let pymatmul: PythonObject = Python.import_module(\"pymatmul\")\n    let py = Python.import_module(\"builtins\")\n\n    let gflops = pymatmul.benchmark_matmul_python(128, 128, 128).to_float64()\n    py.print(py.str(\"{:<13}{:>8.3f} GFLOPS\").format(\"Python:\", gflops))\n\n    return gflops\n\n\ndef run_matmul_numpy() -> Float64:\n    let pymatmul: PythonObject = Python.import_module(\"pymatmul\")\n    let py = Python.import_module(\"builtins\")\n\n    let gflops = pymatmul.benchmark_matmul_numpy(M, N, K).to_float64()\n    py.print(py.str(\"{:<13}{:>8.3f} GFLOPS\").format(\"Numpy:\", gflops))\n\n    return gflops\n\n\nfn naive(inout C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n\n\n# Mojo has SIMD vector types, we can vectorize the Matmul code as follows.\nalias nelts = simdwidthof[type]()  # The SIMD vector width.\n\n\n# Using stdlib vectorize function\nfn vectorized(inout C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n\n            @parameter\n            fn dot[nelts: Int](n: Int):\n                C.store[nelts](\n                    m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n)\n                )\n\n            vectorize[nelts, dot](C.cols)\n\n\n# Parallelize the code by using the builtin parallelize function\nfn parallelized(inout C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        for k in range(A.cols):\n\n            @parameter\n            fn dot[nelts: Int](n: Int):\n                C.store[nelts](\n                    m, n, C.load[nelts](m, n) + A[m, k] * B.load[nelts](k, n)\n                )\n\n            vectorize[nelts, dot](C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n\n\n# Perform 2D tiling on the iteration space defined by end_x and end_y.\nfn tile[tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    for y in range(0, end_y, tile_y):\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n\n\n# Use the above tile function to perform tiled matmul.\nfn tiled(inout C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n\n                @parameter\n                fn dot[\n                    nelts: Int,\n                ](n: Int):\n                    C.store[nelts](\n                        m,\n                        n + x,\n                        C.load[nelts](m, n + x) + A[m, k] * B.load[nelts](k, n + x),\n                    )\n\n                vectorize[nelts, dot](tile_x)\n\n        # We hardcode the tile factor to be 4.\n        alias tile_size = 4\n        tile[calc_tile, nelts * tile_size, tile_size](C.cols, B.rows)\n\n    parallelize[calc_row](C.rows, C.rows)\n\n\n# Unroll the vectorized loop by a constant factor.\n# from Functional import vectorize_unroll\nfn unrolled(inout C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n\n                @parameter\n                fn dot[\n                    nelts: Int,\n                ](n: Int):\n                    C.store[nelts](\n                        m,\n                        n + x,\n                        C.load[nelts](m, n + x) + A[m, k] * B.load[nelts](k, n + x),\n                    )\n\n                # Vectorize by nelts and unroll by tile_x/nelts\n                # Here unroll factor is 4\n                vectorize_unroll[nelts, tile_x // nelts, dot](tile_x)\n\n        alias tile_size = 4\n        tile[calc_tile, nelts * tile_size, tile_size](C.cols, B.rows)\n\n    parallelize[calc_row](C.rows, C.rows)\n\n\n# Perform 2D tiling on the iteration space defined by end_x and end_y, parallelizing over y.\nfn tile_parallel[\n    tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int\n](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    @parameter\n    fn row(yo: Int):\n        let y = tile_y * yo\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n\n    parallelize[row](end_y // tile_y, M)\n\n\n# Use stack allocation for tiles to accumulate values efficiently,\n# avoiding repeated reads and writes to memory. Also reorder the loops\n# and do not fully unroll the loop over the reduction dimension.\nfn reordered(inout C: Matrix, A: Matrix, B: Matrix):\n    alias tile_k = 8\n    alias tile_k_unroll = 8\n\n    @parameter\n    fn calc_tile[tile_j: Int, tile_i: Int](jo: Int, io: Int):\n        # Allocate the tile of accumulators on the stack.\n        var accumulators = Matrix(\n            tile_i, tile_j, stack_allocation[tile_i * tile_j, DType.float32]()\n        )\n\n        for ko in range(0, A.cols, tile_k * tile_k_unroll):\n\n            @parameter\n            fn calc_tile_row[](i: Int):\n                for i in range(0, tile_k):\n\n                    @parameter\n                    fn calc_tile_inner[k: Int]():\n                        @parameter\n                        fn calc_tile_cols[nelts: Int](j: Int):\n                            accumulators.store[nelts](\n                                i,\n                                j,\n                                accumulators.load[nelts](i, j)\n                                + A[io + i, ko + k] * B.load[nelts](ko + k, jo + j),\n                            )\n\n                        vectorize_unroll[nelts, tile_j // nelts, calc_tile_cols](tile_j)\n\n                    unroll[tile_k_unroll, calc_tile_inner]()\n\n            for i in range(0, tile_i):\n                calc_tile_row(i)\n\n        # Copy the local tile to the output\n        for i in range(tile_i):\n            for j in range(tile_j):\n                C[io + i, jo + j] = accumulators[i, j]\n\n    alias tile_i = 32\n    alias tile_j = nelts * 4\n    tile_parallel[calc_tile, tile_j, tile_i](C.cols, C.rows)\n\n\n# Perform 2D tiling on the iteration space defined by end_x and end_y, parallelizing\n# over x and y, and iterating in an order that has better L3 cache locality\nfn tile_parallel_swizzled[\n    tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int\n](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    alias tile_outer = 8\n    alias group_size = tile_outer * 4\n\n    # L3 cache swizzling\n    @parameter\n    fn row(swizzled: Int):\n        let group_id = swizzled // group_size\n        let group_offset_x = (group_id * tile_outer) % (N // tile_y)\n        let yo = (swizzled % group_size) // tile_outer\n        let xo = group_offset_x + (swizzled % tile_outer)\n        let y = tile_y * yo\n        let x = tile_x * xo\n        tiled_fn[tile_x, tile_y](x, y)\n\n    parallelize[row]((end_y // tile_y * end_x // tile_x), M * 2)\n\n\n# Same as previous example but utilisizing tile swizzling for better L3 cache locality.\nfn swizzled(inout C: Matrix, A: Matrix, B: Matrix):\n    alias tile_k = 8\n    alias tile_k_unroll = 8\n\n    @parameter\n    fn calc_tile[tile_j: Int, tile_i: Int](jo: Int, io: Int):\n        # Allocate the tile of accumulators on the stack.\n        var accumulators = Matrix(\n            tile_i, tile_j, stack_allocation[tile_i * tile_j, DType.float32]()\n        )\n\n        for ko in range(0, A.cols, tile_k * tile_k_unroll):\n\n            @parameter\n            fn calc_tile_row[](i: Int):\n                for i in range(0, tile_k):\n\n                    @parameter\n                    fn calc_tile_inner[k: Int]():\n                        @parameter\n                        fn calc_tile_cols[nelts: Int](j: Int):\n                            accumulators.store[nelts](\n                                i,\n                                j,\n                                accumulators.load[nelts](i, j)\n                                + A[io + i, ko + k] * B.load[nelts](ko + k, jo + j),\n                            )\n\n                        vectorize_unroll[nelts, tile_j // nelts, calc_tile_cols](tile_j)\n\n                    unroll[tile_k_unroll, calc_tile_inner]()\n\n            for i in range(0, tile_i):\n                calc_tile_row(i)\n\n        # Copy the local tile to the output\n        for i in range(tile_i):\n            for j in range(tile_j):\n                C[io + i, jo + j] = accumulators[i, j]\n\n    alias tile_i = 32\n    alias tile_j = nelts * 4\n    tile_parallel_swizzled[calc_tile, tile_j, tile_i](C.cols, C.rows)\n\n\n@always_inline\nfn bench[\n    func: fn (inout Matrix, Matrix, Matrix) -> None, name: StringLiteral\n](base_gflops: Float64, numpy_gflops: Float64) raises:\n    var A = Matrix.rand(M, K)\n    var B = Matrix.rand(K, N)\n    var C = Matrix(M, N)\n\n    @always_inline\n    @parameter\n    fn test_fn():\n        _ = func(C, A, B)\n\n    let secs = benchmark.run[test_fn]().mean()\n    # Prevent the matrices from being freed before the benchmark run\n    A.data.free()\n    B.data.free()\n    C.data.free()\n    let gflops = ((2 * M * N * K) / secs) / 1e9\n    let speedup: Float64 = gflops / base_gflops\n    let numpy_speedup: Float64 = gflops / numpy_gflops\n\n    let py = Python.import_module(\"builtins\")\n    _ = py.print(\n        py.str(\"{:<13}{:>8.3f} GFLOPS {:>9.2f}x Python {:>5.2f}x Numpy\").format(\n            name, gflops, speedup, numpy_speedup\n        )\n    )\n\n\n@always_inline\nfn test[\n    func: fn (inout Matrix, Matrix, Matrix) -> None\n](A: Matrix, B: Matrix) raises -> SIMD[type, 1]:\n    var C = Matrix(M, N)\n    _ = func(C, A, B)\n    var result = SIMD[type, 1]()\n    for i in range(C.rows):\n        for j in range(C.cols):\n            result += C[i, j]\n    return result\n\n\nfn test_all() raises:\n    constrained[M == N, \"M and N must be equal for matrix multiplication\"]()\n\n    let A = Matrix.rand(M, K)\n    let B = Matrix.rand(K, N)\n\n    let result = test[naive](A, B)\n\n    if test[vectorized](A, B) != result:\n        raise Error(\"Vectorize output does not match\")\n    if test[parallelized](A, B) != result:\n        raise Error(\"Parallelize output incorrect\")\n    if test[tiled](A, B) != result:\n        raise Error(\"Tiled output incorrect\")\n    if test[unrolled](A, B) != result:\n        raise Error(\"Unroll output incorrect\")\n    if test[reordered](A, B) != result:\n        raise Error(\"Loop reorder output incorrect\")\n    if test[swizzled](A, B) != result:\n        raise Error(\"Swizzled output incorrect\")\n\n    A.data.free()\n    B.data.free()\n\n\nfn main() raises:\n    # Uncomment below to test correctness of Matmuls\n    # test_all()\n    print(\"CPU Results\\n\")\n    let python_gflops = run_matmul_python()\n    let numpy_gflops = run_matmul_numpy()\n\n    bench[naive, \"Naive:\"](python_gflops, numpy_gflops)\n    bench[vectorized, \"Vectorized:\"](python_gflops, numpy_gflops)\n    bench[parallelized, \"Parallelized:\"](python_gflops, numpy_gflops)\n    bench[tiled, \"Tiled:\"](python_gflops, numpy_gflops)\n    bench[unrolled, \"Unrolled:\"](python_gflops, numpy_gflops)\n    bench[reordered, \"Reordered:\"](python_gflops, numpy_gflops)\n    bench[swizzled, \"Swizzled:\"](python_gflops, numpy_gflops)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/memset.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample implements various memset algorithms and optimizations\n\nfrom autotune import autotune_fork\nfrom math import min, max\nfrom time import time_function\nfrom memory import memset as stdlib_memset\nfrom benchmark import keep\n\nalias type = UInt8\nalias ptr_type = DTypePointer[DType.uint8]\nalias fn_type = fn (ptr_type, type, Int) -> None\n\n\nfn measure_time(func: fn_type, size: Int, iters: Int, samples: Int) -> Int:\n    alias alloc_size = 1024 * 1024\n    let ptr = ptr_type.alloc(alloc_size)\n\n    var best = -1\n    for sample in range(samples):\n\n        @parameter\n        fn runner():\n            for iter in range(iters):\n                # Offset pointer to shake up cache a bit\n                let offset_ptr = ptr.offset((iter * 128) & 1024)\n\n                # memset, change the value we're filling with\n                let v = type(iter&255)\n\n                # Actually call the memset function\n                func(offset_ptr, v.value, size)\n\n                # Avoid compiler optimizing things away\n                keep(v)\n                keep(size)\n                keep(offset_ptr)\n\n        let ns = time_function[runner]()\n        if best < 0 or ns < best:\n            best = ns\n\n    ptr.free()\n    return best\n\n\nalias MULT = 2_000\n\n\nfn visualize_result(size: Int, result: Int):\n    print_no_newline(\"Size: \")\n    if size < 10:\n        print_no_newline(\" \")\n    print_no_newline(size, \"  |\")\n    for _ in range(result // MULT):\n        print_no_newline(\"*\")\n    print()\n\n\nfn benchmark(func: fn_type, title: StringRef):\n    print(\"\\n=====================\")\n    print(title)\n    print(\"---------------------\\n\")\n\n    alias benchmark_iterations = 30 * MULT\n    alias warmup_samples = 10\n    alias benchmark_samples = 1000\n\n    # Warmup\n    for size in range(35):\n        _ = measure_time(func, size, benchmark_iterations, warmup_samples)\n\n    # Actual run\n    for size in range(35):\n        let result = measure_time(\n            func, size, benchmark_iterations, benchmark_samples\n        )\n\n        visualize_result(size, result)\n\n\n@always_inline\nfn overlapped_store[width: Int](ptr: ptr_type, value: type, count: Int):\n    let v = SIMD[DType.uint8, width].splat(value)\n    ptr.simd_store[width](v)\n    ptr.simd_store[width](count - width, v)\n\n\nfn memset_manual(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count <= 16:\n            if count >= 8:\n                # 8 <= count < 16\n                overlapped_store[8](ptr, value, count)\n                return\n            # 4 < count < 8\n            overlapped_store[4](ptr, value, count)\n            return\n\n        # 16 <= count < 32\n        overlapped_store[16](ptr, value, count)\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n\n\nfn memset_system(ptr: ptr_type, value: type, count: Int):\n    stdlib_memset(ptr, value.value, count)\n\n\nfn memset_manual_2(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count >= 16:\n            # 16 <= count < 32\n            overlapped_store[16](ptr, value, count)\n            return\n\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count >= 8:\n            # 8 <= count < 16\n            overlapped_store[8](ptr, value, count)\n            return\n        # 4 < count < 8\n        overlapped_store[4](ptr, value, count)\n\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n\n\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    @parameter\n    if lower == -100 and upper == 0:\n        pass\n    elif lower == 0 and upper == 4:\n        ptr.store(0, value)\n        ptr.store(count - 1, value)\n        if count <= 2:\n            return\n        ptr.store(1, value)\n        ptr.store(count - 2, value)\n    elif lower == 4 and upper == 8:\n        overlapped_store[4](ptr, value, count)\n    elif lower == 8 and upper == 16:\n        overlapped_store[8](ptr, value, count)\n    elif lower == 16 and upper == 32:\n        overlapped_store[16](ptr, value, count)\n    elif lower == 32 and upper == 100:\n        memset_system(ptr, value, count)\n    else:\n        constrained[False]()\n\n\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count > cur:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n    else:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n\n\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count <= cur:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n    else:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n\n\nfn memset_evaluator(funcs: Pointer[fn_type], size: Int) -> Int:\n    # This size is picked at random, in real code we could use a real size\n    # distribution here.\n    let size_to_optimize_for = 17\n    print(\"Optimizing for size: \", size_to_optimize_for)\n\n    var best_idx: Int = -1\n    var best_time: Int = -1\n\n    alias eval_iterations = MULT\n    alias eval_samples = 500\n\n    # Find the function that's the fastest on the size we're optimizing for\n    for f_idx in range(size):\n        let func = funcs.load(f_idx)\n        let cur_time = measure_time(\n            func, size_to_optimize_for, eval_iterations, eval_samples\n        )\n        if best_idx < 0:\n            best_idx = f_idx\n            best_time = cur_time\n        if best_time > cur_time:\n            best_idx = f_idx\n            best_time = cur_time\n\n    return best_idx\n\n\nfn main():\n    benchmark(memset_manual, \"Manual memset\")\n    benchmark(memset_system, \"System memset\")\n    benchmark(memset_manual_2, \"Manual memset v2\")\n    benchmark(memset_system, \"Mojo system memset\")\n    benchmark(memset_manual, \"Mojo manual memset\")\n    benchmark(memset_manual_2, \"Mojo manual memset v2\")\n    benchmark(memset_system, \"Mojo system memset\")\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/nbody.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample implements the nbody benchmarking in\n# https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/nbody.html\n\nfrom utils.index import StaticTuple\nfrom math import sqrt\nfrom benchmark import run\n\nalias PI = 3.141592653589793\nalias SOLAR_MASS = 4 * PI * PI\nalias DAYS_PER_YEAR = 365.24\n\n\n@register_passable(\"trivial\")\nstruct Planet:\n    var pos: SIMD[DType.float64, 4]\n    var velocity: SIMD[DType.float64, 4]\n    var mass: Float64\n\n    fn __init__(\n        pos: SIMD[DType.float64, 4],\n        velocity: SIMD[DType.float64, 4],\n        mass: Float64,\n    ) -> Self:\n        return Self {\n            pos: pos,\n            velocity: velocity,\n            mass: mass,\n        }\n\n\nalias NUM_BODIES = 5\n\n\nfn offset_momentum(inout bodies: StaticTuple[NUM_BODIES, Planet]):\n    var p = SIMD[DType.float64, 4]()\n\n    @unroll\n    for i in range(NUM_BODIES):\n        p += bodies[i].velocity * bodies[i].mass\n\n    var body = bodies[0]\n    body.velocity = -p / SOLAR_MASS\n\n    bodies[0] = body\n\n\nfn advance(inout bodies: StaticTuple[NUM_BODIES, Planet], dt: Float64):\n    @unroll\n    for i in range(NUM_BODIES):\n        for j in range(NUM_BODIES - i - 1):\n            var body_i = bodies[i]\n            var body_j = bodies[j + i + 1]\n            let diff = body_i.pos - body_j.pos\n            let diff_sqr = (diff * diff).reduce_add()\n            let mag = dt / (diff_sqr * sqrt(diff_sqr))\n\n            body_i.velocity -= diff * body_j.mass * mag\n            body_j.velocity += diff * body_i.mass * mag\n\n            bodies[i] = body_i\n            bodies[j + i + 1] = body_j\n\n    @unroll\n    for i in range(NUM_BODIES):\n        var body = bodies[i]\n        body.pos += dt * body.velocity\n        bodies[i] = body\n\n\nfn energy(bodies: StaticTuple[NUM_BODIES, Planet]) -> Float64:\n    var e: Float64 = 0\n\n    @unroll\n    for i in range(NUM_BODIES):\n        let body_i = bodies[i]\n        e += (\n            0.5\n            * body_i.mass\n            * ((body_i.velocity * body_i.velocity).reduce_add())\n        )\n\n        for j in range(NUM_BODIES - i - 1):\n            let body_j = bodies[j + i + 1]\n            let diff = body_i.pos - body_j.pos\n            let distance = sqrt((diff * diff).reduce_add())\n            e -= (body_i.mass * body_j.mass) / distance\n\n    return e\n\n\nfn _run():\n    let Sun = Planet(\n        0,\n        0,\n        SOLAR_MASS,\n    )\n\n    let Jupiter = Planet(\n        SIMD[DType.float64, 4](\n            4.84143144246472090e00,\n            -1.16032004402742839e00,\n            -1.03622044471123109e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            1.66007664274403694e-03 * DAYS_PER_YEAR,\n            7.69901118419740425e-03 * DAYS_PER_YEAR,\n            -6.90460016972063023e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        9.54791938424326609e-04 * SOLAR_MASS,\n    )\n\n    let Saturn = Planet(\n        SIMD[DType.float64, 4](\n            8.34336671824457987e00,\n            4.12479856412430479e00,\n            -4.03523417114321381e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            -2.76742510726862411e-03 * DAYS_PER_YEAR,\n            4.99852801234917238e-03 * DAYS_PER_YEAR,\n            2.30417297573763929e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        2.85885980666130812e-04 * SOLAR_MASS,\n    )\n\n    let Uranus = Planet(\n        SIMD[DType.float64, 4](\n            1.28943695621391310e01,\n            -1.51111514016986312e01,\n            -2.23307578892655734e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            2.96460137564761618e-03 * DAYS_PER_YEAR,\n            2.37847173959480950e-03 * DAYS_PER_YEAR,\n            -2.96589568540237556e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        4.36624404335156298e-05 * SOLAR_MASS,\n    )\n\n    let Neptune = Planet(\n        SIMD[DType.float64, 4](\n            1.53796971148509165e01,\n            -2.59193146099879641e01,\n            1.79258772950371181e-01,\n            0,\n        ),\n        SIMD[DType.float64, 4](\n            2.68067772490389322e-03 * DAYS_PER_YEAR,\n            1.62824170038242295e-03 * DAYS_PER_YEAR,\n            -9.51592254519715870e-05 * DAYS_PER_YEAR,\n            0,\n        ),\n        5.15138902046611451e-05 * SOLAR_MASS,\n    )\n    var system = StaticTuple[NUM_BODIES, Planet](\n        Sun, Jupiter, Saturn, Uranus, Neptune\n    )\n    offset_momentum(system)\n\n    print(\"Energy of System:\", energy(system))\n\n    for i in range(50_000_000):\n        advance(system, 0.01)\n\n    print(\"Energy of System:\", energy(system))\n\n\nfn benchmark():\n    print(run[_run]().mean())\n\n\nfn main():\n    print(\"Starting nbody...\")\n    _run()\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/BoolMLIR.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# Low-level IR in Mojo\n--- cell type: markdown ---\nMojo is a high-level programming language with an extensive set of modern features. Mojo also provides you, the programmer, access to all of the low-level primitives that you need to write powerful -- yet zero-cost -- abstractions.\n\nThese primitives are implemented in [MLIR](https://mlir.llvm.org), an extensible intermediate representation (IR) format for compiler design. Many different programming languages and compilers translate their source programs into MLIR, and because Mojo provides direct access to MLIR features, this means Mojo programs can enjoy the benefits of each of these tools.\n\nGoing one step further, Mojo's unique combination of zero-cost abstractions with MLIR interoperability means that Mojo programs can take full advantage of *anything* that interfaces with MLIR. While this isn't something normal Mojo programmers may ever need to do, it's an extremely powerful capability when extending a system to interface with a new datatype, or an esoteric new accelerator feature.\n\nTo illustrate these ideas, we'll implement a boolean type in Mojo below, which we'll call `OurBool`. We'll make extensive use of MLIR, so let's begin with a short primer.\n\n--- cell type: markdown ---\n## What is MLIR?\n\nMLIR is an intermediate representation of a program, not unlike an assembly language, in which a sequential set of instructions operate on in-memory values.\n\nMore importantly, MLIR is modular and extensible. MLIR is composed of an ever-growing number of \"dialects.\" Each dialect defines operations and optimizations: for example, the ['math' dialect](https://mlir.llvm.org/docs/Dialects/MathOps/) provides mathematical operations such as sine and cosine, the ['amdgpu' dialect](https://mlir.llvm.org/docs/Dialects/AMDGPU/) provides operations specific to AMD processors, and so on.\n\nEach of MLIR's dialects can interoperate with the others. This is why MLIRÂ is said to unlock heterogeneous compute: as newer, faster processors and architectures are developed, new MLIR dialects are implemented to generate optimal code for those environments. Any new MLIR dialect can be translated seamlessly into other dialects, so as more get added, all existing MLIR becomes more powerful.\n\nThis means that our own custom types, such as the `OurBool` type we'll create below, can be used to provide programmers with a high-level, Python-like interface. But \"under the covers,\" Mojo and MLIR will optimize our convenient, high-level types for each new processor that appears in the future.\n\nThere's much more to write about why MLIR is such a revolutionary technology, but let's get back to Mojo and defining the `OurBool` type. There will be opportunities to learn more about MLIR along the way.\n--- cell type: markdown ---\n## Defining the `OurBool` type\n\nWe can use Mojo's `struct` keyword to define a new type `OurBool`:\n--- cell type: code ---\nstruct OurBool:\n    var value: __mlir_type.i1\n--- cell type: markdown ---\nA boolean can represent 0 or 1, \"true\" or \"false.\" To store this information, `OurBool` has a single member, called `value`. Its type is represented *directly in MLIR*, using the MLIR builtin type [`i1`](https://mlir.llvm.org/docs/Dialects/Builtin/#integertype). In fact, you can use any MLIR type in Mojo, by prefixing the type name with `__mlir_type`.\n\nAs we'll see below, representing our boolean value with `i1` will allow us to utilize all of the MLIR operations and optimizations that interface with the `i1` type -- and there are many of them!\n\nHaving defined `OurBool`, we can now declare a variable of this type:\n--- cell type: code ---\nlet a: OurBool\n--- cell type: markdown ---\n## Leveraging MLIR\n\nNaturally, we might next try to create an instance of `OurBool`. Attempting to do so at this point, however, results in an error:\n\n```mojo\nlet a = OurBool() # error: 'OurBool' does not implement an '__init__' method\n```\n\nAs in Python, `__init__` is a [special method](https://docs.python.org/3/reference/datamodel.html#specialnames) that can be defined to customize the behavior of a type. We can implement an `__init__` method that takes no arguments, and returns an `OurBool` with a \"false\" value.\n--- cell type: code ---\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(inout self):\n        self.value = __mlir_op.`index.bool.constant`[\n            value=__mlir_attr.`false`,\n        ]()\n--- cell type: markdown ---\nTo initialize the underlying `i1` value, we use an MLIR operation from its ['index' dialect](https://mlir.llvm.org/docs/Dialects/IndexOps/), called [`index.bool.constant`](https://mlir.llvm.org/docs/Dialects/IndexOps/#indexboolconstant-mlirindexboolconstantop).\n\nMLIR's 'index' dialect provides us with operations for manipulating builtin MLIR types, such as the `i1` we use to store the value of `OurBool`. The `index.bool.constant` operation takes a `true` or `false` compile-time constant as input, and produces a runtime output of type `i1` with the given value.\n\nSo, as shown above, in addition to any MLIR type, Mojo also provides direct access to any MLIR operation via the `__mlir_op` prefix, and to any attribute via the `__mlir_attr` prefix. MLIR attributes are used to represent compile-time constants.\n\nAs you can see above, the syntax for interacting with MLIR isn't always pretty: MLIR attributes are passed in between square brackets `[...]`, and the operation is executed via a parentheses suffix `(...)`, which can take runtime argument values. However, most Mojo programmers will not need to access MLIR directly, and for the few that do, this \"ugly\" syntax gives them superpowers: they can define high-level types that are easy to use, but that internally plug into MLIR and its powerful system of dialects.\n\nWe think this is very exciting, but let's bring things back down to earth: having defined an `__init__` method, we can now create an instance of our `OurBool` type:\n--- cell type: code ---\nlet b = OurBool()\n--- cell type: markdown ---\n## Value semantics in Mojo\n\nWe can now instantiate `OurBool`, but using it is another story:\n\n```mojo\nlet a = OurBool()\nlet b = a # error: 'OurBool' does not implement the '__copyinit__' method\n```\n\nMojo uses \"value semantics\" by default, meaning that it expects to create a copy of `a` when assigning to `b`. However, Mojo doesn't make any assumptions about *how* to copy `OurBool`, or its underlying `i1` value. The error indicates that we should implement a `__copyinit__` method, which would implement the copying logic.\n\nIn our case, however, `OurBool` is a very simple type, with only one \"trivially copyable\" member. We can use a decorator to tell the Mojo compiler that, saving us the trouble of defining our own `__copyinit__` boilerplate. Trivially copyable types must implement an `__init__` method that returns an instance of themselves, so we must also rewrite our initializer slightly.\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__() -> Self:\n        return Self {\n            value: __mlir_op.`index.bool.constant`[\n                value=__mlir_attr.`false`,\n            ]()\n        }\n--- cell type: markdown ---\nWe can now copy `OurBool` as we please:\n--- cell type: code ---\nlet c = OurBool()\nlet d = c\n--- cell type: markdown ---\n## Compile-time constants\n\nIt's not very useful to have a boolean type that can only represent \"false.\" Let's define compile-time constants that represent true and false `OurBool` values.\n\nFirst, let's define another `__init__` constructor for `OurBool` that takes its `i1` value as an argument:\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    # ...\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n--- cell type: markdown ---\nThis allows us to define compile-time constant `OurBool` values, using the `alias` keyword. First, let's define `OurTrue`:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\n--- cell type: markdown ---\nHere we're passing in an MLIR compile-time constant value of `true`, which has the `i1` type that our new `__init__` constructor expects. We can use a slightly different syntax for `OurFalse`:\n--- cell type: code ---\nalias OurFalse: OurBool = __mlir_attr.`false`\n--- cell type: markdown ---\n`OurFalse` is declared to be of type `OurBool`, and then assigned an `i1` type -- in this case, the `OurBool` constructor we added is called implicitly.\n\nWith true and false constants, we can also simplify our original `__init__` constructor for `OurBool`. Instead of constructing an MLIR value, we can simply return our `OurFalse` constant:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    # We can simplify our no-argument constructor:\n    fn __init__() -> Self:\n        return OurFalse\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n--- cell type: markdown ---\nNote also that we can define `OurTrue` before we define `OurBool`. The Mojo compiler is smart enough to figure this out.\n\nWith these constants, we can now define variables with both true and false values of `OurBool`:\n--- cell type: code ---\nlet e = OurTrue\nlet f = OurFalse\n--- cell type: markdown ---\n## Implementing `__bool__`\n\nOf course, the reason booleans are ubiquitous in programming is because they can be used for program control flow. However, if we attempt to use `OurBool` in this way, we get an error:\n\n```mojo\nlet a = OurTrue\nif a: print(\"It's true!\") # error: 'OurBool' does not implement the '__bool__' method\n```\n\nWhen Mojo attempts to execute our program, it needs to be able to determine whether to print \"It's true!\" or not. It doesn't yet know that `OurBool` represents a boolean value -- Mojo just sees a struct that is 1 bit in size. However, Mojo also provides interfaces that convey booleanÂ qualities, which are the same as those used by Mojo's standard library types, like `Bool`. In practice, this means Mojo gives you full control: any type that's packaged with the language's standard library is one for which you could define your own version.\n\nIn the case of our error message, Mojo is telling us that implementing a `__bool__` method on `OurBool` would signify that it has boolean qualities.\n\nThankfully, `__bool__` is simple to implement: Mojo's standard library and builtin types are all implemented on top of MLIR, and so the builtin `Bool` type also defines a constructor that takes an `i1`, just like `OurBool`:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    # ...\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n\n    # Our new method converts `OurBool` to `Bool`:\n    fn __bool__(self) -> Bool:\n        return Bool(self.value)\n--- cell type: markdown ---\nNow we can use `OurBool` anywhere we can use the builtin `Bool` type:\n--- cell type: code ---\nlet g = OurTrue\nif g: print(\"It's true!\")\n--- cell type: markdown ---\n## Avoiding type conversion with `__mlir_i1__`\n\nThe `OurBool` type is looking great, and by providing a conversion to `Bool`,\nit can be used anywhere the builtin `Bool` type can. But we promised you \"full\ncontrol,\" and the ability to define your own version of any type built into\nMojo or its standard library. So, why do we depend on `__bool__` to convert our\ntype into `Bool` (the standard library type)? This is just the formal way for\nMojo to evaluate a type as a boolean, which is useful for real-world scenarios.\nHowever, to define a boolean type from scratch, you have a more low-level\noption.\n\nWhen Mojo evaluates a conditional expression, it actually attempts to convert\nthe expression to an MLIR `i1` value, by searching for the special interface\nmethod `__mlir_i1__`. (The automatic conversion to `Bool` occurs because `Bool`\nis known to implement the `__mlir_i1__` method.)\n\nThus, by implementing the `__mlir_i1__` special methods in `OurBool`, we can\ncreate a type that can replaces `Bool` entirely:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n\n    # Our new method converts `OurBool` to `i1`:\n    fn __mlir_i1__(self) -> __mlir_type.i1:\n        return self.value\n--- cell type: markdown ---\nWe can still use `OurBool` in conditionals just as we did before:\n--- cell type: code ---\nlet h = OurTrue\nif h: print(\"No more Bool conversion!\")\n--- cell type: markdown ---\nBut this time, no conversion to `Bool` occurs. You can try adding `print` statements to the `__bool__` and `__mlir_i1__` methods to see for yourself.\n--- cell type: markdown ---\n## Adding functionality with MLIR\n\nThere are many more ways we can improve `OurBool`. Many of those involve implementing special methods, some of which you may recognize from Python, and some which are specific to Mojo. For example, we can implement inversion of a `OurBool` value by adding a `__invert__` method. We can also add an `__eq__` method, which allows two `OurBool` to be compared with the `==` operator.\n\nWhat sets Mojo apart is the fact that we can implement each of these using MLIR. To implement `__eq__`, for example, we use the [`index.casts`](https://mlir.llvm.org/docs/Dialects/IndexOps/#indexcasts-mlirindexcastsop) operation to cast our `i1` values to the MLIR index dialect's `index` type, and then the [`index.cmp`](https://mlir.llvm.org/docs/Dialects/IndexOps/#indexcmp-mlirindexcmpop) operation to compare them for equality. And with the `__eq__` method implemented, we can then implement `__invert__` in terms of `__eq__`:\n--- cell type: code ---\nalias OurTrue = OurBool(__mlir_attr.`true`)\nalias OurFalse: OurBool = __mlir_attr.`false`\n\n\n@register_passable(\"trivial\")\nstruct OurBool:\n    var value: __mlir_type.i1\n\n    fn __init__(value: __mlir_type.i1) -> Self:\n        return Self {value: value}\n\n    # ...\n\n    fn __mlir_i1__(self) -> __mlir_type.i1:\n        return self.value\n\n    fn __eq__(self, rhs: OurBool) -> Self:\n        let lhsIndex = __mlir_op.`index.casts`[_type=__mlir_type.index](\n            self.value\n        )\n        let rhsIndex = __mlir_op.`index.casts`[_type=__mlir_type.index](\n            rhs.value\n        )\n        return Self(\n            __mlir_op.`index.cmp`[\n                pred=__mlir_attr.`#index<cmp_predicate eq>`\n            ](lhsIndex, rhsIndex)\n        )\n\n    fn __invert__(self) -> Self:\n        return OurFalse if self == OurTrue else OurTrue\n--- cell type: markdown ---\nThis allows us to use the `~` operator with `OurBool`:\n--- cell type: code ---\nlet i = OurFalse\nif ~i: print(\"It's false!\")\n--- cell type: markdown ---\nThis extensible design is what allows even \"built in\" Mojo types like `Bool`, `Int`, and even `Tuple` to be implemented in the Mojo standard library in terms of MLIR, rather than hard-coded into the Mojo language. This also means that there's almost nothing that those types can achieve that user-defined types cannot.\n\nBy extension, this means that the incredible performance that Mojo unlocks for machine learning workflows isn't due to some magic being performed behind a curtain -- you can define your own high-level types that, in their implementation, use low-level MLIR to achieve unprecedented speed and control.\n--- cell type: markdown ---\n## The promise of modularity\n\nAs we've seen, Mojo's integration with MLIR allows Mojo programmers to implement zero-cost abstractons on par with Mojo's own builtin and standard library types.\n\nMLIR is open-source and extensible: new dialects are being added all the time, and those dialects then become available to use in Mojo. All the while, Mojo code gets more powerful and more optimized for new hardware -- with no additional work necessary by Mojo programmers.\n\nWhat this means is that your own custom types, whether those be `OurBool` or `OurTensor`, can be used to provide programmers with an easy-to-use and unchanging interface. But behind the scenes, MLIR will optimize those convenient, high-level types for the computing environments of tomorrow.\n\nIn other words: Mojo isn't magic, it's modular."
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/HelloMojo.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# Mojo language basics\n--- cell type: markdown ---\nMojo is a powerful programming language that's primarily designed for\nhigh-performance systems programming, so it has a lot in common with other\nsystems languages like Rust and C++. Yet, Mojo is also designed to become a\nsuperset of Python, so a lot of language features and concepts you might know\nfrom Python translate nicely to Mojo. \n\nFor example, if you're in a REPL environment or Jupyter notebook (like this\ndocument), you can run top-level code just like Python:\n--- cell type: code ---\nprint(\"Hello Mojo!\")\n--- cell type: markdown ---\nYou don't normally see that with other systems programming languages.\n\nMojo preserves Python's dynamic features and language syntax, and it even\nallows you to import and run code from Python packages. However, it's important\nto know that Mojo is an entirely new language, not just a new implementation of\nPython with syntax sugar. Mojo takes the Python language to a whole new level,\nwith systems programming features, strong type-checking, memory safety,\nnext-generation compiler technologies, and more. Yet, it's still designed to be\na simple language that's useful for general-purpose programming.\n\nThis page provides a gentle introduction to the Mojo language, and requires\nonly a little programming experience. So let's get started!\n\nIf you're an experienced systems programmer and want a deep dive into the\nlanguage, check out the [Mojo programming\nmanual](https://docs.modular.com/mojo/programming-manual.html).\n--- cell type: markdown ---\n## Language basics\n\nFirst and foremost, Mojo is a compiled language and a lot of its performance\nand memory-safety features are derived from that fact. Mojo code can be\nahead-of-time (AOT) or just-in-time (JIT) compiled.\n\nLike other compiled languages, Mojo programs (`.mojo` or `.ðŸ”¥` files) require a\n`main()` function as the entry point to the program. For example:\n--- cell type: code ---\nfn main():\n    var x: Int = 1\n    x += 1\n    print(x)\n--- cell type: markdown ---\nIf you know Python, you might have expected the function name to be `def\nmain()` instead of `fn main()`. Both actually work in Mojo, but using `fn`\nbehaves a bit differently, as we'll discuss below.\n\nOf course, if you're building a Mojo module (an API library), not a Mojo\nprogram, then your file doesn't need a `main()` function (because it will be\nimported by other programs that do have one).\n\n<div class=\"alert alert-block alert-info\">\n\n**Note:** When you're writing code in a `.mojo`/`.ðŸ”¥` file, you can't run\ntop-level code as shown on this pageâ€”all code in a Mojo program or module\nmust be encased in a function or struct. However, top-level code does work in a\nREPL or Jupyter notebook (such as the [notebook for this\npage](https://github.com/modularml/mojo/blob/main/examples/notebooks/HelloMojo.ipynb)).\n\n</div>\n\nNow let's explain the code in this `main()` function.\n--- cell type: markdown ---\n### Syntax and semantics\n\nThis is simple: Mojo supports (or will support) all of Python's syntax and\nsemantics. If you're not familiar with Python syntax, there are a ton of great\nresources online that can teach you.\n\nFor example, like Python, Mojo uses line breaks and indentation to define code\nblocks (not curly braces), and Mojo supports all of Python's control-flow syntax\nsuch as `if` conditions and `for` loops.\n\nHowever, Mojo is still a work in progress, so there are some things from Python\nthat aren't implemented in Mojo yet (see the [Mojo\nroadmap](https://docs.modular.com/mojo/roadmap.html)). All the missing Python\nfeatures will arrive in time, but Mojo already includes many features and\ncapabilities beyond what's available in Python.\n\nAs such, the following sections will focus on some of the language features that\nare unique to Mojo (compared to Python).\n--- cell type: markdown ---\n\n### Functions\n\nMojo functions can be declared with either `fn` (shown above) or `def` (as\nin Python). The `fn` declaration enforces strongly-typed and memory-safe\nbehaviors, while `def` provides Python-style dynamic behaviors.\n\nBoth `fn` and `def` functions have their value, and it's important that you\nlearn them both. However, for the purposes of this introduction, we're going to\nfocus on `fn` functions only. For much more detail about both, see the\n[programming manual](https://docs.modular.com/mojo/programming-manual.html).\n\nIn the following sections, you'll learn how `fn` functions enforce\nstrongly-typed and memory-safe behaviors in your code.\n--- cell type: markdown ---\n### Variables\n\nYou can declare variables (such as `x` in the above `main()` function) with\n`var` to create a mutable value, or with `let` to create an immutable value.\n\nIf you change `var` to `let` in the `main()` function above and run it, you'll\nget a compiler error like this:\n\n```text\nerror: Expression [15]:7:5: expression must be mutable for in-place operator destination\n    x += 1\n    ^\n```\n\nThat's because `let` makes the value immutable, so you can't increment it.\n\nAnd if you delete `var` completely, you'll get an error because `fn` functions\nrequire explicit variable declarations (unlike Python-style `def` functions).\n\nFinally, notice that the `x` variable has an explicit `Int` type specification.\nDeclaring the type is not required for variables in `fn`, but it is desirable\nsometimes. If you omit it, Mojo infers the type, as shown here:\n--- cell type: code ---\nfn do_math():\n    let x: Int = 1\n    let y = 2\n    print(x + y)\n\ndo_math()\n--- cell type: markdown ---\n### Function arguments and returns\n\nAlthough types aren't required for variables declared in the function body,\nthey are required for arguments and return values for an `fn` function.\n\nFor example, here's how to declare `Int` as the type for function arguments and\nthe return value:\n--- cell type: code ---\nfn add(x: Int, y: Int) -> Int:\n    return x + y\n\nz = add(1, 2)\nprint(z)\n--- cell type: markdown ---\n#### Optional arguments and keyword arguments\n\nYou can also specify argument default values (also known as optional\narguments), and pass values with keyword argument names. For example:\n--- cell type: code ---\nfn pow(base: Int, exp: Int = 2) -> Int:\n    return base ** exp\n\n# Uses default value for `exp`\nz = pow(3)\nprint(z)\n\n# Uses keyword argument names (with order reversed)\nz = pow(exp=3, base=2)\nprint(z)\n--- cell type: markdown ---\n\n<div class=\"alert alert-block alert-info\">\n\n**Note:** Mojo currently includes only partial support for keyword arguments, so\nsome features such as keyword-only arguments and variadic keyword arguments (e.g. `**kwargs`)\nare not supported yet.\n\n</div>\n--- cell type: markdown ---\n#### Argument mutability and ownership\n\nMojo supports full [value\nsemantics](https://en.wikipedia.org/wiki/Value_semantics) and enforces memory\nsafety with a robust value ownership model (similar to the Rust borrow\nchecker). Essentially, that means Mojo allows you to share references to values\n(instead of making a copy every time you pass a value to a function), but doing\nso requires that you follow Mojo's ownership rules (to ensure memory safety) as\ndescribed in this section.\n\nNotice that, above, `add()` doesn't modify `x` or `y`, it only reads the\nvalues. In fact, as written, the function *cannot* modify them because `fn`\narguments are **immutable references** by default. This ensures memory safety\n(no surprise changes to the data) while also avoiding a copy (which could be\na performance hit).\n\nIn terms of argument conventions, this is called \"borrowing,\" and although it's\nthe default for `fn` functions, you can make it explicit with the `borrowed`\ndeclaration like this (this behaves exactly the same as the `add()` above):\n--- cell type: code ---\nfn add(borrowed x: Int, borrowed y: Int) -> Int:\n    return x + y\n--- cell type: markdown ---\nIf you want the arguments to be mutable, you need to declare each argument\nconvention as `inout`. This means that changes made to the arguments *in*side\nthe function are visible *out*side the function. \n\nFor example, this function is able to modify the original variables:\n--- cell type: code ---\nfn add_inout(inout x: Int, inout y: Int) -> Int:\n    x += 1\n    y += 1\n    return x + y\n\nvar a = 1\nvar b = 2\nc = add_inout(a, b)\nprint(a)\nprint(b)\nprint(c)\n--- cell type: markdown ---\nAnother option is to declare the argument as `owned`, which provides\nthe function full ownership of the value (it's mutable and guaranteed unique).\nThis way, the function can modify the value and not worry about affecting\nvariables outside the function. For example:\n--- cell type: code ---\nfn set_fire(owned text: String) -> String:\n    text += \"ðŸ”¥\"\n    return text\n\nfn mojo():\n    let a: String = \"mojo\"\n    let b = set_fire(a)\n    print(a)\n    print(b)\n\nmojo()\n--- cell type: markdown ---\nIn this case, Mojo makes a copy of `a` and passes it as the `text` argument.\nThe original `a` string is still alive and well.\n\nHowever, if you want to give the function ownership of the value and **do not**\nwant to make a copy (which can be an expensive operation for some types), then\nyou can add the `^` \"transfer\" operator when you pass `a` to the function. The\ntransfer operator effectively destroys the local variable nameâ€”any attempt to\ncall upon it later causes a compiler error.\n\nTry it above by changing the call to `set_fire()` to look like this:\n\n```mojo\n    let b = set_fire(a^)\n```\n\nYou'll now get an error because the transfer operator effectively destroys the\n`a` variable, so when the following `print()` function tries to use `a`, that\nvariable isn't initialized anymore.\n\nIf you delete `print(a)`, then it works fine.\n\nThese argument conventions are designed to provide systems programmers with\ntotal control for memory optimizations while ensuring safe access and timely\ndeallocationsâ€”the Mojo compiler ensures that no two variables have mutable\naccess to the same value at the same time, and the lifetime of each value is\nwell-defined to strictly prevent any memory errors such as \"use-after-free\" and\n\"double-free.\"\n\n<div class=\"alert alert-block alert-info\">\n\n**Note:** Currently, Mojo always makes a copy when a function returns a value.\n\n</div>\n--- cell type: markdown ---\n## Structures\n\nYou can build high-level abstractions for types (or \"objects\") in a `struct`. A\n`struct` in Mojo is similar to a `class` in Python: they both support methods,\nfields, operator overloading, decorators for metaprogramming, etc. However,\nMojo structs are completely staticâ€”they are bound at compile-time, so they do\nnot allow dynamic dispatch or any runtime changes to the structure. (Mojo will\nalso support classes in the future.)\n\nFor example, here's a basic struct:\n--- cell type: code ---\nstruct MyPair:\n    var first: Int\n    var second: Int\n\n    fn __init__(inout self, first: Int, second: Int):\n        self.first = first\n        self.second = second\n    \n    fn dump(self):\n        print(self.first, self.second)\n--- cell type: markdown ---\nAnd here's how you can use it:\n--- cell type: code ---\nlet mine = MyPair(2, 4)\nmine.dump()\n--- cell type: markdown ---\nIf you're familiar with Python, then the `__init__()` method and the `self`\nargument should be familiar to you. If you're _not_ familiar with Python, then\nnotice that, when we call `dump()`, we don't actually pass a value for the\n`self` argument. The value for `self` is automatically provided with the\ncurrent instance of the struct (it's used similar to the `this` name used in\nsome other languages to refer to the current instance of the object/type).\n\nFor much more detail about structs and other special methods like `__init__()`\n(also known as \"dunder\" methods), see the [programming\nmanual](https://docs.modular.com/mojo/programming-manual.html).\n--- cell type: markdown ---\n## Python integration\n\nAlthough Mojo is still a work in progress and is not a full superset of Python\nyet, we've built a mechanism to import Python modules as-is, so you can\nleverage existing Python code right away. Under the hood, this mechanism uses\nthe CPython interpreter to run Python code, and thus it works seamlessly with\nall Python modules today.\n\nFor example, here's how you can import and use NumPy (you must have Python\n`numpy` installed):\n--- cell type: code ---\nfrom python import Python\n\nlet np = Python.import_module(\"numpy\")\n\nar = np.arange(15).reshape(3, 5)\nprint(ar)\nprint(ar.shape)\n--- cell type: markdown ---\n\n<div class=\"alert alert-block alert-info\">\n\n**Note:** Mojo is not a feature-complete superset of Python yet. So, you can't\nalways copy Python code and run it in Mojo. For more details on our plans,\nplease refer to the [Mojo roadmap and sharp edges](/mojo/roadmap.html).\n\n</div>\n--- cell type: markdown ---\n\n<div class=\"alert alert-block alert-info\">\n\n**Caution:** When you install Mojo, the installer searches your system for a\nversion of Python to use with Mojo, and adds the path to the `modular.cfg`\nconfig file. If you change your Python version or switch virtual environments,\nMojo will then be looking at the wrong Python library, which can cause problems\nsuch as errors when you import Python packages (Mojo says only `An error\noccurred in Python`â€”this is a separate [known\nissue](https://github.com/modularml/mojo/issues/536)). The current solution is\nto override Mojo's path to the Python library, using the `MOJO_PYTHON_LIBRARY`\nenvironment variable. For instructions on how to find and set this path, see\n[this related issue](https://github.com/modularml/mojo/issues/551).\n\n</div>\n--- cell type: markdown ---\n## Next steps\n\nWe hope this page covered enough of the basics to get you started. It's\nintentionally brief, so if you want more detail about any of the topics touched\nupon here, check out the [Mojo\nprogramming manual](https://docs.modular.com/mojo/programming-manual.html).\n\n- If you want to package your code as a library, read about\n  [Mojo modules and packages](/mojo/manual/get-started/packages.html).\n\n- If you want to explore some Mojo code, check out our\n  [code examples on GitHub](https://github.com/modularml/mojo/tree/main/examples#mojo-code-examples).\n\n- To see all the available Mojo APIs, check out the [Mojo standard library\n  reference](/mojo/lib.html).\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-info\">\n\n**Note:** The Mojo SDK is still in early development. Some things are still\nrough, but you can expect constant changes and improvements to both the\nlanguage and tools. Please see the [known\nissues](/mojo/roadmap.html#mojo-sdk-known-issues) and [report any other\nissues on GitHub](https://github.com/modularml/mojo/issues/new/choose).\n\n</div>"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/Mandelbrot.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# Mandelbrot in Mojo with Python plots\n\n--- cell type: markdown ---\nNot only is Mojo great for writing high-performance code, but it also allows us to leverage the huge Python ecosystem of libraries and tools. With seamless Python interoperability, Mojo can use Python for what it's good at, especially GUIs, without sacrificing performance in critical code. Let's take the classic Mandelbrot set algorithm and implement it in Mojo.\n\nThis tutorial shows two aspects of Mojo. First, it shows that Mojo can be used to develop fast programs for irregular applications. It also shows how we can leverage Python for visualizing the results.\n--- cell type: code ---\n#|code-fold: true\nimport benchmark\nfrom complex import ComplexSIMD, ComplexFloat64\nfrom math import iota\nfrom python import Python\nfrom runtime.llcl import num_cores\nfrom algorithm import parallelize, vectorize\nfrom tensor import Tensor\nfrom utils.index import Index\n\nalias float_type = DType.float64\nalias simd_width = 2 * simdwidthof[float_type]()\n--- cell type: markdown ---\nFirst set some parameters, you can try changing these to see different results:\n--- cell type: code ---\nalias width = 960\nalias height = 960\nalias MAX_ITERS = 200\n\nalias min_x = -2.0\nalias max_x = 0.6\nalias min_y = -1.5\nalias max_y = 1.5\n--- cell type: markdown ---\nThe core [Mandelbrot](https://en.wikipedia.org/wiki/Mandelbrot_set) algorithm involves computing an iterative complex function for each pixel until it \"escapes\" the complex circle of radius 2, counting the number of iterations to escape:\n\n$$z_{i+1} = z_i^2 + c$$\n--- cell type: code ---\n# Compute the number of steps to escape.\ndef mandelbrot_kernel(c: ComplexFloat64) -> Int:\n    z = c\n    for i in range(MAX_ITERS):\n        z = z * z + c\n        if z.squared_norm() > 4:\n            return i\n    return MAX_ITERS\n\n\ndef compute_mandelbrot() -> Tensor[float_type]:\n    # create a matrix. Each element of the matrix corresponds to a pixel\n    t = Tensor[float_type](height, width)\n\n    dx = (max_x - min_x) / width\n    dy = (max_y - min_y) / height\n\n    y = min_y\n    for row in range(height):\n        x = min_x\n        for col in range(width):\n            t[Index(row, col)] = mandelbrot_kernel(ComplexFloat64(x, y))\n            x += dx\n        y += dy\n    return t\n--- cell type: markdown ---\nPlotting the number of iterations to escape with some color gives us the canonical Mandelbrot set plot. To render it we can directly leverage Python's `matplotlib` right from Mojo!\n\nFirst install the required libraries:\n--- cell type: code ---\n%%python\nfrom importlib.util import find_spec\nimport shutil\nimport subprocess\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which('python3'): python = \"python3\"\n        elif shutil.which('python'): python = \"python\"\n        else: raise (\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n\ninstall_if_missing(\"numpy\")\ninstall_if_missing(\"matplotlib\")\n--- cell type: code ---\ndef show_plot(tensor: Tensor[float_type]):\n    alias scale = 10\n    alias dpi = 64\n\n    np = Python.import_module(\"numpy\")\n    plt = Python.import_module(\"matplotlib.pyplot\")\n    colors = Python.import_module(\"matplotlib.colors\")\n\n    numpy_array = np.zeros((height, width), np.float64)\n\n    for row in range(height):\n        for col in range(width):\n            numpy_array.itemset((col, row), tensor[col, row])\n\n    fig = plt.figure(1, [scale, scale * height // width], dpi)\n    ax = fig.add_axes([0.0, 0.0, 1.0, 1.0], False, 1)\n    light = colors.LightSource(315, 10, 0, 1, 1, 0)\n\n    image = light.shade(numpy_array, plt.cm.hot, colors.PowerNorm(0.3), \"hsv\", 0, 0, 1.5)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.show()\n\nshow_plot(compute_mandelbrot())\n--- cell type: markdown ---\n## Vectorizing Mandelbrot\nWe showed a naive implementation of the Mandelbrot algorithm, but there are two things we can do to speed it up. We can early-stop the loop iteration when a pixel is known to have escaped, and we can leverage Mojo's access to hardware by vectorizing the loop, computing multiple pixels simultaneously. To do that we will use the `vectorize` higher order generator.\n\nWe start by defining our main iteration loop in a vectorized fashion\n--- cell type: code ---\nfn mandelbrot_kernel_SIMD[\n    simd_width: Int\n](c: ComplexSIMD[float_type, simd_width]) -> SIMD[float_type, simd_width]:\n    \"\"\"A vectorized implementation of the inner mandelbrot computation.\"\"\"\n    let cx = c.re\n    let cy = c.im\n    var x = SIMD[float_type, simd_width](0)\n    var y = SIMD[float_type, simd_width](0)\n    var y2 = SIMD[float_type, simd_width](0)\n    var iters = SIMD[float_type, simd_width](0)\n\n    var t: SIMD[DType.bool, simd_width] = True\n    for i in range(MAX_ITERS):\n        if not t.reduce_or():\n            break\n        y2 = y*y\n        y = x.fma(y + y, cy)\n        t = x.fma(x, y2) <= 4\n        x = x.fma(x, cx - y2)\n        iters = t.select(iters + 1, iters)\n    return iters\n--- cell type: markdown ---\nThe above function is parameterized on the `simd_width` and processes simd_width pixels. It only escapes once all pixels within the vector lane are done. We can use the same iteration loop as above, but this time we vectorize within each row instead. We use the `vectorize` generator to make this a simple function call.\n--- cell type: code ---\nfn vectorized():\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we oeprate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](row * width + col, mandelbrot_kernel_SIMD[simd_width](c))\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n\n    @parameter\n    fn bench[simd_width: Int]():\n        for row in range(height):\n            worker(row)\n\n    let vectorized = benchmark.run[bench[simd_width]]().mean[\"ms\"]()\n    print(\"Vectorized\", \":\", vectorized, \"ms\")\n\n    try:\n        _ = show_plot(t)\n    except e:\n        print(\"failed to show plot:\", e)\n\nvectorized()\n--- cell type: markdown ---\n## Parallelizing Mandelbrot\nWhile the vectorized implementation above is efficient, we can get better performance by parallelizing on the cols. This again is simple in Mojo using the `parallelize` higher order function. Only the function that performs the invocation needs to change.\n--- cell type: code ---\nfn parallelized():\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we oeprate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](row * width + col, mandelbrot_kernel_SIMD[simd_width](c))\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n\n    @parameter\n    fn bench_parallel[simd_width: Int]():\n        parallelize[worker](height, height)\n\n    let parallelized = benchmark.run[bench_parallel[simd_width]]().mean[\"ms\"]()\n    print(\"Parallelized:\", parallelized, \"ms\")\n\n    try:\n        _ = show_plot(t)\n    except e:\n        print(\"failed to show plot:\", e)\n\nparallelized()\n--- cell type: markdown ---\n## Benchmarking\n\nIn this section we increase the size to 4096x4096 and run 1000 iterations for a larger test to stress the CPU \n--- cell type: code ---\nfn compare():\n    let t = Tensor[float_type](height, width)\n\n    @parameter\n    fn worker(row: Int):\n        let scale_x = (max_x - min_x) / width\n        let scale_y = (max_y - min_y) / height\n\n        @parameter\n        fn compute_vector[simd_width: Int](col: Int):\n            \"\"\"Each time we oeprate on a `simd_width` vector of pixels.\"\"\"\n            let cx = min_x + (col + iota[float_type, simd_width]()) * scale_x\n            let cy = min_y + row * scale_y\n            let c = ComplexSIMD[float_type, simd_width](cx, cy)\n            t.data().simd_store[simd_width](row * width + col, mandelbrot_kernel_SIMD[simd_width](c))\n\n        # Vectorize the call to compute_vector where call gets a chunk of pixels.\n        vectorize[simd_width, compute_vector](width)\n\n\n    @parameter\n    fn bench[simd_width: Int]():\n        for row in range(height):\n            worker(row)\n\n    let vectorized = benchmark.run[bench[simd_width]]().mean[\"ms\"]()\n    print(\"Number of threads:\", num_cores())\n    print(\"Vectorized:\", vectorized, \"ms\")\n\n    # Parallelized\n    @parameter\n    fn bench_parallel[simd_width: Int]():\n        parallelize[worker](height, height)\n\n    let parallelized = benchmark.run[bench_parallel[simd_width]]().mean[\"ms\"]()\n    print(\"Parallelized:\", parallelized, \"ms\")\n    print(\"Parallel speedup:\", vectorized / parallelized)\n\n    _ = t # Make sure tensor isn't destroyed before benchmark is finished\n--- cell type: code ---\ncompare()"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/Matmul.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# Matrix multiplication in Mojo\n--- cell type: markdown ---\nThis notebook describes how to write a matrix multiplication (matmul) algorithm in Mojo. We will start with a pure Python implementation, transition to a naive implementation that is essentially a copy of the Python one, then add types, then continue the optimizations by vectorizing, tiling, and parallelizing the implementation.\n--- cell type: markdown ---\nFirst, let's define matrix multiplication. Given two dense matrices $A$ and $B$ of dimensions $M\\times K$ and $K\\times N$ respectively, we want to compute their dot product $C = A . B$ (also known as matmul). The dot product $C += A . B$ is defined by\n--- cell type: markdown ---\n$$C_{i,j} += \\sum_{k \\in [0 \\cdots K)} A_{i,k} B_{k,j}$$\n--- cell type: markdown ---\n> Please take look at our [blog](https://www.modular.com/blog/ais-compute-fragmentation-what-matrix-multiplication-teaches-us) post on matmul and why it is important for ML and DL workloads.\n--- cell type: markdown ---\nThe format of this notebook is to start with an implementation which is identical to that of Python (effectively renaming the file extension), then look at how adding types to the implementation helps performance before extending the implementation by leveraging the vectorization and parallelization capabilities available on modern hardware. Throughout the execution, we report the GFlops achieved.\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-info\">\n<b>Note:</b> Mojo Playground is designed only for testing the Mojo language.\nThe cloud environment is not always stable and performance varies, so it is not\nan appropriate environment for performance benchmarking. However, we believe it\ncan still demonstrate the magnitude of performance gains provided by Mojo. For\nmore information about the compute power in the Mojo Playground, see the <a\nhref=\"https://docs.modular.com/mojo/faq.html#mojo-playground\">Mojo FAQ</a>.\n</div>\n--- cell type: markdown ---\n## Python Implementation\n--- cell type: markdown ---\nLet's first implement matmul in Python directly from the definition.\n--- cell type: code ---\n%%python\ndef matmul_python(C, A, B):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n--- cell type: markdown ---\nLet's benchmark our implementation using 128 by 128 square matrices and report the achieved GFLops.\n--- cell type: markdown ---\nInstall numpy if it's not already:\n--- cell type: code ---\n%%python\nfrom importlib.util import find_spec\nimport shutil\nimport subprocess\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which('python3'): python = \"python3\"\n        elif shutil.which('python'): python = \"python\"\n        else: raise (\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n\ninstall_if_missing(\"numpy\")\n--- cell type: code ---\n%%python\nfrom timeit import timeit\nimport numpy as np\n\nclass Matrix:\n    def __init__(self, value, rows, cols):\n        self.value = value\n        self.rows = rows\n        self.cols = cols\n\n    def __getitem__(self, idxs):\n        return self.value[idxs[0]][idxs[1]]\n\n    def __setitem__(self, idxs, value):\n        self.value[idxs[0]][idxs[1]] = value\n\ndef benchmark_matmul_python(M, N, K):\n    A = Matrix(list(np.random.rand(M, K)), M, K)\n    B = Matrix(list(np.random.rand(K, N)), K, N)\n    C = Matrix(list(np.zeros((M, N))), M, N)\n    secs = timeit(lambda: matmul_python(C, A, B), number=2)/2\n    gflops = ((2*M*N*K)/secs) / 1e9\n    print(gflops, \"GFLOP/s\")\n    return gflops\n--- cell type: code ---\npython_gflops = benchmark_matmul_python(128, 128, 128).to_float64()\n--- cell type: markdown ---\n## Importing the Python implementation to Mojo\n--- cell type: markdown ---\nUsing Mojo is as simple as Python. First, let's include that modules from the Mojo stdlib that we are going to use:\n--- cell type: code ---\n#|code-fold: true\n#|code-summary: \"Import utilities and define `Matrix` (click to show/hide)\"\n\nimport benchmark\nfrom sys.intrinsics import strided_load\nfrom math import div_ceil, min\nfrom memory import memset_zero\nfrom memory.unsafe import DTypePointer\nfrom random import rand, random_float64\nfrom sys.info import simdwidthof\nfrom runtime.llcl import Runtime\n--- cell type: markdown ---\nThen, we can copy and paste our Python code. Mojo is a superset of Python, so the same Python code will run as Mojo code\n--- cell type: code ---\n# This exactly the same Python implementation,\n# but is infact Mojo code!\ndef matmul_untyped(C, A, B):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n--- cell type: markdown ---\nWe can then benchmark the implementation. As before we use a 128 by 128 matrix\n--- cell type: code ---\nfn matrix_getitem(self: object, i: object) raises -> object:\n    return self.value[i]\n\n\nfn matrix_setitem(self: object, i: object, value: object) raises -> object:\n    self.value[i] = value\n    return None\n\n\nfn matrix_append(self: object, value: object) raises -> object:\n    self.value.append(value)\n    return None\n\n\nfn matrix_init(rows: Int, cols: Int) raises -> object:\n    let value = object([])\n    return object(\n        Attr(\"value\", value), Attr(\"__getitem__\", matrix_getitem), Attr(\"__setitem__\", matrix_setitem),\n        Attr(\"rows\", rows), Attr(\"cols\", cols), Attr(\"append\", matrix_append),\n    )\n\ndef benchmark_matmul_untyped(M: Int, N: Int, K: Int, python_gflops: Float64):\n    C = matrix_init(M, N)\n    A = matrix_init(M, K)\n    B = matrix_init(K, N)\n    for i in range(M):\n        c_row = object([])\n        b_row = object([])\n        a_row = object([])\n        for j in range(N):\n            c_row.append(0.0)\n            b_row.append(random_float64(-5, 5))\n            a_row.append(random_float64(-5, 5))\n        C.append(c_row)\n        B.append(b_row)\n        A.append(a_row)\n\n    @parameter\n    fn test_fn():\n        try:\n            _ = matmul_untyped(C, A, B)\n        except:\n            pass\n\n    let secs = benchmark.run[test_fn]().mean()\n    _ = (A, B, C)\n    let gflops = ((2*M*N*K)/secs) / 1e9\n    let speedup : Float64 = gflops / python_gflops\n    print(gflops, \"GFLOP/s, a\", speedup.value, \"x speedup over Python\")\n--- cell type: code ---\nbenchmark_matmul_untyped(128, 128, 128, python_gflops)\n--- cell type: markdown ---\nNote the huge speedup with no effort that we have gotten.\n--- cell type: markdown ---\n## Adding types to the Python implementation\n--- cell type: markdown ---\nThe above program, while achieving better performance than Python, is still not the best we can get from Mojo. If we tell Mojo the types of the inputs, it can optimize much of the code away and reduce dispatching costs (unlike Python, which only uses types for type checking, Mojo exploits type info for performance optimizations as well).\n--- cell type: markdown ---\nTo do that, let's first define a `Matrix` struct. The `Matrix` struct contains a data pointer along with size fields. While the `Matrix` struct can be parametrized on any data type, here we set the data type to be Float32 for conciseness.\n--- cell type: code ---\nalias type = DType.float32\n\nstruct Matrix:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    # Initialize zeroeing all values\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        memset_zero(self.data, rows * cols)\n        self.rows = rows\n        self.cols = cols\n\n    # Initialize taking a pointer, don't set any elements\n    fn __init__(inout self, rows: Int, cols: Int, data: DTypePointer[DType.float32]):\n        self.data = data\n        self.rows = rows\n        self.cols = cols\n\n    ## Initialize with random values\n    @staticmethod\n    fn rand(rows: Int, cols: Int) -> Self:\n        let data = DTypePointer[type].alloc(rows * cols)\n        rand(data, rows * cols)\n        return Self(rows, cols, data)\n\n    fn __getitem__(self, y: Int, x: Int) -> Float32:\n        return self.load[1](y, x)\n\n    fn __setitem__(self, y: Int, x: Int, val: Float32):\n        return self.store[1](y, x, val)\n\n    fn load[nelts: Int](self, y: Int, x: Int) -> SIMD[DType.float32, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    fn store[nelts: Int](self, y: Int, x: Int, val: SIMD[DType.float32, nelts]):\n        return self.data.simd_store[nelts](y * self.cols + x, val)\n--- cell type: markdown ---\n> Note that we implement `getitem` and `setitem` in terms of `load` and `store`. For the naive implementation of matmul it does not make a difference, but we will utilize this later in a more optimized vectorized version of matmul.\n--- cell type: markdown ---\nWith the above `Matrix` type we can effectively copy and paste the Python implementation and just add type annotations:\n--- cell type: code ---\n# Note that C, A, and B have types.\nfn matmul_naive(C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n--- cell type: markdown ---\nWe are going to benchmark the implementations as we improve, so let's write a helper function that will do that for us: \n--- cell type: code ---\nalias M = 1024\nalias N = 1024\nalias K = 1024\n\n@always_inline\nfn bench[\n    func: fn (Matrix, Matrix, Matrix) -> None](base_gflops: Float64):\n    var C = Matrix(M, N)\n    var A = Matrix.rand(M, K)\n    var B = Matrix.rand(K, N)\n\n    @always_inline\n    @parameter\n    fn test_fn():\n        _ = func(C, A, B)\n\n    let secs = benchmark.run[test_fn]().mean()\n    # Prevent the matrices from being freed before the benchmark run\n    A.data.free()\n    B.data.free()\n    C.data.free()\n    let gflops = ((2 * M * N * K) / secs) / 1e9\n    let speedup: Float64 = gflops / base_gflops\n    # print(gflops, \"GFLOP/s\", speedup, \" speedup\")\n    print(gflops, \"GFLOP/s, a\", speedup.value, \"x speedup over Python\")\n--- cell type: markdown ---\nBenchmarking shows significant speedups. We increase the size of the matrix to 512 by 512, since Mojo is much faster than Python.\n--- cell type: code ---\nbench[matmul_naive](python_gflops)\n--- cell type: markdown ---\nAdding type annotations gives a huge improvement compared to the original untyped version.\n--- cell type: markdown ---\n## Vectorizing the inner most loop\n--- cell type: markdown ---\nWe can do better than the above implementation by utilizing the vector instructions. Rather than assuming a vector width, we query the simd width of the specified dtype using `simd_width`. This makes our code portable as we transition to other hardware. Leverage SIMD instructions is as easy as:\n--- cell type: code ---\n# Mojo has SIMD vector types, we can vectorize the Matmul code as follows.\n# nelts = number of float32 elements that can fit in SIMD register\nalias nelts = simdwidthof[DType.float32]()\nfn matmul_vectorized_0(C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for nv in range(0, C.cols, nelts):\n                C.store[nelts](m,nv, C.load[nelts](m,nv) + A[m,k] * B.load[nelts](k,nv))\n\n            # Handle remaining elements with scalars.\n            for n in range(nelts*(C.cols//nelts), C.cols):\n                C[m,n] += A[m,k] * B[k,n]\n--- cell type: markdown ---\nWe can benchmark the above implementation. Note that many compilers can detect naive loops and perform optimizations on them. Mojo, however, allows you to be explicit and precisely control what optimizations are applied.\n--- cell type: code ---\nbench[matmul_vectorized_0](python_gflops)\n--- cell type: markdown ---\nVectorization is a common optimization, and Mojo provides a higher-order function that performs vectorization for you. The `vectorize` function takes a vector width and a function which is parametric on the vector width and is going to be evaluated in a vectorized manner.\n--- cell type: code ---\n# Simplify the code by using the builtin vectorize function\nfrom algorithm import vectorize\nfn matmul_vectorized_1(C: Matrix, A: Matrix, B: Matrix):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            @parameter\n            fn dot[nelts : Int](n : Int):\n                C.store[nelts](m,n, C.load[nelts](m,n) + A[m,k] * B.load[nelts](k,n))\n            vectorize[nelts, dot](C.cols)\n--- cell type: markdown ---\nThere is only a slight difference in terms of performance between the two implementations:\n--- cell type: code ---\nbench[matmul_vectorized_1](python_gflops)\n--- cell type: markdown ---\n## Parallelizing Matmul\n--- cell type: markdown ---\nWith Mojo we can easily run code in parallel with the `parallelize` function.\n\nLet's modify our matmul implementation and make it multi-threaded (for simplicity, we only `parallelize` on the M dimension).\n\nIn `parallelize` below we're overpartitioning by distributing the work more evenly among processors. This ensures they all have something to work on even if some tasks finish before others, or some processors are stragglers. Intel and Apple now have separate performance and efficiency cores and this mitigates the problems that can cause.\n--- cell type: code ---\n# Parallelize the code by using the builtin parallelize function\nfrom algorithm import parallelize\nfn matmul_parallelized(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        for k in range(A.cols):\n            @parameter\n            fn dot[nelts : Int](n : Int):\n                C.store[nelts](m,n, C.load[nelts](m,n) + A[m,k] * B.load[nelts](k,n))\n            vectorize[nelts, dot](C.cols)\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nWe can benchmark the parallel matmul implementation.\n--- cell type: code ---\nbench[matmul_parallelized](python_gflops)\n--- cell type: markdown ---\n## Tiling Matmul\n--- cell type: markdown ---\nTiling is an optimization performed for matmul to increase cache locality. The idea is to keep sub-matrices resident in the cache and increase the reuse. The tile function itself can be written in Mojo as:\n--- cell type: code ---\nfrom algorithm import Static2DTileUnitFunc as Tile2DFunc\n--- cell type: code ---\n# Perform 2D tiling on the iteration space defined by end_x and end_y.\nfn tile[tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    for y in range(0, end_y, tile_y):\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n--- cell type: markdown ---\nThe above will perform 2 dimensional tiling over a 2D iteration space defined to be between $([0, end_x], [0, end_y])$. Once we define it above, we can use it within our matmul kernel. For simplicity we choose `4` as the tile height and since we also want to vectorize we use `4 * nelts` as the tile width (since we vectorize on the columns).\n--- cell type: code ---\n# Use the above tile function to perform tiled matmul.\nfn matmul_tiled_parallelized(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n                @parameter\n                fn dot[nelts : Int,](n : Int):\n                    C.store[nelts](m,n + x, C.load[nelts](m,n+x) + A[m,k] * B.load[nelts](k,n+x))\n                vectorize[nelts, dot](tile_x)\n\n        # We hardcode the tile factor to be 4.\n        alias tile_size = 4\n        tile[calc_tile, nelts * tile_size, tile_size](A.cols, C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nAgain, we can benchmark the tiled parallel matmul implementation:\n--- cell type: code ---\nbench[matmul_tiled_parallelized](python_gflops)\n--- cell type: markdown ---\nOne source of overhead in the above implementation is the fact that the we are not unrolling the loops introduced by vectorize of the dot function. We can do that via the `vectorize_unroll` higher-order function in Mojo:\n--- cell type: code ---\n# Unroll the vectorized loop by a constant factor.\nfrom algorithm import vectorize_unroll\nfn matmul_tiled_unrolled_parallelized(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n                @parameter\n                fn dot[nelts : Int,](n : Int):\n                    C.store[nelts](m,n+x, C.load[nelts](m,n+x) + A[m,k] * B.load[nelts](k,n+x))\n\n                # Vectorize by nelts and unroll by tile_x/nelts\n                # Here unroll factor is 4\n                vectorize_unroll[nelts, tile_x//nelts, dot](tile_x)\n\n        alias tile_size = 4\n        tile[calc_tile, nelts*tile_size, tile_size](A.cols, C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nAgain, we can benchmark the new tiled parallel matmul implementation with unrolled and vectorized inner loop:\n--- cell type: code ---\nbench[matmul_tiled_unrolled_parallelized](python_gflops)\n--- cell type: markdown ---\n## Searching for the `tile_factor`\n--- cell type: code ---\nfrom autotune import autotune, search\nfrom time import now\nfrom memory.unsafe import Pointer\n\nalias matmul_fn_sig_type = fn(C: Matrix, A: Matrix, B: Matrix, /) -> None\n--- cell type: markdown ---\nThe choice of the tile factor can greatly impact the performance of the full matmul,\nbut the optimal tile factor is highly hardware-dependent, and is influenced by the\ncache configuration and other hard-to-model effects. We want to write portable code\nwithout having to know everything about the hardware, so we can ask Mojo to automatically\nselect the best tile factor using autotuning.\n--- cell type: code ---\n# Autotune the tile size used in the matmul.\n@adaptive\nfn matmul_autotune_impl(C: Matrix, A: Matrix, B: Matrix, /):\n    @parameter\n    fn calc_row(m: Int):\n        @parameter\n        fn calc_tile[tile_x: Int, tile_y: Int](x: Int, y: Int):\n            for k in range(y, y + tile_y):\n                @parameter\n                fn dot[nelts : Int](n : Int):\n                    C.store[nelts](m,n+x, C.load[nelts](m,n+x) + A[m,k] * B.load[nelts](k,n+x))\n                vectorize_unroll[nelts, tile_x // nelts, dot](tile_x)\n\n        # Instead of hardcoding to tile_size = 4, search for the fastest\n        # tile size by evaluating this function as tile size varies.\n        alias tile_size = autotune(1, 2, 4, 8, 16, 32)\n        tile[calc_tile, nelts * tile_size, tile_size](A.cols, C.cols)\n\n    parallelize[calc_row](C.rows, C.rows)\n--- cell type: markdown ---\nThis will generate multiple candidates for the matmul function. To teach Mojo how\nto find the best tile factor, we provide an evaluator function Mojo can use to\nassess each candidate.\n--- cell type: code ---\nfn matmul_evaluator(funcs: Pointer[matmul_fn_sig_type], size: Int) -> Int:\n    print(\"matmul_evaluator, number of candidates: \", size)\n\n    let eval_begin: Int = now()\n\n    # This size is picked at random, in real code we could use a real size\n    # distribution here.\n    let M = 512\n    let N = 512\n    let K = 512\n    print(\"Optimizing for size:\", M, \"x\", N, \"x\", K)\n\n    var best_idx: Int = -1\n    var best_time: Int = -1\n\n    alias eval_iterations = 10\n    alias eval_samples = 10\n\n    var C = Matrix(M, N)\n    var A = Matrix(M, K)\n    var B = Matrix(K, N)\n    let Cptr = Pointer[Matrix].address_of(C).address\n    let Aptr = Pointer[Matrix].address_of(A).address\n    let Bptr = Pointer[Matrix].address_of(B).address\n\n    # Find the function that's the fastest on the size we're optimizing for\n    for f_idx in range(size):\n        let func = funcs.load(f_idx)\n\n        @always_inline\n        @parameter\n        fn wrapper():\n            func(C, A, B)\n        let cur_time = benchmark.run[wrapper](1, 100_000, 5, 10).mean[\"ns\"]().to_int()\n\n        if best_idx < 0:\n            best_idx = f_idx\n            best_time = cur_time\n        if best_time > cur_time:\n            best_idx = f_idx\n            best_time = cur_time\n\n    let eval_end: Int = now()\n    # Prevent matrices from being destroyed before we finished benchmarking them.\n    A.data.free()\n    B.data.free()\n    C.data.free()\n    print(\"Time spent in matmul_evaluator, ms:\", (eval_end - eval_begin) // 1000000)\n    print(\"Best candidate idx:\", best_idx)\n    return best_idx\n--- cell type: markdown ---\nFinally, we need to define an entry function that would simply call the best candidate.\n--- cell type: code ---\nfn matmul_autotune(C: Matrix, A: Matrix, B: Matrix):\n    alias best_impl: matmul_fn_sig_type\n    search[\n        matmul_fn_sig_type,\n        VariadicList(matmul_autotune_impl.__adaptive_set),\n        matmul_evaluator -> best_impl\n    ]()\n    # Run the best candidate\n    return best_impl(C, A, B)\n--- cell type: markdown ---\nLet's benchmark our new implementation:\n--- cell type: code ---\nbench[matmul_autotune](python_gflops)\n--- cell type: markdown ---\n# Tile and accumulate in registers\n--- cell type: markdown ---\nPerform 2D tiling on the iteration space defined by end_x and end_y, parallelizing over y.\n--- cell type: code ---\nfn tile_parallel[\n    tiled_fn: Tile2DFunc, tile_x: Int, tile_y: Int\n](end_x: Int, end_y: Int):\n    # Note: this assumes that ends are multiples of the tiles.\n    @parameter\n    fn row(yo: Int):\n        let y = tile_y * yo\n        for x in range(0, end_x, tile_x):\n            tiled_fn[tile_x, tile_y](x, y)\n\n    parallelize[row](end_y // tile_y, 512)\n--- cell type: markdown ---\nTile the output and accumulate in registers. This strategy means we can\ncompute tile_i * tile_j values of output for only reading tile_i + tile_j input values.\n--- cell type: code ---\nfrom memory import stack_allocation\n\nfn accumulate_registers(C: Matrix, A: Matrix, B: Matrix):\n    @parameter\n    fn calc_tile[tile_j: Int, tile_i: Int](jo: Int, io: Int):\n        # Allocate the tile of accumulators on the stack.\n        var accumulators = Matrix(tile_i, tile_j, stack_allocation[tile_i * tile_j, DType.float32]())\n\n        for k in range(0, A.cols):\n            @parameter\n            fn calc_tile_row[i: Int]():\n                @parameter\n                fn calc_tile_cols[nelts: Int](j: Int):\n                    accumulators.store[nelts](i, j, accumulators.load[nelts](i, j) + A[io + i, k] * B.load[nelts](k, jo + j))\n\n                vectorize_unroll[nelts, tile_j // nelts, calc_tile_cols](tile_j)\n\n            unroll[tile_i, calc_tile_row]()\n\n        # Copy the local tile to the output\n        for i in range(tile_i):\n            for j in range(tile_j):\n                C[io + i, jo + j] = accumulators[i, j]\n\n    alias tile_i = 4\n    alias tile_j = nelts*4\n    tile_parallel[calc_tile, tile_j, tile_i](C.cols, C.rows)\n--- cell type: code ---\nbench[accumulate_registers](python_gflops)"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/Memset.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# Fast memset in Mojo\n--- cell type: markdown ---\nIn this tutorial we will implement a memset version optimized for small sizes\nusing Mojo's autotuning feature.\n\nThe idea behind the implementation is based on Nadav Rotem's work [[1](https://github.com/nadavrot/memset_benchmark)], and is also well-described in [[2](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4f7c3da72d557ed418828823a8e59942859d677f.pdf)].\n\nWe briefly summarize the approach below.\n--- cell type: markdown ---\n## High-level overview\n\nFor the best memset performance we want to use the widest possible register\nwidth for the memory access. For instance, if we want to store 19 bytes, we\nwant to use vector width 16 and use two overlapping stores. To store 9 bytes,\nwe would want to use two 8-byte stores.\n\nHowever, before we get to actually doing stores, we need to perform size\nchecks to make sure that we're in the right range. I.e. we want to use 8\nbytes stores for sizes 8-16, 16 bytes stores for sizes 16-32, etc.\n\nThe order in which we do the size checks significantly affects performance\nand ideally we would like to run as few checks as possible for the sizes\nthat occur most often. I.e. if most of the sizes we see are 16-32, then we\nwant to first check if it's within that range before we check if it's in\n8-16 or some other range.\n\nThis results in a number of different comparison \"trees\" that can be used to\nperform the size checks, and in this tutorial we use Mojo's autotuning to pick\nthe most optimal one given the distribution of input data.\n--- cell type: markdown ---\n## Implementation\n\nWe will start as we always start - with imports and type aliases.\n--- cell type: code ---\nfrom autotune import autotune_fork, search\nfrom math import min, max\nfrom memory.unsafe import DTypePointer, Pointer\nfrom time import time_function\nfrom benchmark import keep\nfrom memory import memset as stdlib_memset\n\nalias type = UInt8\nalias ptr_type = DTypePointer[DType.uint8]\n\nalias fn_type = fn(ptr: ptr_type, value: type, count: Int, /) -> None\n--- cell type: markdown ---\nNow let's add some auxiliary functions. We will use them to benchmark various\nmemset implementations and visualize results.\n--- cell type: code ---\nfn measure_time(func: fn_type, size: Int, iters: Int, samples: Int) -> Int:\n    alias alloc_size = 1024 * 1024\n    let ptr = ptr_type.alloc(alloc_size)\n\n    var best = -1\n    for sample in range(samples):\n\n        @parameter\n        fn runner():\n            for iter in range(iters):\n                # Offset pointer to shake up cache a bit\n                let offset_ptr = ptr.offset((iter * 128) & 1024)\n\n                # memset, change the value we're filling with\n                let v = type(iter&255)\n\n                # Actually call the memset function\n                func(offset_ptr, v.value, size)\n\n                # Avoid compiler optimizing things away\n                keep(v)\n                keep(size)\n                keep(offset_ptr)\n\n        let ns = time_function[runner]()\n        if best < 0 or ns < best:\n            best = ns\n\n    ptr.free()\n    return best\n\nalias MULT = 2_000\n\nfn visualize_result(size: Int, result: Int):\n    print_no_newline(\"Size: \")\n    if size < 10:\n        print_no_newline(\" \")\n    print_no_newline(size, \"  |\")\n    for _ in range(result // MULT):\n        print_no_newline(\"*\")\n    print()\n\n\nfn benchmark(func: fn_type, title: StringRef):\n    print(\"\\n=====================\")\n    print(title)\n    print(\"---------------------\\n\")\n\n    alias benchmark_iterations = 30 * MULT\n    alias warmup_samples = 10\n    alias benchmark_samples = 1000\n\n    # Warmup\n    for size in range(35):\n        _ = measure_time(\n            func, size, benchmark_iterations, warmup_samples\n        )\n\n    # Actual run\n    for size in range(35):\n        let result = measure_time(\n            func, size, benchmark_iterations, benchmark_samples\n        )\n\n        visualize_result(size, result)\n--- cell type: markdown ---\n### Reproducing results from the paper\n\nLet's implement a memset version from the paper in Mojo and compare it against\nthe system memset.\n\n--- cell type: code ---\n@always_inline\nfn overlapped_store[\n    width: Int\n](ptr: ptr_type, value: type, count: Int):\n    let v = SIMD[DType.uint8, width].splat(value)\n    ptr.simd_store[width](v)\n    ptr.simd_store[width](count - width, v)\n\n\nfn memset_manual(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count <= 16:\n            if count >= 8:\n                # 8 <= count < 16\n                overlapped_store[8](ptr, value, count)\n                return\n            # 4 < count < 8\n            overlapped_store[4](ptr, value, count)\n            return\n\n        # 16 <= count < 32\n        overlapped_store[16](ptr, value, count)\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n\n\nfn memset_system(ptr: ptr_type, value: type, count: Int):\n    stdlib_memset(ptr, value.value, count)\n--- cell type: markdown ---\nLet's benchmark our version of memset vs the standard memset.\n\n> _**Note**: We're optimizing memset for tiniest sizes and benchmarking that properly is tricky. The notebook environment makes it even harder, and while we tried our best to tune the notebook to demonstrate the performance difference, it is hard to guarantee that the results will be stable from run to run._\n--- cell type: code ---\nbenchmark(memset_manual, \"Manual memset\")\nbenchmark(memset_system, \"System memset\")\n--- cell type: markdown ---\n### Tweaking the implementation for different sizes\n\nWe can see that it's already much faster for small sizes.\nThat version was specifically optimized for a certain input size distribution,\ne.g. we can see that sizes 8-16 and 0-4 work fastest.\n\nBut what if in **our use case** the distribution is different? Let's imagine that\nin our case the most common sizes are 16-32 - is this version the most optimal\nversion we can use then? The answer is obviously \"no\", and we can easily tweak\nthe implementation to work better for these sizes - we just need to move the\ncorresponding check closer to the beginning of the function. E.g. like so:\n--- cell type: code ---\nfn memset_manual_2(ptr: ptr_type, value: type, count: Int):\n    if count < 32:\n        if count >= 16:\n            # 16 <= count < 32\n            overlapped_store[16](ptr, value, count)\n            return\n\n        if count < 5:\n            if count == 0:\n                return\n            # 0 < count <= 4\n            ptr.store(0, value)\n            ptr.store(count - 1, value)\n            if count <= 2:\n                return\n            ptr.store(1, value)\n            ptr.store(count - 2, value)\n            return\n\n        if count >= 8:\n            # 8 <= count < 16\n            overlapped_store[8](ptr, value, count)\n            return\n        # 4 < count < 8\n        overlapped_store[4](ptr, value, count)\n\n    else:\n        # 32 < count\n        memset_system(ptr, value, count)\n--- cell type: markdown ---\nLet's check the performance of this version.\n--- cell type: code ---\nbenchmark(memset_manual_2, \"Manual memset v2\")\nbenchmark(memset_system, \"Mojo system memset\")\n--- cell type: markdown ---\nThe performance is now much better on the 16-32 sizes!\n\nThe problem is that we had to manually re-write the code. Wouldn't it be nice\nif it was done automatically?\n\nIn Mojo this is possible (and quite easy) - we can generate multiple\nimplementations and let the compiler pick the fastest one for us evaluating\nthem on sizes we want!\n--- cell type: markdown ---\n### Mojo implementation\n\nLet's dive into that.\n\nThe first thing we need to do is to generate all possible candidates. To do\nthat we will need to iteratively generate size checks to understand what size\nfor the overlapping store we can use. Once we localize the size interval, we\njust call the overlapping store of the corresponding size.\n\nTo express this we will implement an adaptive function `memset_impl_layer` two\nparameters designating the current interval of possible size values. When we\ngenerate a new size check, we split that interval into two parts and\nrecursively call the same functions on those two parts. Once we reach the\nminimal intervals, we will call the corresponding overlapped_store function.\n\nThis first implementation covers minimal interval cases:\n--- cell type: code ---\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    @parameter\n    if lower == -100 and upper == 0:\n        pass\n    elif lower == 0 and upper == 4:\n        ptr.store(0, value)\n        ptr.store(count - 1, value)\n        if count <= 2:\n            return\n        ptr.store(1, value)\n        ptr.store(count - 2, value)\n    elif lower == 4 and upper == 8:\n        overlapped_store[4](ptr, value, count)\n    elif lower == 8 and upper == 16:\n        overlapped_store[8](ptr, value, count)\n    elif lower == 16 and upper == 32:\n        overlapped_store[16](ptr, value, count)\n    elif lower == 32 and upper == 100:\n        memset_system(ptr, value, count)\n    else:\n        constrained[False]()\n--- cell type: markdown ---\nLet's now add an implementation for the other case, where we need to generate a\nsize check.\n--- cell type: code ---\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count > cur:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n    else:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n--- cell type: markdown ---\nHere we use `autotune_fork` to generate all possible at that point checks.\n\nWe will discard values beyond the current interval, and for the values within\nwe will recursively call this function on the interval splits.\n\nThis is sufficient to generate multiple correct versions of memset, but to\nachieve the best performance we need to take into account one more factor: when\nwe're dealing with such small sizes, even the code location matters a lot. E.g.\nif we swap Then and Else branches and invert the condition, we might get a\ndifferent performance of the final function.\n\nTo account for that, let's add one more implementation of our function, but now\nwith branches swapped:\n--- cell type: code ---\n@adaptive\n@always_inline\nfn memset_impl_layer[\n    lower: Int, upper: Int\n](ptr: ptr_type, value: type, count: Int):\n    alias cur: Int\n    autotune_fork[Int, 0, 4, 8, 16, 32 -> cur]()\n\n    constrained[cur > lower]()\n    constrained[cur < upper]()\n\n    if count <= cur:\n        memset_impl_layer[lower, min(cur, upper)](ptr, value, count)\n    else:\n        memset_impl_layer[max(cur, lower), upper](ptr, value, count)\n--- cell type: markdown ---\nWe defined building blocks for our implementation, now we need to add a top\nlevel entry-point that will kick off the recursion we've just defined.\n\nWe will simply call our function with [-100,100] interval - -100 and 100 simply\ndesignate that no checks have been performed yet. This interval will be refined\nas we generate more and more check until we have enough to emit actual stores.\n--- cell type: code ---\n@adaptive\nfn memset_autotune_impl(ptr: ptr_type, value: type, count: Int, /):\n    memset_impl_layer[-100, 100](ptr, value, count)\n--- cell type: markdown ---\nOk, we're done with our memset implementation, now we just need to plug it to\nautotuning infrastructure to let the Mojo compiler do the search and pick the\nbest implementation.\n\nTo do that, we need to define an evaluator - this is a function that will take\nan array of function pointers to all implementations of our function and will\nneed to return an index of the best candidate.\n\nThere are no limitations in how this function can be implemented - it can\nreturn the first or a random candidate, or it can actually benchmark all of\nthem and pick the fastest - this is what we're going to do for this example.\n--- cell type: code ---\nfn memset_evaluator(funcs: Pointer[fn_type], size: Int) -> Int:\n    # This size is picked at random, in real code we could use a real size\n    # distribution here.\n    let size_to_optimize_for = 17\n\n    var best_idx: Int = -1\n    var best_time: Int = -1\n\n    alias eval_iterations = MULT\n    alias eval_samples = 500\n\n    # Find the function that's the fastest on the size we're optimizing for\n    for f_idx in range(size):\n        let func = funcs.load(f_idx)\n        let cur_time = measure_time(\n            func, size_to_optimize_for, eval_iterations, eval_samples\n        )\n        if best_idx < 0:\n            best_idx = f_idx\n            best_time = cur_time\n        if best_time > cur_time:\n            best_idx = f_idx\n            best_time = cur_time\n\n    return best_idx\n--- cell type: markdown ---\nThe evaluator is ready, the last brush stroke is to add a function that will\ncall the best candidate.\n\nThe search will be performed at compile time, and at runtime we will go\ndirectly to the best implementation.\n--- cell type: code ---\nfn memset_autotune(ptr: ptr_type, value: type, count: Int):\n    # Get the set of all candidates\n    alias candidates = memset_autotune_impl.__adaptive_set\n\n    # Use the evaluator to select the best candidate.\n    alias best_impl: fn_type\n    search[fn_type, VariadicList(candidates), memset_evaluator -> best_impl]()\n\n    # Run the best candidate\n    return best_impl(ptr, value, count)\n--- cell type: markdown ---\nWe are now ready to benchmark our function, let's see how its performance looks!\n--- cell type: code ---\nbenchmark(memset_manual, \"Mojo manual memset\")\nbenchmark(memset_manual_2, \"Mojo manual memset v2\")\nbenchmark(memset_system, \"Mojo system memset\")\nbenchmark(memset_autotune, \"Mojo autotune memset\")"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/README.md",
        "content": "# Mojo Jupyter notebooks\n\nMojo supports programming in [Jupyter notebooks](https://jupyter.org/), just\nlike Python.\n\nThis page explains how to get started with Mojo notebooks, and this repo\ndirectory contains notebooks that demonstrate some of Mojo's features\n(most of which we originally published on the [Mojo\nPlayground](https://playground.modular.com/)).\n\nIf you're not familiar with Jupyter notebooks, they're files that allow you to\ncreate documents with live code, equations, visualizations, and explanatory\ntext. They're basically documents with executable code blocks, making them\ngreat for sharing code experiments and programming tutorials. We actually wrote\nthe [Mojo programming\nmanual](https://docs.modular.com/mojo/programming-manual.html) as a Jupyter\nnotebook, so we can easily test all the code samples.\n\nAnd because Mojo allows you to import Python modules, you can use visualization\nlibraries in your notebooks to draw graphs and charts, or display images. For\nan example, check out the `Mandelbrot.ipynb` notebook, which uses `matplotlib`\nto draw the Mandelbrot set calculated in Mojo, and the `RayTracing.ipynb`\nnotebook, which draws images using `numpy`.\n\n## Get started in VS Code\n\nVisual Studio Code is a great environment for programming with Jupyter\nnotebooks. Especially if you're developing with Mojo on a remote system, using\nVS Code is ideal because it allows you to edit and interact with notebooks on\nthe remote machine where you've installed Mojo.\n\nAll you need is the Mojo SDK and the Jupyter VS Code extension:\n\n1. Install the [Mojo SDK](https://developer.modular.com/download).\n\n2. Install [Visual Studio Code](https://code.visualstudio.com/) and the\n   [Jupyter\n   extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter).\n\n3. Then open any `.ipynb` file with Mojo code, click **Select Kernel** in the\n   top-right corner of the document, and then select **Jupyter Kernel > Mojo**.\n\n   The Mojo kernel should have been installed automatically when you installed\n   the Mojo SDK. If the Mojo kernel is not listed, make sure that your\n   `$MODULAR_HOME` environment variable is set on the system where you\n   installed the Mojo SDK (specified in the `~/.profile` or `~/.bashrc` file).\n\n   Now run some Mojo code!\n\n## Get started with JupyterLab\n\nYou can also select the Mojo kernel when running notebooks in a local instance\nof JupyterLab. The following is just a quick setup guide for Linux users with\nthe Mojo SDK installed locally, and it might not work with your system (these\ninstructions don't support remote access to the JupyterLab). For more details\nabout using JupyterLab, see the complete [JupyterLab installation\nguide](https://jupyterlab.readthedocs.io/en/latest/getting_started/installation.html).\n\n**Note:** You must run this setup on the same machine where you've installed\nthe [Mojo SDK](https://developer.modular.com/download). However, syntax\nhighlighting for Mojo code is not currently enabled in JupyterLab (coming soon).\n\n1. Install JupyterLab:\n\n    ```sh\n    python3 -m pip install jupyterlab\n    ```\n\n2. Make sure the user-level `bin` is in your `$PATH`:\n\n    ```sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n3. Launch JupyterLab:\n\n    ```sh\n    jupyter lab\n    ```\n\n4. When you open any of the `.ipynb` notebooks from this repository, JupyterLab\n   should automatically select the Mojo kernel (which was installed with the\n   Mojo SDK).\n\n   Now run some Mojo code!\n\n## Notes and tips\n\n- Code in a Jupyter notebook cell behaves like code in a Mojo REPL environment:\n  The `main()` function is not required, but there are some caveats:\n\n  - Top-level variables (variables declared outside a function) are not visible\n    inside functions.\n\n  - Redefining undeclared variables is not supported (variables without a `let`\n    or `var` in front). If youâ€™d like to redefine a variable across notebook\n    cells, you must declare the variable with either `let` or `var`.\n\n- You can use `%%python` at the top of a code cell and write normal Python\n  code. Variables, functions, and imports defined in a Python cell are available\n  from subsequent Mojo code cells.\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/RayTracing.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# Ray tracing in Mojo\n--- cell type: markdown ---\nThis tutorial about [ray tracing](https://en.wikipedia.org/wiki/Ray_tracing_(graphics)) is based on the popular tutorial [Understandable RayTracing in C++](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing). The mathematical explanations are well described in that tutorial, so we'll just point you to the appropriate sections for reference as we implement a basic ray tracer in Mojo.\n--- cell type: markdown ---\n## Step 1: Basic definitions\n\nWe'll start by defining a `Vec3f` struct, which will use to represent a vector in 3D space as well as RGB pixels. We'll use a `SIMD` representation for our vector to enable vectorized operations. Note that since the SIMD type only allows a power of 2, we always pad the underlying storage with a 0.\n--- cell type: code ---\nfrom math import rsqrt\n\n\n@register_passable(\"trivial\")\nstruct Vec3f:\n    var data: SIMD[DType.float32, 4]\n\n    @always_inline\n    fn __init__(x: Float32, y: Float32, z: Float32) -> Self:\n        return Vec3f {data: SIMD[DType.float32, 4](x, y, z, 0)}\n\n    @always_inline\n    fn __init__(data: SIMD[DType.float32, 4]) -> Self:\n        return Vec3f {data: data}\n\n    @always_inline\n    @staticmethod\n    fn zero() -> Vec3f:\n        return Vec3f(0, 0, 0)\n\n    @always_inline\n    fn __sub__(self, other: Vec3f) -> Vec3f:\n        return self.data - other.data\n\n    @always_inline\n    fn __add__(self, other: Vec3f) -> Vec3f:\n        return self.data + other.data\n\n    @always_inline\n    fn __matmul__(self, other: Vec3f) -> Float32:\n        return (self.data * other.data).reduce_add()\n\n    @always_inline\n    fn __mul__(self, k: Float32) -> Vec3f:\n        return self.data * k\n\n    @always_inline\n    fn __neg__(self) -> Vec3f:\n        return self.data * -1.0\n\n    @always_inline\n    fn __getitem__(self, idx: Int) -> SIMD[DType.float32, 1]:\n        return self.data[idx]\n\n    @always_inline\n    fn cross(self, other: Vec3f) -> Vec3f:\n        let self_zxy = self.data.shuffle[2, 0, 1, 3]()\n        let other_zxy = other.data.shuffle[2, 0, 1, 3]()\n        return (self_zxy * other.data - self.data * other_zxy).shuffle[\n            2, 0, 1, 3\n        ]()\n\n    @always_inline\n    fn normalize(self) -> Vec3f:\n        return self.data * rsqrt(self @ self)\n--- cell type: markdown ---\nWe now define our `Image` struct, which will store the RGB pixels of our images. It also contains a method to conver this Mojo struct into a numpy image, which will be used for implementing a straightforward displaying mechanism. We will also implement a function for loading PNG files from disk.\n--- cell type: markdown ---\nFirst install the required libraries:\n--- cell type: code ---\n%%python\nfrom importlib.util import find_spec\nimport shutil\nimport subprocess\n\nfix = \"\"\"\n-------------------------------------------------------------------------\nfix following the steps here:\n    https://github.com/modularml/mojo/issues/1085#issuecomment-1771403719\n-------------------------------------------------------------------------\n\"\"\"\n\ndef install_if_missing(name: str):\n    if find_spec(name):\n        return\n\n    print(f\"{name} not found, installing...\")\n    try:\n        if shutil.which('python3'): python = \"python3\"\n        elif shutil.which('python'): python = \"python\"\n        else: raise (\"python not on path\" + fix)\n        subprocess.check_call([python, \"-m\", \"pip\", \"install\", name])\n    except:\n        raise ImportError(f\"{name} not found\" + fix)\n\ninstall_if_missing(\"numpy\")\ninstall_if_missing(\"matplotlib\")\n--- cell type: code ---\nfrom python import Python\nfrom python.object import PythonObject\n\nstruct Image:\n    # reference count used to make the object efficiently copyable\n    var rc: Pointer[Int]\n    # the two dimensional image is represented as a flat array\n    var pixels: Pointer[Vec3f]\n    var height: Int\n    var width: Int\n\n    fn __init__(inout self, height: Int, width: Int):\n        self.height = height\n        self.width = width\n        self.pixels = Pointer[Vec3f].alloc(self.height * self.width)\n        self.rc = Pointer[Int].alloc(1)\n        self.rc.store(1)\n\n    fn __copyinit__(inout self, other: Self):\n        other._inc_rc()\n        self.pixels = other.pixels\n        self.rc = other.rc\n        self.height = other.height\n        self.width = other.width\n\n    fn __del__(owned self):\n        self._dec_rc()\n\n    fn _get_rc(self) -> Int:\n        return self.rc.load()\n\n    fn _dec_rc(self):\n        let rc = self._get_rc()\n        if rc > 1:\n            self.rc.store(rc - 1)\n            return\n        self._free()\n\n    fn _inc_rc(self):\n        let rc = self._get_rc()\n        self.rc.store(rc + 1)\n\n    fn _free(self):\n        self.rc.free()\n        self.pixels.free()\n\n    @always_inline\n    fn set(self, row: Int, col: Int, value: Vec3f) -> None:\n        self.pixels.store(self._pos_to_index(row, col), value)\n\n    @always_inline\n    fn _pos_to_index(self, row: Int, col: Int) -> Int:\n        # Convert a (rol, col) position into an index in the underlying linear storage\n        return row * self.width + col\n\n    def to_numpy_image(self) -> PythonObject:\n        let np = Python.import_module(\"numpy\")\n        let plt = Python.import_module(\"matplotlib.pyplot\")\n\n        let np_image = np.zeros((self.height, self.width, 3), np.float32)\n\n        # We use raw pointers to efficiently copy the pixels to the numpy array\n        let out_pointer = Pointer(\n            __mlir_op.`pop.index_to_pointer`[\n                _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n            ](\n                SIMD[DType.index, 1](\n                    np_image.__array_interface__[\"data\"][0].__index__()\n                ).value\n            )\n        )\n        let in_pointer = Pointer(\n            __mlir_op.`pop.index_to_pointer`[\n                _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n            ](SIMD[DType.index, 1](self.pixels.__as_index()).value)\n        )\n\n        for row in range(self.height):\n            for col in range(self.width):\n                let index = self._pos_to_index(row, col)\n                for dim in range(3):\n                    out_pointer.store(\n                        index * 3 + dim, in_pointer[index * 4 + dim]\n                    )\n\n        return np_image\n\n\ndef load_image(fname: String) -> Image:\n    let np = Python.import_module(\"numpy\")\n    let plt = Python.import_module(\"matplotlib.pyplot\")\n\n    let np_image = plt.imread(fname)\n    let rows = np_image.shape[0].__index__()\n    let cols = np_image.shape[1].__index__()\n    let image = Image(rows, cols)\n\n    let in_pointer = Pointer(\n        __mlir_op.`pop.index_to_pointer`[\n            _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n        ](\n            SIMD[DType.index, 1](\n                np_image.__array_interface__[\"data\"][0].__index__()\n            ).value\n        )\n    )\n    let out_pointer = Pointer(\n        __mlir_op.`pop.index_to_pointer`[\n            _type=__mlir_type[`!kgen.pointer<scalar<f32>>`]\n        ](SIMD[DType.index, 1](image.pixels.__as_index()).value)\n    )\n    for row in range(rows):\n        for col in range(cols):\n            let index = image._pos_to_index(row, col)\n            for dim in range(3):\n                out_pointer.store(\n                    index * 4 + dim, in_pointer[index * 3 + dim]\n                )\n    return image\n--- cell type: markdown ---\nWe then add a function for quickly displaying an `Image` into the notebook. Our Python interop comes in quite handy.\n--- cell type: code ---\ndef render(image: Image):\n    np = Python.import_module(\"numpy\")\n    plt = Python.import_module(\"matplotlib.pyplot\")\n    colors = Python.import_module(\"matplotlib.colors\")\n    dpi = 32\n    fig = plt.figure(1, [image.height // 10, image.width // 10], dpi)\n\n    plt.imshow(image.to_numpy_image())\n    plt.axis(\"off\")\n    plt.show()\n--- cell type: markdown ---\nFinally, we test all our code so far with a simple image, which is the one rendered in the [Step 1 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-1-write-an-image-to-the-disk).\n--- cell type: code ---\nlet image = Image(192, 256)\n\nfor row in range(image.height):\n    for col in range(image.width):\n        image.set(\n            row,\n            col,\n            Vec3f(Float32(row) / image.height, Float32(col) / image.width, 0),\n        )\n\nrender(image)\n--- cell type: markdown ---\n## Step 2: Ray tracing\n\nNow we'll perform ray tracing from a camera into a scene with a sphere. Before reading the code below, we suggest you read more about how this works conceptually from [Step 2 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-2-the-crucial-one-ray-tracing).\n--- cell type: markdown ---\nWe first define the `Material` and `Sphere` structs, which are the new data structures we'll need.\n--- cell type: code ---\nfrom math import sqrt\n\n\n@register_passable(\"trivial\")\nstruct Material:\n    var color: Vec3f\n    var albedo: Vec3f\n    var specular_component: Float32\n\n    fn __init__(color: Vec3f) -> Material:\n        return Material {\n            color: color, albedo: Vec3f(0, 0, 0), specular_component: 0\n        }\n\n    fn __init__(\n        color: Vec3f, albedo: Vec3f, specular_component: Float32\n    ) -> Material:\n        return Material {\n            color: color, albedo: albedo, specular_component: specular_component\n        }\n\n\nalias W = 1024\nalias H = 768\nalias bg_color = Vec3f(0.02, 0.02, 0.02)\nlet shiny_yellow = Material(Vec3f(0.95, 0.95, 0.4), Vec3f(0.7, 0.6, 0), 30.0)\nlet green_rubber = Material(Vec3f( 0.3,  0.7, 0.3), Vec3f(0.9, 0.1, 0), 1.0)\n\n\n@register_passable(\"trivial\")\nstruct Sphere:\n    var center: Vec3f\n    var radius: Float32\n    var material: Material\n\n    fn __init__(c: Vec3f, r: Float32, material: Material) -> Self:\n        return Sphere {center: c, radius: r, material: material}\n\n    @always_inline\n    fn intersects(self, orig: Vec3f, dir: Vec3f, inout dist: Float32) -> Bool:\n        \"\"\"This method returns True if a given ray intersects this sphere.\n        And if it does, it writes in the `dist` parameter the distance to the\n        origin of the ray.\n        \"\"\"\n        let L = orig - self.center\n        let a = dir @ dir\n        let b = 2 * (dir @ L)\n        let c = L @ L - self.radius * self.radius\n        let discriminant = b * b - 4 * a * c\n        if discriminant < 0:\n            return False\n        if discriminant == 0:\n            dist = -b / 2 * a\n            return True\n        let q = -0.5 * (b + sqrt(discriminant)) if b > 0 else -0.5 * (\n            b - sqrt(discriminant)\n        )\n        var t0 = q / a\n        let t1 = c / q\n        if t0 > t1:\n            t0 = t1\n        if t0 < 0:\n            t0 = t1\n            if t0 < 0:\n                return False\n\n        dist = t0\n        return True\n--- cell type: markdown ---\nWe then define a `cast_ray` method, which will be used to figure out the color of a particular pixel in the image we'll produce. It basically works by identifying whether this ray intersects the sphere or not.\n--- cell type: code ---\nfn cast_ray(orig: Vec3f, dir: Vec3f, sphere: Sphere) -> Vec3f:\n    var dist: Float32 = 0\n    if not sphere.intersects(orig, dir, dist):\n        return bg_color\n\n    return sphere.material.color\n--- cell type: markdown ---\nLastly, we parallelize the ray tracing for every pixel row-wise.\n--- cell type: code ---\nfrom math import tan, acos\nfrom algorithm import parallelize\n\n\nfn create_image_with_sphere(sphere: Sphere, height: Int, width: Int) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(row, col, cast_ray(Vec3f.zero(), dir, sphere))\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nrender(\n    create_image_with_sphere(Sphere(Vec3f(-3, 0, -16), 2, shiny_yellow), H, W)\n)\n--- cell type: markdown ---\n## Step 3: More spheres\n\nThis section corresponds to the [Step 3 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-3-add-more-spheres). \n\nWe include here all the necessary changes:\n\n- We add 3 more spheres to the scene, 2 of them being of ivory material.\n- When we intersect the ray with the sphere, we render the color of the closest sphere.\n--- cell type: code ---\nfrom algorithm import parallelize\nfrom utils.vector import DynamicVector\nfrom math.limit import inf\n\n\nfn scene_intersect(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    background: Material,\n) -> Material:\n    var spheres_dist = inf[DType.float32]()\n    var material = background\n\n    for i in range(0, spheres.size):\n        var dist = inf[DType.float32]()\n        if spheres[i].intersects(orig, dir, dist) and dist < spheres_dist:\n            spheres_dist = dist\n            material = spheres[i].material\n\n    return material\n\n\nfn cast_ray(\n    orig: Vec3f, dir: Vec3f, spheres: DynamicVector[Sphere]\n) -> Material:\n    let background = Material(Vec3f(0.02, 0.02, 0.02))\n    return scene_intersect(orig, dir, spheres, background)\n\n\nfn create_image_with_spheres(\n    spheres: DynamicVector[Sphere], height: Int, width: Int\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(row, col, cast_ray(Vec3f.zero(), dir, spheres).color)\n\n    parallelize[_process_row](height)\n\n    return image\n\nlet spheres = DynamicVector[Sphere]()\nspheres.push_back(Sphere(Vec3f(-3,      0, -16),   2, shiny_yellow))\nspheres.push_back(Sphere(Vec3f(-1.0, -1.5, -12), 1.8, green_rubber))\nspheres.push_back(Sphere(Vec3f( 1.5, -0.5, -18),   3, green_rubber))\nspheres.push_back(Sphere(Vec3f( 7,      5, -18),   4, shiny_yellow))\n\nrender(create_image_with_spheres(spheres, H, W))\n--- cell type: markdown ---\n## Step 4: Add lighting\n\nThis section corresponds to the [Step 4 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-4-lighting). Please read that section for an explanation of the trick used to estimate the light intensity of pixel based on the angle of intersection between each ray and the spheres. The changes are minimal and are primarily about handling this intersection angle.\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct Light:\n    var position: Vec3f\n    var intensity: Float32\n\n    fn __init__(p: Vec3f, i: Float32) -> Self:\n        return Light {position: p, intensity: i}\n--- cell type: code ---\nfrom math import max\n\n\nfn scene_intersect(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    inout material: Material,\n    inout hit: Vec3f,\n    inout N: Vec3f,\n) -> Bool:\n    var spheres_dist = inf[DType.float32]()\n\n    for i in range(0, spheres.size):\n        var dist: Float32 = 0\n        if spheres[i].intersects(orig, dir, dist) and dist < spheres_dist:\n            spheres_dist = dist\n            hit = orig + dir * dist\n            N = (hit - spheres[i].center).normalize()\n            material = spheres[i].material\n\n    return (spheres_dist != inf[DType.float32]()).__bool__()\n\n\nfn cast_ray(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n) -> Material:\n    var point = Vec3f.zero()\n    var material = Material(Vec3f.zero())\n    var N = Vec3f.zero()\n    if not scene_intersect(orig, dir, spheres, material, point, N):\n        return bg_color\n\n    var diffuse_light_intensity: Float32 = 0\n    for i in range(lights.size):\n        let light_dir = (lights[i].position - point).normalize()\n        diffuse_light_intensity += lights[i].intensity * max(0, light_dir @ N)\n\n    return material.color * diffuse_light_intensity\n\n\nfn create_image_with_spheres_and_lights(\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    height: Int,\n    width: Int,\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(\n                row, col, cast_ray(Vec3f.zero(), dir, spheres, lights).color\n            )\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nlet lights = DynamicVector[Light]()\nlights.push_back(Light(Vec3f(-20, 20, 20), 1.0))\nlights.push_back(Light(Vec3f(20, -20, 20), 0.5))\n\nrender(create_image_with_spheres_and_lights(spheres, lights, H, W))\n--- cell type: markdown ---\n## Step 5: Add specular lighting\n\nThis section corresponds to the [Step 5 of the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing#step-5-specular-lighting). The changes to the code are quite minimal, but the rendered picture looks much more realistic!\n--- cell type: code ---\nfrom math import pow\n\n\nfn reflect(I: Vec3f, N: Vec3f) -> Vec3f:\n    return I - N * (I @ N) * 2.0\n\n\nfn cast_ray(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n) -> Material:\n    var point = Vec3f.zero()\n    var material = Material(Vec3f.zero())\n    var N = Vec3f.zero()\n    if not scene_intersect(orig, dir, spheres, material, point, N):\n        return bg_color\n\n    var diffuse_light_intensity: Float32 = 0\n    var specular_light_intensity: Float32 = 0\n    for i in range(lights.size):\n        let light_dir = (lights[i].position - point).normalize()\n        diffuse_light_intensity += lights[i].intensity * max(0, light_dir @ N)\n        specular_light_intensity += (\n            pow(\n                max(0.0, -reflect(-light_dir, N) @ dir),\n                material.specular_component,\n            )\n            * lights[i].intensity\n        )\n\n    let result = material.color * diffuse_light_intensity * material.albedo.data[\n        0\n    ] + Vec3f(\n        1.0, 1.0, 1.0\n    ) * specular_light_intensity * material.albedo.data[\n        1\n    ]\n    let result_max = max(result[0], max(result[1], result[2]))\n    # Cap the resulting vector\n    if result_max > 1:\n        return result * (1.0 / result_max)\n    return result\n\n\nfn create_image_with_spheres_and_specular_lights(\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    height: Int,\n    width: Int,\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(\n                row, col, cast_ray(Vec3f.zero(), dir, spheres, lights).color\n            )\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nrender(create_image_with_spheres_and_specular_lights(spheres, lights, H, W))\n--- cell type: markdown ---\n## Step 6: Add background\n\nAs a last step, let's use an image for the background instead of a uniform fill. The only code that we need to change is the code where we used to return `bg_color`. Now we will determine a point in the background image to which the ray is directed and draw that.\n--- cell type: code ---\nfrom math import abs\n\n\nfn cast_ray(\n    orig: Vec3f,\n    dir: Vec3f,\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    bg: Image,\n) -> Material:\n    var point = Vec3f.zero()\n    var material = Material(Vec3f.zero())\n    var N = Vec3f.zero()\n    if not scene_intersect(orig, dir, spheres, material, point, N):\n        # Background\n        # Given a direction vector `dir` we need to find a pixel in the image\n        let x = dir[0]\n        let y = dir[1]\n\n        # Now map x from [-1,1] to [0,w-1] and do the same for y.\n        let w = bg.width\n        let h = bg.height\n        let col = ((1.0 + x) * 0.5 * (w - 1)).to_int()\n        let row = ((1.0 + y) * 0.5 * (h - 1)).to_int()\n        return Material(bg.pixels[bg._pos_to_index(row, col)])\n\n    var diffuse_light_intensity: Float32 = 0\n    var specular_light_intensity: Float32 = 0\n    for i in range(lights.size):\n        let light_dir = (lights[i].position - point).normalize()\n        diffuse_light_intensity += lights[i].intensity * max(0, light_dir @ N)\n        specular_light_intensity += (\n            pow(\n                max(0.0, -reflect(-light_dir, N) @ dir),\n                material.specular_component,\n            )\n            * lights[i].intensity\n        )\n\n    let result = material.color * diffuse_light_intensity * material.albedo.data[\n        0\n    ] + Vec3f(\n        1.0, 1.0, 1.0\n    ) * specular_light_intensity * material.albedo.data[\n        1\n    ]\n    let result_max = max(result[0], max(result[1], result[2]))\n    # Cap the resulting vector\n    if result_max > 1:\n        return result * (1.0 / result_max)\n    return result\n\n\nfn create_image_with_spheres_and_specular_lights(\n    spheres: DynamicVector[Sphere],\n    lights: DynamicVector[Light],\n    height: Int,\n    width: Int,\n    bg: Image,\n) -> Image:\n    let image = Image(height, width)\n\n    @parameter\n    fn _process_row(row: Int):\n        let y = -((2.0 * row + 1) / height - 1)\n        for col in range(width):\n            let x = ((2.0 * col + 1) / width - 1) * width / height\n            let dir = Vec3f(x, y, -1).normalize()\n            image.set(\n                row, col, cast_ray(Vec3f.zero(), dir, spheres, lights, bg).color\n            )\n\n    parallelize[_process_row](height)\n\n    return image\n\n\nlet bg = load_image(\"images/background.png\")\nrender(\n    create_image_with_spheres_and_specular_lights(spheres, lights, H, W, bg)\n)\n--- cell type: markdown ---\n## Next steps\n\nWe've only explored the basics of ray tracing here, but you can add shadows, reflections and so much more! Fortunately these are explained in [the C++ tutorial](https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing), and we leave the corresponding Mojo implementations as an exercise for you."
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/notebooks/programming-manual.ipynb",
        "content": "--- cell type: markdown ---\n*Copyright 2023 Modular, Inc: Licensed under the Apache License v2.0 with LLVM Exceptions.*\n--- cell type: markdown ---\n# MojoðŸ”¥ programming manual\n--- cell type: markdown ---\n\nMojo is a programming language that is as easy to use as Python but with the\nperformance of C++ and Rust. Furthermore, Mojo provides the ability to leverage\nthe entire Python library ecosystem.\n\nMojo achieves this feat by utilizing next-generation compiler technologies with\nintegrated caching, multithreading, and cloud distribution technologies.\nFurthermore, Mojo's autotuning and compile-time metaprogramming features allow\nyou to write code that is portable to even the most exotic hardware.\n\nMore importantly, **Mojo allows you to leverage the entire Python ecosystem**\nso you can continue to use tools you are familiar with. Mojo is designed to\nbecome a **superset** of Python over time by preserving Python's dynamic\nfeatures while adding new primitives for [systems\nprogramming](https://en.wikipedia.org/wiki/Systems_programming). These new\nsystem programming primitives will allow Mojo developers to build\nhigh-performance libraries that currently require C, C++, Rust, CUDA, and other\naccelerator systems. By bringing together the best of dynamic languages and\nsystems languages, we hope to provide a **unified** programming model that\nworks across levels of abstraction, is friendly for novice programmers, and\nscales across many use cases from accelerators through to application\nprogramming and scripting.\n\nThis document is an introduction to the Mojo programming language, not a\ncomplete language guide. It assumes knowledge of Python and systems programming\nconcepts. At the moment, Mojo is still a work in progress and the documentation\nis targeted to developers with systems programming experience. As the language\ngrows and becomes more broadly available, we intend for it to be friendly and\naccessible to everyone, including beginner programmers. It's just not there\ntoday.\n\n--- cell type: markdown ---\n## Using the Mojo compiler\n\nWith the [Mojo SDK](https://docs.modular.com/mojo/manual/get-started/), you can\nrun a Mojo program from a terminal just like you can with Python. So if you\nhave a file named `hello.mojo` (or `hello.ðŸ”¥`â€”yes, the file extension can be an\nemoji!), just type `mojo hello.mojo`:\n\n```mojo\n$ cat hello.ðŸ”¥\ndef main():\n    print(\"hello world\")\n    for x in range(9, 0, -3):\n        print(x)\n$ mojo hello.ðŸ”¥\nhello world\n9\n6\n3\n$\n```\n\nAgain, you can use either the `.ðŸ”¥` or `.mojo` suffix.\n\nFor more details about the Mojo compiler tools, see the [`mojo` CLI\ndocs](https://docs.modular.com/mojo/cli/).\n--- cell type: markdown ---\n## Basic systems programming extensions\n\nGiven our goal of compatibility and Python's strength with high-level\napplications and dynamic APIs, we don't have to spend much time explaining\nhow those portions of the language work. On the other hand, Python's support\nfor systems programming is mainly delegated to C, and we want to provide a\nsingle system that is great in that world. As such, this section breaks down\neach major component and feature and describes how to use them with examples.\n\n### `let` and `var` declarations\n\nInside a `def` in Mojo, you may assign a value to a name and it implicitly\ncreates a function scope variable just like in Python. This provides a very\ndynamic and low-ceremony way to write code, but it is a challenge for two\nreasons:\n\n1) Systems programmers often want to declare that a value is immutable for\n    type-safety and performance.\n2) They may want to get an error if they mistype a variable name in an\n    assignment.\n\nTo support this, Mojo provides scoped runtime value declarations: `let` is\nimmutable, and `var` is mutable. These values use lexical scoping and support\nname shadowing:\n--- cell type: code ---\ndef your_function(a, b):\n    let c = a\n    # Uncomment to see an error:\n    # c = b  # error: c is immutable\n\n    if c != b:\n        let d = b\n        print(d)\n\nyour_function(2, 3)\n\n--- cell type: markdown ---\n`let` and `var` declarations support type specifiers as well as patterns, and\nlate initialization:\n--- cell type: code ---\ndef your_function():\n    let x: Int = 42\n    let y: Float64 = 17.0\n\n    let z: Float32\n    if x != 0:\n        z = 1.0\n    else:\n        z = foo()\n    print(z)\n\ndef foo() -> Float32:\n    return 3.14\n\nyour_function()\n\n--- cell type: markdown ---\nNote that `let` and `var` are completely optional when in a `def` function\n(you can instead use implicitly declared values, just like Python),\nbut they're required for all variables in an `fn` function.\n\nAlso beware that when using Mojo in a REPL environment (such as this notebook),\ntop-level variables (variables that live outside a function or struct) are\ntreated like variables in a `def`, so they allow implicit value type\ndeclarations (they do not require `var` or `let` declarations, nor type\ndeclarations). This matches the Python REPL behavior.\n\n### `struct` types\n\nMojo is based on MLIR and LLVM, which offer a cutting-edge compiler and code\ngeneration system used in many programming languages. This lets us have better\ncontrol over data organization, direct access to data fields, and other ways to\nimprove performance. An important feature of modern systems programming\nlanguages is the ability to build high-level and safe abstractions on top of\nthese complex, low-level operations without any performance loss. In Mojo, this\nis provided by the `struct` type.\n\nA `struct` in Mojo is similar to a Python `class`: they both support methods,\nfields, operator overloading, decorators for metaprogramming, etc. Their\ndifferences are as follows:\n\n- Python classes are dynamic: they allow for dynamic dispatch, monkey-patching\n(or \"swizzling\"), and dynamically binding instance properties at runtime.\n\n- Mojo structs are static: they are bound at compile-time (you cannot add\nmethods at runtime). Structs allow you to trade flexibility for performance\nwhile being safe and easy to use.\n\nHere's a simple definition of a struct:\n--- cell type: code ---\nstruct MyPair:\n    var first: Int\n    var second: Int\n\n    # We use 'fn' instead of 'def' here - we'll explain that soon\n    fn __init__(inout self, first: Int, second: Int):\n        self.first = first\n        self.second = second\n\n    fn __lt__(self, rhs: MyPair) -> Bool:\n        return self.first < rhs.first or\n              (self.first == rhs.first and\n               self.second < rhs.second)\n\n--- cell type: markdown ---\nSyntactically, the biggest difference compared to a Python `class` is that all\ninstance properties in a `struct` **must** be explicitly declared with a `var`\nor `let` declaration.\n\nIn Mojo, the structure and contents of a \"struct\" are set in advance and can't\nbe changed while the program is running. Unlike in Python, where you can add,\nremove, or change attributes of an object on the fly, Mojo doesn't allow that\nfor structs. This means you can't use `del` to remove a method or change its\nvalue in the middle of running the program.\n\nHowever, the static nature of `struct` has some great benefits! It helps Mojo\nrun your code faster. The program knows exactly where to find the struct's\ninformation and how to use it without any extra steps or delays.\n\nMojo's structs also work really well with features you might already know from\nPython, like operator overloading (which lets you change how math symbols like\n`+` and `-` work with your own data). Furthermore, *all* the \"standard types\"\n(like `Int`, `Bool`, `String` and even `Tuple`) are made using structs. This\nmeans they're part of the standard set of tools you can use, rather than being\nhardwired into the language itself. This gives you more flexibility and control\nwhen writing your code.\n\n<div class=\"alert alert-block alert-success\">\n\nIf you're wondering what the `inout` means on the `self` argument: this\nindicates that the argument is mutable and changes made inside the function are\nvisible to the caller. For details, see below about\n[inout arguments](#mutable-arguments-inout).\n\n</div>\n--- cell type: markdown ---\n#### `Int` vs `int`\n\nIn Mojo, you might notice that we use `Int` (with a capital \"I\"), which is\ndifferent from Python's `int` (with a lowercase \"i\"). This difference is on\npurpose, and it's actually a good thing!\n\nIn Python, the `int` type can handle really big numbers and has some extra\nfeatures, like checking if two numbers are the same object. But this comes with\nsome extra baggage that can slow things down. Mojo's `Int` is different. It's\ndesigned to be simple, fast, and tuned for your computer's hardware to handle\nquickly.\n\nWe made this choice for two main reasons:\n\n1. We want to give programmers who need to work closely with computer hardware\n(systems programmers) a transparent and reliable way to interact with hardware.\nWe don't want to rely on fancy tricks (like JIT compilers) to make things\nfaster.\n\n2. We want Mojo to work well with Python without causing any issues. By using a\ndifferent name (Int instead of int), we can keep both types in Mojo without\nchanging how Python's int works.\n\nAs a bonus, `Int` follows the same naming style as other custom data types you\nmight create in Mojo. Additionally, `Int` is a `struct` that's included in\nMojo's standard set of tools.\n\n### Strong type checking\n\nEven though you can still use flexible types like in Python, Mojo lets you use\nstrict type checking. Type-checking can make your code more predictable,\nmanageable, and secure.\n\nOne of the primary ways to employ strong type checking is with Mojo's `struct`\ntype. A `struct` definition in Mojo defines a compile-time-bound name, and\nreferences to that name in a type context are treated as a strong specification\nfor the value being defined. For example, consider the following code that uses\nthe `MyPair` struct shown above:\n--- cell type: code ---\ndef pair_test() -> Bool:\n    let p = MyPair(1, 2)\n    # Uncomment to see an error:\n    # return p < 4 # gives a compile time error\n    return True\n\n--- cell type: markdown ---\nIf you uncomment the first return statement and run it, youâ€™ll get a\ncompile-time error telling you that `4` cannot be converted to `MyPair`, which\nis what the right-hand-side of `__lt__()` requires (in the `MyPair` definition).\n\nThis is a familiar experience when working with systems programming languages,\nbut it's not how Python works. Python has syntactically identical features for\n[MyPy](https://mypy.readthedocs.io/) type annotations, but they are not\nenforced by the compiler: instead, they are hints that inform static analysis.\nBy tying types to specific declarations, Mojo can handle both the classical\ntype annotation hints and strong type specifications without breaking\ncompatibility.\n\nType checking isn't the only use-case for strong types. Since we know the types\nare accurate, we can optimize the code based on those types, pass values in\nregisters, and be as efficient as C for argument passing and other low-level\ndetails. This is the foundation of the safety and predictability guarantees\nMojo provides to systems programmers.\n\n### Overloaded functions and methods\n\nLike Python, you can define functions in Mojo without specifying argument data\ntypes and Mojo will handle them dynamically. This is nice when you want\nexpressive APIs that just work by accepting arbitrary inputs and let dynamic\ndispatch decide how to handle the data. However, when you want to ensure type\nsafety, as discussed above, Mojo also offers full support for overloaded\nfunctions and methods.\n\nThis allows you to define multiple functions with the same name but with\ndifferent arguments. This is a common feature seen in many languages, such as\nC++, Java, and Swift.\n\nWhen resolving a function call, Mojo tries each candidate and uses the one that\nworks (if only one works), or it picks the closest match (if it can determine a\nclose match), or it reports that the call is ambiguous if it can't figure\nout which one to pick. In the latter case, you can resolve the ambiguity by\nadding an explicit cast on the call site. \n\nLet's look at an example:\n--- cell type: code ---\nstruct Complex:\n    var re: Float32\n    var im: Float32\n\n    fn __init__(inout self, x: Float32):\n        \"\"\"Construct a complex number given a real number.\"\"\"\n        self.re = x\n        self.im = 0.0\n\n    fn __init__(inout self, r: Float32, i: Float32):\n        \"\"\"Construct a complex number given its real and imaginary components.\"\"\"\n        self.re = r\n        self.im = i\n\n--- cell type: markdown ---\n\nYou can overload methods in structs and classes and overload module-level\nfunctions.\n\nMojo doesn't support overloading solely on result type, and doesn't use result\ntype or contextual type information for type inference, keeping things simple,\nfast, and predictable.  Mojo will never produce an \"expression too complex\"\nerror, because its type-checker is simple and fast by definition.\n\nAgain, if you leave your argument names without type definitions, then the\nfunction behaves just like Python with dynamic types. As soon as you define a\nsingle argument type, Mojo will look for overload candidates and resolve\nfunction calls as described above.\n\nAlthough we haven't discussed parameters yet (they're different from function\narguments), you can also [overload functions and methods based on\nparameters](#overloading-on-parameters).\n\n### `fn` definitions\n\nThe extensions above are the cornerstone that provides low-level programming\nand provide abstraction capabilities, but many systems programmers prefer more\ncontrol and predictability than what `def` in Mojo provides. To recap, `def` is\ndefined by necessity to be very dynamic, flexible and generally compatible with\nPython: arguments are mutable, local variables are implicitly declared on first\nuse, and scoping isn't enforced. This is great for high level programming and\nscripting, but is not always great for systems programming. To complement this,\nMojo provides an `fn` declaration which is like a \"strict mode\" for `def`.\n\n> Alternative: instead of using a new keyword like `fn`, we could instead add a\nmodifier or decorator like `@strict def`. However, we need to take new keywords\nanyway and there is little cost to doing so. Also, in practice in systems\nprogramming domains, `fn` is used all the time so it probably makes sense to\nmake it first class.\n\nAs far as a caller is concerned, `fn` and `def` are interchangeable: there is\nnothing a `def` can provide that a `fn` cannot (and vice versa). The\ndifference is that a `fn` is more limited and controlled on the *inside* of\nits body (alternatively: pedantic and strict). Specifically, `fn`s have a\nnumber of limitations compared to `def` functions:\n\n1. Argument values default to being immutable in the body of the function (like\na `let`), instead of mutable (like a `var`). This catches accidental mutations,\nand permits the use of non-copyable types as arguments.\n\n2. Argument values require a type specification (except for `self` in a\nmethod), catching accidental omission of type specifications. Similarly, a\nmissing return type specifier is interpreted as returning `None` instead of an\nunknown return type. Note that both can be explicitly declared to return\n`object`, which allows one to opt-in to the behavior of a `def` if desired.\n\n3. Implicit declaration of local variables is disabled, so all locals must be\ndeclared. This catches name typos and dovetails with the scoping provided by\n`let` and `var`.\n\n4. Both support raising exceptions, but this must be explicitly declared on a\n`fn` with the `raises` keyword.\n\nProgramming patterns will vary widely across teams, and this level of\nstrictness will not be for everyone. We expect that folks who are used to C++\nand already use MyPy-style type annotations in Python to prefer the use of\n`fn`s, but higher level programmers and ML researchers to continue to use\n`def`. Mojo allows you to freely intermix `def` and `fn` declarations, e.g.\nimplementing some methods with one and others with the other, and allows each\nteam or programmer to decide what is best for their use-case.\n\nFor more about argument behavior in Mojo functions, see the section below about\n[Argument passing control and memory\nownership](#argument-passing-control-and-memory-ownership).\n--- cell type: markdown ---\n### The `__copyinit__`, `__moveinit__`, and `__takeinit__` special methods\n\nMojo supports full \"value semantics\" as seen in languages like C++ and Swift,\nand it makes defining simple aggregates of fields very easy with the [`@value`\ndecorator](#value-decorator).\n\nFor advanced use cases, Mojo allows you to define custom constructors (using\nPython's existing `__init__` special method), custom destructors (using the\nexisting `__del__` special method) and custom copy and move constructors using\nthe `__copyinit__`, `__moveinit__` and `__takeinit__` special methods.\n\nThese low-level customization hooks can be useful when doing low level systems\nprogramming, e.g. with manual memory management.  For example, consider a\ndynamic string type that needs to allocate memory for the string data when\nconstructed and destroy it when the value is destroyed:\n--- cell type: code ---\nfrom memory.unsafe import Pointer\n\nstruct HeapArray:\n    var data: Pointer[Int]\n    var size: Int\n\n    fn __init__(inout self, size: Int, val: Int):\n        self.size = size\n        self.data = Pointer[Int].alloc(self.size)\n        for i in range(self.size):\n            self.data.store(i, val)\n\n    fn __del__(owned self):\n        self.data.free()\n\n    fn dump(self):\n        print_no_newline(\"[\")\n        for i in range(self.size):\n            if i > 0:\n                print_no_newline(\", \")\n            print_no_newline(self.data.load(i))\n        print(\"]\")\n\n--- cell type: markdown ---\nThis array type is implemented using low level functions to show a simple\nexample of how this works. However, if you try to copy an instance of\n`HeapArray` with the `=` operator, you might be surprised:\n--- cell type: code ---\nvar a = HeapArray(3, 1)\na.dump()   # Should print [1, 1, 1]\n# Uncomment to see an error:\n# var b = a  # ERROR: Vector doesn't implement __copyinit__\n\nvar b = HeapArray(4, 2)\nb.dump()   # Should print [2, 2, 2, 2]\na.dump()   # Should print [1, 1, 1]\n\n--- cell type: markdown ---\nIf you uncomment the line to copy `a` into `b`, you'll see that Mojo doesn't\nallow you to make a copy of our array: `HeapArray` contains an instance of\n`Pointer` (which is equivalent to a low-level C pointer), and Mojo doesn't know\nwhat kind of data it points to or how to copy it. More generally, some types\n(like atomic numbers) cannot be copied or moved around because their address\nprovides an **identity** just like a class instance does.\n\nIn this case, we do want our array to be copyable. To enable this, we have to\nimplement the `__copyinit__` special method, which is conventionally\nimplemented like this:\n--- cell type: code ---\nstruct HeapArray:\n    var data: Pointer[Int]\n    var size: Int\n\n    fn __init__(inout self, size: Int, val: Int):\n        self.size = size\n        self.data = Pointer[Int].alloc(self.size)\n        for i in range(self.size):\n            self.data.store(i, val)\n\n    fn __copyinit__(inout self, existing: Self):\n        self.size = existing.size\n        self.data = Pointer[Int].alloc(self.size)\n        for i in range(self.size):\n            self.data.store(i, existing.data.load(i))\n\n    fn __del__(owned self):\n        self.data.free()\n\n    fn dump(self):\n        print_no_newline(\"[\")\n        for i in range(self.size):\n            if i > 0:\n                print_no_newline(\", \")\n            print_no_newline(self.data.load(i))\n        print(\"]\")\n\n--- cell type: markdown ---\nWith this implementation, our code above works correctly and the `b = a` copy\nproduces a logically distinct instance of the array with its own lifetime and\ndata:\n--- cell type: code ---\nvar a = HeapArray(3, 1)\na.dump()   # Should print [1, 1, 1]\n# This is no longer an error:\nvar b = a\n\nb.dump()   # Should print [1, 1, 1]\na.dump()   # Should print [1, 1, 1]\n\n--- cell type: markdown ---\nMojo also supports the `__moveinit__` method which allows both Rust-style\nmoves (which transfers a value from one place to another when the source lifetime ends) and the `__takeinit__` method for C++-style moves (where the\ncontents of a value is logically transfered out of the source, but its destructor is still run), and allows\ndefining custom move logic. For more detail, see the [Value\nLifecycle](#value-lifecycle-birth-life-and-death-of-a-value)\nsection below.\n\nMojo provides full control over the lifetime of a value, including the ability\nto make types copyable, move-only, and not-movable. This is more control than\nlanguages like Swift and Rust offer, which require values to at least be\nmovable. If you are curious how `existing` can be passed into the\n`__copyinit__` method without itself creating a copy, check out the section on\n[Borrowed arguments](#immutable-arguments-borrowed) below.\n--- cell type: markdown ---\n## Argument passing control and memory ownership\n\nIn both Python and Mojo, much of the language revolves around function calls: a\nlot of the (apparently) built-in behaviors are implemented in the standard\nlibrary with \"dunder\" (double-underscore) methods. Inside these\nmagic functions is where a lot of memory ownership is determined through\nargument passing.\n\nLet's review some details about how Python and Mojo pass arguments:\n\n- All values passed into a *Python* `def` function use reference semantics. This\nmeans the function can modify mutable objects passed into it and those changes\nare visible outside the function. However, the behavior is sometimes surprising\nfor the uninitiated, because you can change the object that an argument points\nto and that change is not visible outside the function.\n\n- All values passed into a *Mojo* `def` function use value semantics by default.\nCompared to Python, this is an important difference: A Mojo `def` function\nreceives a copy of all argumentsâ€”it can modify arguments inside the function,\nbut the changes are **not** visible outside the function.\n\n- All values passed into a Mojo [`fn` function](#fn-definitions) are immutable\nreferences by default. This means the function can read the original object (it\nis *not* a copy), but it cannot modify the object at all.\n\nThis convention for immutable argument passing in a Mojo `fn` is called\n\"borrowing.\" In the following sections, we'll explain how you can change the\nargument passing behavior in Mojo, for both `def` and `fn` functions.\n\n### Why argument conventions are important\n\nIn Python, all fundamental values are references to objectsâ€”as described above,\na Python function can modify the original object. Thus, Python developers are\nused to thinking about everything as reference semantic. However, at the\nCPython or machine level, you can see that the references themselves are\nactually passed *by-copy*â€”Python copies a pointer and adjusts reference counts.\n\nThis Python approach provides a comfortable programming model for most people,\nbut it requires all values to be heap-allocated (and results are occasionally\nsurprising due to reference sharing). Mojo classes (TODO: will) follow\nthe same reference-semantic approach for most objects, but this isn't practical\nfor simple types like integers in a systems programming context. In these\nscenarios, we want the values to live on the stack or even in hardware\nregisters. As such, Mojo structs are always inlined into their container,\nwhether that be as the field of another type or into the stack frame of the\ncontaining function.\n\nThis raises some interesting questions: How do you implement methods that need\nto mutate `self` of a structure type, such as `__iadd__`? How does `let` work,\nand how does it prevent mutation? How are the lifetimes of these values\ncontrolled to keep Mojo a memory-safe language?\n\nThe answer is that the Mojo compiler uses dataflow analysis and type\nannotations to provide full control over value copies, aliasing of references,\nand mutation control. These features are similar in many ways to features in\nthe Rust language, but they work somewhat differently in order to make\nMojo easier to learn, and they integrate better into the Python ecosystem\nwithout requiring a massive annotation burden.\n\nIn the following sections, you'll learn about how you can control memory\nownership for objects passed into Mojo `fn` functions.\n--- cell type: markdown ---\n### Immutable arguments (`borrowed`)\n\nA borrowed object is an **immutable reference** to an object that a function\nreceives, instead of receiving a copy of the object. So the\ncallee function has full read-and-execute access to the object, but it cannot\nmodify it (the caller still has exclusive \"ownership\" of the object).\n\nFor example, consider this struct that we don't want to copy when passing around\ninstances of it:\n--- cell type: code ---\n# Don't worry about this code yet. It's just needed for the function below.\n# It's a type so expensive to copy around so it does not have a\n# __copyinit__ method.\nstruct SomethingBig:\n    var id_number: Int\n    var huge: HeapArray\n    fn __init__(inout self, id: Int):\n        self.huge = HeapArray(1000, 0)\n        self.id_number = id\n\n    # self is passed by-reference for mutation as described above.\n    fn set_id(inout self, number: Int):\n        self.id_number = number\n\n    # Arguments like self are passed as borrowed by default.\n    fn print_id(self):  # Same as: fn print_id(borrowed self):\n        print(self.id_number)\n\n--- cell type: markdown ---\nWhen passing an instance of `SomethingBig` to a function, it's necessary to\npass a reference because `SomethingBig` cannot be copied (it has no\n`__copyinit__` method). And, as mentioned above, `fn` arguments are immutable\nreferences by default, but you can explicitly define it with the `borrowed`\nkeyword as shown in the `use_something_big()` function here:\n--- cell type: code ---\nfn use_something_big(borrowed a: SomethingBig, b: SomethingBig):\n    \"\"\"'a' and 'b' are both immutable, because 'borrowed' is the default.\"\"\"\n    a.print_id()\n    b.print_id()\n\nlet a = SomethingBig(10)\nlet b = SomethingBig(20)\nuse_something_big(a, b)\n\n--- cell type: markdown ---\nThis default applies to all arguments uniformly, including the `self` argument\nof methods. This is much more efficient when passing large values or when\npassing expensive values like a reference-counted pointer (which is the default\nfor Python/Mojo classes), because the copy constructor and destructor don't\nhave to be invoked when passing the argument. \n\nBecause the default argument convention for `fn` functions is `borrowed`, Mojo\nhas simple and logical code that does the right thing by default. For example,\nwe don't want to copy or move all of `SomethingBig` just to invoke the\n`print_id()` method, or when calling `use_something_big()`.\n\nThis borrowed argument convention is similar in some ways to passing an\nargument by `const&` in C++, which avoids a copy of the value and disables\nmutability in the callee. However, the borrowed convention differs from\n`const&` in C++ in two important ways:\n\n1. The Mojo compiler implements a borrow checker (similar to Rust) that\nprevents code from dynamically forming mutable references to a value when there\nare immutable references outstanding, and it prevents multiple mutable\nreferences to the same value. You are allowed to have multiple borrows (as the\ncall to `use_something_big` does above) but you cannot pass something by mutable\nreference and borrow at the same time. (TODO: Not currently enabled).\n\n2. Small values like `Int`, `Float`, and `SIMD` are passed directly in machine\nregisters instead of through an extra indirection (this is because they are\ndeclared with the [`@register_passable`\ndecorator](#register_passable-struct-decorator)). This is a [significant\nperformance\nenhancement](https://www.forrestthewoods.com/blog/should-small-rust-structs-be-passed-by-copy-or-by-borrow/)\nwhen compared to languages like C++ and Rust, and moves this optimization from\nevery call site to being declarative on a type.\n\nSimilar to Rust, Mojo's borrow checker enforces the exclusivity of invariants.\nThe major difference between Rust and Mojo is that Mojo does not require a\nsigil on the caller side to pass by borrow. Also, Mojo is more efficient when\npassing small values, and Rust defaults to moving values instead of passing\nthem around by borrow. These policy and syntax decisions allow Mojo to provide\nan easier-to-use programming model.\n--- cell type: markdown ---\n### Mutable arguments (`inout`)\n\nOn the other hand, if you define an `fn` function and want an argument to be\nmutable, you must declare the argument as mutable with the `inout` keyword.\n\n<div class=\"alert alert-block alert-success\">\n\n**Tip:** When you see `inout`, it means any changes made to the argument\n**in**side the function are visible **out**side the function.\n\n</div>\n\nConsider the following example, in which the `__iadd__` function (which\nimplements the in-place add operation such as `x += 2`) tries to modify `self`:\n--- cell type: code ---\nstruct MyInt:\n    var value: Int\n\n    fn __init__(inout self, v: Int):\n        self.value = v\n\n    fn __copyinit__(inout self, existing: MyInt):\n        self.value = existing.value\n\n    # self and rhs are both immutable in __add__.\n    fn __add__(self, rhs: MyInt) -> MyInt:\n        return MyInt(self.value + rhs.value)\n\n    # ... but this cannot work for __iadd__\n    # Uncomment to see the error:\n    #fn __iadd__(self, rhs: Int):\n    #    self = self + rhs  # ERROR: cannot assign to self!\n\n--- cell type: markdown ---\nIf you uncomment the `__iadd__()` method, you'll get a compiler error.\n\nThe problem here is that `self` is immutable because this is a Mojo `fn`\nfunction, so it can't change the internal state of the argument (the default\nargument convention is `borrowed`). The solution is to declare that the\nargument is mutable by adding the `inout` keyword on the `self` argument name:\n--- cell type: code ---\nstruct MyInt:\n    var value: Int\n\n    fn __init__(inout self, v: Int):\n        self.value = v\n\n    fn __copyinit__(inout self, existing: MyInt):\n        self.value = existing.value\n\n    # self and rhs are both immutable in __add__.\n    fn __add__(self, rhs: MyInt) -> MyInt:\n        return MyInt(self.value + rhs.value)\n\n    # ... now this works:\n    fn __iadd__(inout self, rhs: Int):\n        self = self + rhs\n\n--- cell type: markdown ---\nNow the `self` argument is mutable in the function and any changes are visible\nin the caller, so we can perform in-place addition with `MyInt`:\n--- cell type: code ---\nvar x: MyInt = 42\nx += 1\nprint(x.value) # prints 43 as expected\n\n# However...\nlet y = x\n# Uncomment to see the error:\n# y += 1       # ERROR: Cannot mutate 'let' value\n\n--- cell type: markdown ---\nIf you uncomment the last line above, mutation of the `let` value fails\nbecause it isn't possible to form a mutable reference to an immutable value\n(`let` makes the variable immutable).\n\nOf course, you can declare multiple `inout` arguments. For example, you can\ndefine and use a swap function like this:\n--- cell type: code ---\nfn swap(inout lhs: Int, inout rhs: Int):\n    let tmp = lhs\n    lhs = rhs\n    rhs = tmp\n\nvar x = 42\nvar y = 12\nprint(x, y)  # Prints 42, 12\nswap(x, y)\nprint(x, y)  # Prints 12, 42\n\n--- cell type: markdown ---\nA very important aspect of this system is that it all composes correctly.\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-success\">\n\nNotice that we don't call this argument passing \"by reference.\" Although the\n`inout` convention is conceptually the same, we don't call it by-reference\npassing because the implementation may actually pass values using pointers.\n\n</div>\n--- cell type: markdown ---\n### Transfer arguments (`owned` and `^`)\n\nThe final argument convention that Mojo supports is the `owned` argument\nconvention. This convention is used for functions that want to take exclusive\nownership over a value, and it is often used with the postfixed `^` operator.\n\nFor example, imagine you're working with a move-only type like a unique\npointer:\n--- cell type: code ---\n# This is not really a unique pointer, we just model its behavior here\n# to serve the examples below.\nstruct UniquePointer:\n    var ptr: Int\n\n    fn __init__(inout self, ptr: Int):\n        self.ptr = ptr\n\n    fn __moveinit__(inout self, owned existing: Self):\n        self.ptr = existing.ptr\n\n    fn __del__(owned self):\n        self.ptr = 0\n\n--- cell type: markdown ---\nWhile the `borrow` convention makes it easy to work with this unique pointer\nwithout ceremony, at some point you might want to transfer ownership to some\nother function. This is a situation where you want to use the `^` \"transfer\"\noperator with your movable type.\n\nThe `^` operator ends the lifetime of a value binding and transfers the value\nownership to something else (in the following example, ownership is transferred\nto the `take_ptr()` function). To support this, you can define functions as\ntaking `owned` arguments. For example, you can define `take_ptr()` to take\nownership of an argument as follows:\n--- cell type: code ---\nfn take_ptr(owned p: UniquePointer):\n    print(\"take_ptr\")\n    print(p.ptr)\n\nfn use_ptr(borrowed p: UniquePointer):\n    print(\"use_ptr\")\n    print(p.ptr)\n\nfn work_with_unique_ptrs():\n    let p = UniquePointer(100)\n    use_ptr(p)    # Pass to borrowing function.\n    take_ptr(p^)  # Pass ownership of the `p` value to another function.\n\n    # Uncomment to see an error:\n    # use_ptr(p) # ERROR: p is no longer valid here!\n\nwork_with_unique_ptrs()\n\n--- cell type: markdown ---\nNotice that if you uncomment the second call to `use_ptr()`, you get an error\nbecause the `p` value has been transferred to the `take_ptr()` function and,\nthus, the `p` value is destroyed.\n\nBecause it is declared `owned`, the `take_ptr()` function knows it has unique\naccess to the value.  This is very important for things like unique pointers,\nand it's useful when you want to avoid copies.\n\nFor example, you will notably see the `owned` convention on destructors and on\nconsuming move initializers. For example, our `HeapArray` struct defined\nearlier uses `owned` in its `__del__()` method, because you need to own a value\nto destroy it (or to steal its parts, in the case of a move constructor).\n--- cell type: markdown ---\n### Comparing `def` and `fn` argument passing\n\nMojo's `def` function is essentially just sugaring for the `fn` function:\n\n- A `def` argument without an explicit type annotation defaults to `Object`.\n\n- A `def` argument without a convention keyword (such as `inout` or `owned`) is\npassed by implicit copy into a mutable var with the same name as the argument.\n(This requires that the type have a `__copyinit__` method.)\n\nFor example, these two functions have the same behavior:\n\n```mojo\ndef example(inout a: Int, b: Int, c):\n    # b and c use value semantics so they're mutable in the function\n    ...\n\nfn example(inout a: Int, b_in: Int, c_in: Object):\n    # b_in and c_in are immutable references, so we make mutable shadow copies\n    var b = b_in\n    var c = c_in\n    ...\n```\n\nThe shadow copies typically add no overhead, because references for small types\nlike `Object` are cheap to copy. The expensive part is adjusting the reference\ncount, but that's eliminated by a move optimization.\n--- cell type: markdown ---\n## Python integration\n\nIt's easy to use Python modules you know and love in Mojo. You can import\nany Python module into your Mojo program and create Python types from Mojo\ntypes.\n\n### Importing Python modules\n\nTo import a Python module in Mojo, just call `Python.import_module()` with the\nmodule name:\n--- cell type: code ---\nfrom python import Python\n\n# This is equivalent to Python's `import numpy as np`\nlet np = Python.import_module(\"numpy\")\n\n# Now use numpy as if writing in Python\narray = np.array([1, 2, 3])\nprint(array)\n\n--- cell type: markdown ---\nYes, this imports Python NumPy, and you can import *any other Python module*.\n\nCurrently, you cannot import individual members (such as a single Python class\nor function)â€”you must import the whole Python module and then access members\nthrough the module name.\n\n### Mojo types in Python\n\nMojo primitive types implicitly convert into Python objects.\nToday we support lists, tuples, integers, floats, booleans, and strings.\n\nFor example, given this Python function that prints Python types:\n--- cell type: code ---\n%%python\ndef type_printer(my_list, my_tuple, my_int, my_string, my_float):\n    print(type(my_list))\n    print(type(my_tuple))\n    print(type(my_int))\n    print(type(my_string))\n    print(type(my_float))\n\n--- cell type: markdown ---\nYou can pass the Python function Mojo types with no problem:\n--- cell type: code ---\ntype_printer([0, 3], (False, True), 4, \"orange\", 3.4)\n\n--- cell type: markdown ---\nNotice that in a Jupyter notebook, the Python function declared above is\nautomatically available to any Mojo code in following code cells.\n\nMojo doesn't have a standard Dictionary yet, so it is not yet possible\nto create a Python dictionary from a Mojo dictionary. You can work with\nPython dictionaries in Mojo though! To create a Python dictionary, use the\n`dict` method:\n--- cell type: code ---\nfrom python import Python\nfrom python.object import PythonObject\n\nvar dictionary = Python.dict()\ndictionary[\"fruit\"] = \"apple\"\ndictionary[\"starch\"] = \"potato\"\n\nvar keys: PythonObject = [\"fruit\", \"starch\", \"protein\"]\nvar N: Int = keys.__len__().__index__()\nprint(N, \"items\")\n\nfor i in range(N):\n    if Python.is_type(dictionary.get(keys[i]), Python.none()):\n        print(keys[i], \"is not in dictionary\")\n    else:\n        print(keys[i], \"is included\")\n\n--- cell type: markdown ---\n#### Importing local Python modules\n\nIf you have some local Python code you want to use in Mojo, just add\nthe directory to the Python path and then import the module.\n\nFor example, suppose you have a Python file named `mypython.py`:\n\n```python\nimport numpy as np\n\ndef my_algorithm(a, b):\n    array_a = np.random.rand(a, a)\n    return array_a + b\n```\n\nHere's how you can import it and use it in a Mojo file:\n\n```mojo\nfrom python import Python\n\nPython.add_to_path(\"path/to/module\")\nlet mypython = Python.import_module(\"mypython\")\n\nlet c = mypython.my_algorithm(2, 3)\nprint(c)\n```\n\nThere's no need to worry about memory management when using Python in Mojo.\nEverything just works because Mojo was designed for Python from the beginning.\n--- cell type: markdown ---\n## Parameterization: compile-time metaprogramming\n\nOne of Python's most amazing features is its extensible runtime\nmetaprogramming features. This has enabled a wide range of libraries and\nprovides a flexible and extensible programming model that Python programmers\neverywhere benefit from. Unfortunately, these features also come at a cost:\nbecause they are evaluated at runtime, they directly impact run-time efficiency\nof the underlying code. Because they are not known to the IDE, it is difficult\nfor IDE features like code completion to understand them and use them to\nimprove the developer experience.\n\nOutside the Python ecosystem, static metaprogramming is also an important part\nof development, enabling the development of new programming paradigms and\nadvanced libraries. There are many examples of prior art in this space, with\ndifferent tradeoffs, for example:\n\n1. Preprocessors (e.g. C preprocessor, Lex/YACC, etc) are perhaps the heaviest\nhanded. They are fully general but the worst in terms of developer experience\nand tools integration.\n\n2. Some languages (like Lisp and Rust) support (sometimes \"hygienic\") macro\nexpansion features, enabling syntactic extension and boilerplate reduction with\nsomewhat better tooling integration.\n\n3. Some older languages like C++ have very large and complex metaprogramming\nlanguages (templates) that are a dual to the *runtime* language. These are\nnotably difficult to learn and have poor compile times and error messages.\n\n4. Some languages (like Swift) build many features into the core language in a\nfirst-class way to provide good ergonomics for common cases at the expense of\ngenerality.\n\n5. Some newer languages like Zig integrate a language interpreter into the\ncompilation flow, and allow the interpreter to reflect over the AST as it is\ncompiled. This allows many of the same features as a macro system with better\nextensibility and generality.\n\nFor Modular's work in AI, high-performance machine learning kernels, and\naccelerators, we need high abstraction capabilities provided by advanced\nmetaprogramming systems. We needed high-level zero-cost abstractions,\nexpressive libraries, and large-scale integration of multiple variants of\nalgorithms. We want library developers to be able to extend the system, just\nlike they do in Python, providing an extensible developer platform.\n\nThat said, we are not willing to sacrifice developer experience (including\ncompile times and error messages) nor are we interested in building a parallel\nlanguage ecosystem that is difficult to teach. We can learn from these previous\nsystems but also have new technologies to build on top of, including MLIR and\nfine-grained language-integrated caching technologies.\n\nAs such, Mojo supports compile-time metaprogramming built\ninto the compiler as a separate stage of compilationâ€”after parsing, semantic\nanalysis, and IR generation, but before lowering to target-specific code. It\nuses the same host language for runtime programs as it does for metaprograms,\nand leverages MLIR to represent and evaluate these programs predictably.\n\nLet's take a look at some simple examples.\n\n<div class=\"alert alert-block alert-success\">\n\n**About \"parameters\":** Python developers use the words \"arguments\" and\n\"parameters\" fairly interchangeably for \"things that are passed into\nfunctions.\" We decided to reclaim \"parameter\" and \"parameter expression\" to\nrepresent a compile-time value in Mojo, and continue to use \"argument\" and\n\"expression\" to refer to runtime values. This allows us to align around words\nlike \"parameterized\" and \"parametric\" for compile-time metaprogramming.\n\n</div>\n--- cell type: markdown ---\n### Defining parameterized types and functions\n\nYou can parameterize structs and functions by specifying parameter names and\ntypes in square brackets (using an extended version of the [PEP695\nsyntax](https://peps.python.org/pep-0695/)). Unlike argument values, parameter\nvalues are known at compile-time, which enables an additional level of\nabstraction and code reuse, plus compiler optimizations such as\n[autotuning](#autotuning-adaptive-compilation).\n\nFor instance, let's look at a\n[SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) type,\nwhich represents a low-level vector register in hardware that holds multiple\ninstances of a scalar data-type. Hardware accelerators are constantly\nintroducing new vector data types, and even CPUs may have 512-bit or longer SIMD\nvectors. In order to access the SIMD instructions on these processors, the data\nmust be shaped into the proper SIMD width (data type) and length (vector size).\n\nHowever, it's not feasible to define all the different SIMD variations with\nMojo's built-in types. So, Mojo's `SIMD` type (defined as a struct) exposes the\ncommon SIMD operations in its methods, and makes the SIMD data type and size\nvalues parametric. This allows you to directly map your data to the SIMD vectors\non any hardware. \n\nHere is a cut-down (non-functional) version of Mojo's `SIMD` type definition:\n\n```mojo\nstruct SIMD[type: DType, size: Int]:\n    var value: â€¦ # Some low-level MLIR stuff here\n\n    # Create a new SIMD from a number of scalars\n    fn __init__(inout self, *elems: SIMD[type, 1]):  ...\n\n    # Fill a SIMD with a duplicated scalar value.\n    @staticmethod\n    fn splat(x: SIMD[type, 1]) -> SIMD[type, size]: ...\n\n    # Cast the elements of the SIMD to a different elt type.\n    fn cast[target: DType](self) -> SIMD[target, size]: ...\n\n    # Many standard operators are supported.\n    fn __add__(self, rhs: Self) -> Self: ...\n```\n\nDefining each SIMD variant with parameters is great for code reuse because the\n`SIMD` type can express all the different vector variants statically, instead of\nrequiring the language to pre-define every variant.\n\nBecause `SIMD` is a parameterized type, the `self` argument in its functions\ncarries those parametersâ€”the full type name is `SIMD[type, size]`. Although\nit's valid to write this out (as shown in the return type of `splat()`), this\ncan be verbose, so we recommend using the `Self` type (from\n[PEP673](https://peps.python.org/pep-0673/)) like the `__add__` example does.\n--- cell type: markdown ---\n### Overloading on parameters\n\nFunctions and methods can be overloaded on their parameter signatures. The\noverload resolution logic filters for candidates according to the following\nrules, in order of precedence:\n\n1) Candidates with the minimal number of implicit conversions (in both arguments\nand parameters).\n2) Candidates without variadic arguments.\n3) Candidates without variadic parameters.\n4) Candidates with the shortest parameter signature.\n5) Non-`@staticmethod` candidates (over `@staticmethod` ones, if available). \n\nIf there is more than one candidate after applying these rules, the overload\nresolution fails. For example:\n--- cell type: code ---\n@register_passable(\"trivial\")\nstruct MyInt:\n    \"\"\"A type that is implicitly convertible to `Int`.\"\"\"\n    var value: Int\n\n    @always_inline(\"nodebug\")\n    fn __init__(_a: Int) -> Self:\n        return Self {value: _a}\n\nfn foo[x: MyInt, a: Int]():\n    print(\"foo[x: MyInt, a: Int]()\")\n\nfn foo[x: MyInt, y: MyInt]():\n    print(\"foo[x: MyInt, y: MyInt]()\")\n\nfn bar[a: Int](b: Int):\n    print(\"bar[a: Int](b: Int)\")\n\nfn bar[a: Int](*b: Int):\n    print(\"bar[a: Int](*b: Int)\")\n\nfn bar[*a: Int](b: Int):\n    print(\"bar[*a: Int](b: Int)\")\n\nfn parameter_overloads[a: Int, b: Int, x: MyInt]():\n    # `foo[x: MyInt, a: Int]()` is called because it requires no implicit\n    # conversions, whereas `foo[x: MyInt, y: MyInt]()` requires one.\n    foo[x, a]()\n\n    # `bar[a: Int](b: Int)` is called because it does not have variadic\n    # arguments or parameters.\n    bar[a](b)\n\n    # `bar[*a: Int](b: Int)` is called because it has variadic parameters.\n    bar[a, a, a](b)\n\nparameter_overloads[1, 2, MyInt(3)]()\n\nstruct MyStruct:\n    fn __init__(inout self):\n        pass\n\n    fn foo(inout self):\n        print(\"calling instance menthod\")\n\n    @staticmethod\n    fn foo():\n        print(\"calling static menthod\")\n\nfn test_static_overload():\n    var a = MyStruct()\n    # `foo(inout self)` takes precedence over a static method.\n    a.foo()\n\n--- cell type: markdown ---\n### Using parameterized types and functions\n\nYou can instantiate parametric types and functions by passing values to the\nparameters in square brackets. For example, for the `SIMD` type above, `type`\nspecifies the data type and `size` specifies the length of the SIMD vector (it\nmust be a power of 2):\n--- cell type: code ---\n# Make a vector of 4 floats.\nlet small_vec = SIMD[DType.float32, 4](1.0, 2.0, 3.0, 4.0)\n\n# Make a big vector containing 1.0 in float16 format.\nlet big_vec = SIMD[DType.float16, 32].splat(1.0)\n\n# Do some math and convert the elements to float32.\nlet bigger_vec = (big_vec+big_vec).cast[DType.float32]()\n\n# You can write types out explicitly if you want of course.\nlet bigger_vec2 : SIMD[DType.float32, 32] = bigger_vec\n\nprint('small_vec type:', small_vec.element_type, 'length:', len(small_vec))\nprint('bigger_vec2 type:', bigger_vec2.element_type, 'length:', len(bigger_vec2))\n\n--- cell type: markdown ---\nNote that the `cast()` method also needs a parameter to specify the type you\nwant from the cast (the method definition above expects a `target` parametric\nvalue). Thus, just like how the `SIMD` struct is a generic type definition, the\n`cast()` method is a generic method definition that gets instantiated at\ncompile-time instead of runtime, based on the parameter value.\n\nThe code above shows the use of concrete types (that is, it\ninstantiates `SIMD` using known type values), but the major power of parameters\ncomes from the ability to define parametric algorithms and types (code that\nuses the parameter values). For example, here's how to define a parametric\nalgorithm with `SIMD` that is type- and width-agnostic:\n--- cell type: code ---\nfrom math import sqrt\n\nfn rsqrt[dt: DType, width: Int](x: SIMD[dt, width]) -> SIMD[dt, width]:\n    return 1 / sqrt(x)\n\nprint(rsqrt[DType.float16, 4](42))\n\n--- cell type: markdown ---\nNotice that the `x` argument is actually a `SIMD` type based on the function\nparameters. The runtime program can use the value of parameters, because the\nparameters are resolved at compile-time before they are needed by the runtime\nprogram (but compile-time parameter expressions cannot use runtime values).\n\nThe Mojo compiler is also smart about type inference with parameters. Note\nthat the above function is able to call the parametric\n[`sqrt[]()`](https://docs.modular.com/mojo/stdlib/math/math.html#sqrt) function\nwithout specifying the parametersâ€”the compiler infers its parameters based on\nthe parametric `x` value passed into it, as if you\nwrote `sqrt[dt, width](x)` explicitly. Also note that `rsqrt()` chose to\ndefine its first parameter named `width` even though the `SIMD` type names it\n`size`, and there is no problem.\n--- cell type: markdown ---\n### Optional parameters and keyword parameters\n\nJust as you can specify [optional arguments](https://docs.modular.com/mojo/manual/basics/#function-arguments-and-returns)\nin function signatures, you can also define an optional _parameter_ by \ngiving it a default value. You can also pass parameters by keyword.\nFor a function or struct with multiple optional parameters, using keywords\nallows you to pass only the parameters you want to specify, regardless of\ntheir position in the function signature. \n\nFor example, here's a function with two parameters, each with a default value:\n--- cell type: code ---\nfn speak[a: Int = 3, msg: StringLiteral = \"woof\"]():\n    print(msg, a)\n\nfn use_defaults() raises:\n    speak()             # prints 'woof 3'\n    speak[5]()          # prints 'woof 5'\n    speak[7, \"meow\"]()  # prints 'meow 7'\n    speak[msg=\"baaa\"]() # prints 'baaa 3'\n\n--- cell type: markdown ---\nRecall that Mojo can infer parameter values in a parametric function, based on\nthe parametric values attached to an argument value (see the `rsqrt[]()`\nexample above). If the parametric function also has a default value defined,\nthen the inferred parameter type takes precedence.\n\nFor example, in the following code, we update the parametric `speak[]()` function\nto take an argument with a parametric type. Although the function has a default\nparameter value for `a`, Mojo instead uses the inferred `a` parameter value\nfrom the `bar` argument (as written, the default `a` value can never be used,\nbut this is just for demonstration purposes):\n--- cell type: code ---\n@value\nstruct Bar[v: Int]:\n    pass\n\nfn foo[a: Int = 3, msg: StringLiteral = \"woof\"](bar: Bar[a]):\n    print(msg, a)\n\nfn use_inferred():\n    foo(Bar[9]())  # prints 'woof 9'\n\n--- cell type: markdown ---\nAs mentioned above, you can also use optional parameters and keyword \nparameters in a struct:\n--- cell type: code ---\nstruct KwParamStruct[greeting: String = \"Hello\", name: String = \"ðŸ”¥mojoðŸ”¥\"]:\n    fn __init__(inout self):\n        print(greeting, name)\n\nfn use_kw_params():\n    let a = KwParamStruct[]()                 # prints 'Hello ðŸ”¥mojoðŸ”¥'\n    let b = KwParamStruct[name=\"World\"]()     # prints 'Hello World'\n    let c = KwParamStruct[greeting=\"Hola\"]()  # prints 'Hola ðŸ”¥mojoðŸ”¥'\n\n--- cell type: markdown ---\n<div class=\"alert alert-block alert-info\">\n\n**Note:** Mojo currently includes only partial support for keyword parameters, so\nsome features such as keyword-only parameters and variadic keyword parameters \n(for example, `**kwparams`) are not supported yet.\n\n</div>\n--- cell type: markdown ---\n\n### Automatic parameterization of functions\n\nMojo  supports \"automatic\" parameterization of functions. If a \nfunction argument type is parametric but the function signature\n*doesn't* specify the parameters, they are automatically added as \ninput parameters on the function. This is easier to understand\nwith an example:\n--- cell type: code ---\nfn print_params(vec: SIMD):\n    print(vec.type)\n    print(vec.size)\n\nfn main():\n    let v = SIMD[DType.float64, 4](1.0, 2.0, 3.0, 4.0)\n    print_params(v)\n\n--- cell type: markdown ---\nIn the above example, the `print_params` function is automatically \nparameterized. It takes a parameterized type (`SIMD`), but doesn't specify \nparameter values for it. Instead, it treats the types' parameters as its own, as\nif you had written them explicitly:\n--- cell type: code ---\nfn print_params[type: DType, size: Int](vec: SIMD[type, size]):\n    print(vec.type)\n    print(vec.size)\n\n--- cell type: markdown ---\n\nWhen you pass `print_params()` a concrete instance of the `SIMD` type, it can \naccess the original input parameters as attributes on the instance (for example,\n`vec.type`). This is necessary for an automatically parameterized\nfunction, since it doesn't have any other way to access the parameters. But it \nactually works in any context. You can access the input parameters of a \nparameterized type as attributes on the type:\n--- cell type: code ---\nfn main():\n    print(SIMD[DType.float32, 2].size) # prints 2\n\n--- cell type: markdown ---\nOr as attributes on an _instance_ of the type:\n--- cell type: code ---\nfn main():\n    let x = SIMD[DType.int32, 2](4, 8)\n    print(x.type) # prints int32\n\n--- cell type: markdown ---\n### Parameter expressions are just Mojo code\n\nA parameter expression is any code expression (such as `a+b`) that occurs where\na parameter is expected. Parameter expressions support operators and function\ncalls, just like runtime code, and all parameter types use the same type\nsystem as the runtime program (such as `Int` and `DType`).\n\nBecause parameter expressions use the same grammar and types as runtime\nMojo code, you can use many \"dependent type\" features. For example, you might\nwant to define a helper function to concatenate two SIMD vectors:\n--- cell type: code ---\nfn concat[ty: DType, len1: Int, len2: Int](\n        lhs: SIMD[ty, len1], rhs: SIMD[ty, len2]) -> SIMD[ty, len1+len2]:\n\n    var result = SIMD[ty, len1 + len2]()\n    for i in range(len1):\n        result[i] = SIMD[ty, 1](lhs[i])\n    for j in range(len2):\n        result[len1 + j] = SIMD[ty, 1](rhs[j])\n    return result\n\nlet a = SIMD[DType.float32, 2](1, 2)\nlet x = concat[DType.float32, 2, 2](a, a)\n\nprint('result type:', x.element_type, 'length:', len(x))\n\n--- cell type: markdown ---\nNote how the resulting length is the sum of the input vector lengths, and you\ncan express that with a simple `+` operation. For a more complex example, take\na look at the [`SIMD.shuffle()`](https://docs.modular.com/mojo/MojoStdlib/SIMD.html#shuffle) method in\nthe standard library: it takes two input SIMD values, a vector shuffle mask as\na list, and returns a SIMD that matches the length of the shuffle mask.\n\n### Powerful compile-time programming\n\nWhile simple expressions are useful, sometimes you want to write imperative\ncompile-time logic with control flow. For example, the `isclose()` function in\nthe Mojo `Math` module uses exact equality for integers but \"close\" comparison\nfor floating-point. You can even do compile-time recursion. For instance, here\nis an example \"tree reduction\" algorithm that sums all elements of a vector\nrecursively into a scalar:\n--- cell type: code ---\nfn slice[ty: DType, new_size: Int, size: Int](\n        x: SIMD[ty, size], offset: Int) -> SIMD[ty, new_size]:\n    var result = SIMD[ty, new_size]()\n    for i in range(new_size):\n        result[i] = SIMD[ty, 1](x[i + offset])\n    return result\n\nfn reduce_add[ty: DType, size: Int](x: SIMD[ty, size]) -> Int:\n    @parameter\n    if size == 1:\n        return x[0].to_int()\n    elif size == 2:\n        return x[0].to_int() + x[1].to_int()\n\n    # Extract the top/bottom halves, add them, sum the elements.\n    alias half_size = size // 2\n    let lhs = slice[ty, half_size, size](x, 0)\n    let rhs = slice[ty, half_size, size](x, half_size)\n    return reduce_add[ty, half_size](lhs + rhs)\n\nlet x = SIMD[DType.index, 4](1, 2, 3, 4)\nprint(x)\nprint(\"Elements sum:\", reduce_add[DType.index, 4](x))\n\n--- cell type: markdown ---\nThis makes use of the `@parameter if` feature, which is an `if` statement that\nruns at compile-time. It requires that its condition be a valid parameter\nexpression, and ensures that only the live branch of the `if` statement is\ncompiled into the program.\n\n### Mojo types are just parameter expressions\n\nWhile we've shown how you can use parameter expressions within types, type\nannotations can themselves be arbitrary expressions (just like in Python).\nTypes in Mojo have a special metatype type, allowing type-parametric algorithms\nand functions to be defined. \n\nFor example, we can create a simplified `Array` that supports arbitrary types of\nthe elements (via the `AnyType` parameter):\n--- cell type: code ---\nstruct Array[T: AnyType]:\n    var data: Pointer[T]\n    var size: Int\n\n    fn __init__(inout self, size: Int, value: T):\n        self.size = size\n        self.data = Pointer[T].alloc(self.size)\n        for i in range(self.size):\n            self.data.store(i, value)\n\n    fn __getitem__(self, i: Int) -> T:\n        return self.data.load(i)\n\n    fn __del__(owned self):\n        self.data.free()\n\nvar v = Array[Float32](4, 3.14)\nprint(v[0], v[1], v[2], v[3])\n\n--- cell type: markdown ---\nNotice that the `T` parameter is being used as the formal type for the\n`value` arguments and the return type of the `__getitem__` function. Parameters\nallow the `Array` type to provide different APIs based on the different\nuse-cases. \n\nThere are many other cases that benefit from more advanced use of parameters.\nFor example, you can execute a closure N times in parallel, feeding in a value\nfrom the context, like this:\n--- cell type: code ---\nfn parallelize[func: fn (Int) -> None](num_work_items: Int):\n    # Not actually parallel: see the 'algorithm' module for real implementation.\n    for i in range(num_work_items):\n        func(i)\n\n--- cell type: markdown ---\nAnother example where this is important is with variadic generics, where an\nalgorithm or data structure may need to be defined over a list of heterogeneous\ntypes such as for a tuple:\n\n```mojo\nstruct Tuple[*Ts: AnyType]:\n    var _storage : *Ts\n```\n\nAnd although we don't have enough metatype helpers in place yet, we should be\nable to write something like this in the future (though overloading is still a\nbetter way to handle this):\n\n```mojo\nstruct Array[T: AnyType]:\n    fn __getitem__[IndexType: AnyType](self, idx: IndexType)\n       -> (ArraySlice[T] if issubclass(IndexType, Range) else T):\n       ...\n```\n--- cell type: markdown ---\n### `alias`: named parameter expressions\n\nIt is very common to want to *name* compile-time values. Whereas `var` defines a\nruntime value, and `let` defines a runtime constant, we need a way to define a\ncompile-time temporary value. For this, Mojo uses an `alias` declaration. \n\nFor example, the `DType` struct implements a simple enum using aliases for the\nenumerators like this (the actual `DType` implementation details vary a bit):\n\n```mojo\nstruct DType:\n    var value : UI8\n    alias invalid = DType(0)\n    alias bool = DType(1)\n    alias int8 = DType(2)\n    alias uint8 = DType(3)\n    alias int16 = DType(4)\n    alias int16 = DType(5)\n    ...\n    alias float32 = DType(15)\n```\n\nThis allows clients to use `DType.float32` as a parameter expression (which also\nworks as a runtime value) naturally. Note that this is invoking the\nruntime constructor for `DType` at compile-time.\n\nTypes are another common use for alias. Because types are compile-time\nexpressions, it is handy to be able to do things like this:\n--- cell type: code ---\nalias Float16 = SIMD[DType.float16, 1]\nalias UInt8 = SIMD[DType.uint8, 1]\n\nvar x : Float16   # FLoat16 works like a \"typedef\"\n\n--- cell type: markdown ---\nLike `var` and `let`, aliases obey scope, and you can use local aliases within\nfunctions as you'd expect.\n\nBy the way, both `None` and `AnyType` are defined as [type\naliases](https://docs.modular.com/mojo/MojoBuiltin/TypeAliases.html).\n--- cell type: markdown ---\n### Autotuning / Adaptive compilation\n\nMojo parameter expressions allow you to write portable parametric algorithms\nlike you can do in other languages, but when writing high-performance code you\nstill have to pick concrete values to use for the parameters. For example, when\nwriting high-performance numeric algorithms, you might want to use memory\ntiling to accelerate the algorithm, but the dimensions to use depend highly on\nthe available hardware features, the sizes of the cache, what gets fused into\nthe kernel, and many other fiddly details.\n\nEven vector length can be difficult to manage, because the vector length of a\ntypical machine depends on the datatype, and some datatypes like `bfloat16`\ndon't have full support on all implementations. Mojo helps by providing an\n`autotune` function in the standard library. For example if you want to write a\nvector-length-agnostic algorithm to a buffer of data, you might write it like\nthis:\n--- cell type: code ---\nimport benchmark\nfrom autotune import autotune, search\nfrom memory.unsafe import DTypePointer\nfrom algorithm import vectorize\n\nfn buffer_elementwise_add_impl[\n    dt: DType\n](lhs: DTypePointer[dt], rhs: DTypePointer[dt], result: DTypePointer[dt], N: Int):\n    \"\"\"Perform elementwise addition of N elements in RHS and LHS and store\n    the result in RESULT.\n    \"\"\"\n    @parameter\n    fn add_simd[size: Int](idx: Int):\n        let lhs_simd = lhs.simd_load[size](idx)\n        let rhs_simd = rhs.simd_load[size](idx)\n        result.simd_store[size](idx, lhs_simd + rhs_simd)\n\n    # Pick vector length for this dtype and hardware\n    alias vector_len = autotune(1, 4, 8, 16, 32)\n\n    # Use it as the vectorization length\n    vectorize[vector_len, add_simd](N)\n\nfn elementwise_evaluator[dt: DType](\n    fns: Pointer[fn (DTypePointer[dt], DTypePointer[dt], DTypePointer[dt], Int) -> None],\n    num: Int,\n) -> Int:\n    # Benchmark the implementations on N = 64.\n    alias N = 64\n    let lhs = DTypePointer[dt].alloc(N)\n    let rhs = DTypePointer[dt].alloc(N)\n    let result = DTypePointer[dt].alloc(N)\n\n    # Fill with ones.\n    for i in range(N):\n        lhs.store(i, 1)\n        rhs.store(i, 1)\n\n    # Find the fastest implementation.\n    var best_idx: Int = -1\n    var best_time: Int = -1\n    for i in range(num):\n        @parameter\n        fn wrapper():\n            fns.load(i)(lhs, rhs, result, N)\n        let cur_time = benchmark.run[wrapper](1).mean[\"ns\"]().to_int()\n        if best_idx < 0 or best_time > cur_time:\n            best_idx = i\n            best_time = cur_time\n        print(\"time[\", i, \"] =\", cur_time)\n    print(\"selected:\", best_idx)\n    return best_idx\n\nfn buffer_elementwise_add[\n    dt: DType\n](lhs: DTypePointer[dt], rhs: DTypePointer[dt], result: DTypePointer[dt], N: Int):\n    # Forward declare the result parameter.\n    alias best_impl: fn(DTypePointer[dt], DTypePointer[dt], DTypePointer[dt], Int) -> None\n\n    # Perform search!\n    search[\n      fn(DTypePointer[dt], DTypePointer[dt], DTypePointer[dt], Int) -> None,\n      buffer_elementwise_add_impl[dt],\n      elementwise_evaluator[dt] -> best_impl\n    ]()\n\n    # Call the select implementation\n    best_impl(lhs, rhs, result, N)\n\n--- cell type: markdown ---\nWe can now call our function as usual:\n--- cell type: code ---\nlet N = 32\nlet a = DTypePointer[DType.float32].alloc(N)\nlet b = DTypePointer[DType.float32].alloc(N)\nlet res = DTypePointer[DType.float32].alloc(N)\n# Initialize arrays with some values\nfor i in range(N):\n    a.store(i, 2.0)\n    b.store(i, 40.0)\n    res.store(i, -1)\n\nbuffer_elementwise_add[DType.float32](a, b, res, N)\nprint(a.load(10), b.load(10), res.load(10))\n\n--- cell type: markdown ---\nWhen compiling instantiations of this code, Mojo forks compilation of this\nalgorithm and decides which value to use by measuring what works best in\npractice for the target hardware. It evaluates the different values of the\n`vector_len` expression and picks the fastest one according to a user-defined\nperformance evaluator. Because it measures and evaluates each option\nindividually, it might pick a different vector length for `float32` than for\n`int8`, for example. This simple feature is pretty powerfulâ€”going beyond simple\ninteger constantsâ€”because functions and types are also parameter expressions.\n\nNotice that the search for the best vector length is performed by the\n[`search()`](https://docs.modular.com/mojo/stdlib/autotune/autotuning.html#search)\nfunction. `search()` takes an evaluator and a forked function and returns the\nfastest implementation selected by the evaluator as a parameter result. For a\ndeeper dive on this topic, check out the notebooks about [Matrix\nMultiplication](https://docs.modular.com/mojo/notebooks/Matmul.html) and [Fast\nMemset in Mojo](https://docs.modular.com/mojo/notebooks/Memset.html).\n\nAutotuning is an inherently exponential technique that benefits from internal\nimplementation details of the Mojo compiler stack (particularly MLIR, integrated\ncaching, and distribution of compilation). This is also a power-user feature and\nneeds continued development and iteration over time.\n--- cell type: markdown ---\n## \"Value Lifecycle\": Birth, life and death of a value\n\nAt this point, you should understand the core semantics and features for Mojo\nfunctions and types, so we can now discuss how they fit together to express\nnew types in Mojo.\n\nMany existing languages express design points with different tradeoffs: C++, for\nexample, is very powerful but often accused of\n\"getting the defaults wrong\" which leads to bugs and mis-features.  Swift is\neasy to work with, but has a less predictable model that copies values a lot and\nis dependent on an \"ARC optimizer\" for performance. Rust started with strong\nvalue ownership goals to satisfy its borrow checker, but relies on values being\nmovable, which makes it challenging to express custom move constructors and\ncan put a lot of stress on `memcpy` performance. In Python, everything is a\nreference to a class, so it never really faces issues with types.\n\nFor Mojo, we've learned from these existing systems, and we aim to\nprovide a model that's very powerful while still easy to learn and understand.\nWe also don't want to require \"best effort\" and difficult-to-predict\noptimization passes built into a \"sufficiently smart\" compiler.\n\nTo explore these issues, we look at different value classifications and the\nrelevant Mojo features that go into expressing them, and build from the\nbottom-up. We use C++ as the primary comparison point in examples because it is\nwidely known, but we occasionally reference other languages if they provide a\nbetter comparison point.\n\n### Types that cannot be instantiated\n\nThe most bare-bones type in Mojo is one that doesn't allow you to create\ninstances of it: these types have no initializer at all, and if they have a\ndestructor, it will never be invoked (because there cannot be instances to\ndestroy):\n--- cell type: code ---\nstruct NoInstances:\n    var state: Int  # Pretty useless\n\n    alias my_int = Int\n\n    @staticmethod\n    fn print_hello():\n        print(\"hello world\")\n\n--- cell type: markdown ---\nMojo types do not get default constructors, move constructors, member-wise\ninitializers or anything else by default, so it is impossible to create an\ninstance of this `NoInstances` type.  In order to get them, you need to define\nan `__init__` method or use a decorator that synthesizes an initializer.  As\nshown, these types can be useful as \"namespaces\" because you can refer to\nstatic members like `NoInstances.my_int` or `NoInstances.print_hello()` even\nthough you cannot instantiate an instance of the type.\n\n### Non-movable and non-copyable types\n\nIf we take a step up the ladder of sophistication, weâ€™ll get to types that can\nbe instantiated, but once they are pinned to an address in memory, they cannot\nbe implicitly moved or copied.  This can be useful to implement types like\natomic operations (such as `std::atomic` in C++) or other types where the memory\naddress of the value is its identity and is critical to its purpose:\n--- cell type: markdown ---\n```mojo\nstruct Atomic:\n    var state: Int\n\n    fn __init__(inout self, state: Int = 0):\n        self.state = state\n\n    fn __iadd__(inout self, rhs: Int):\n        #...atomic magic...\n\n    fn get_value(self) -> Int:\n        return atomic_load_int(self.state)\n```\n--- cell type: markdown ---\nThis class defines an initializer but no copy or move constructors, so once it\nis initialized it can never be moved or copied.  This is safe and useful because\nMojo's ownership system is fully \"address correct\" - when this is initialized\nonto the stack or in the field of some other type, it never needs to move.\n\nNote that Mojoâ€™s approach controls only the built-in move operations, such as\n`a = b` copies and the [`^` transfer operator](#owned-arguments). One\nuseful pattern you can use for your own types (like `Atomic` above) is to add\nan explicit `copy()` method (a non-\"dunder\" method). This can be useful to make\nexplicit copies of an instance when it is known safe to the programmer.\n--- cell type: markdown ---\n### Unique \"move-only\" types\n\nIf we take one more step up the ladder of capabilities, we will encounter types\nthat are \"unique\" - there are many examples of this in C++, such as types like\n`std::unique_ptr` or even a `FileDescriptor` type that owns an underlying POSIX\nfile descriptor. These types are pervasive in languages like Rust, where\ncopying is discouraged, but \"move\" is free. In Mojo, you can implement these\nkinds of moves by defining the `__moveinit__` method to take ownership of a\nunique type. For example:\n--- cell type: markdown ---\n\n```mojo\n# This is a simple wrapper around POSIX-style fcntl.h functions.\nstruct FileDescriptor:\n    var fd: Int\n\n    # This is how we move our unique type.\n    fn __moveinit__(inout self, owned existing: Self):\n        self.fd = existing.fd\n\n    # This takes ownership of a POSIX file descriptor.\n    fn __init__(inout self, fd: Int):\n        self.fd = fd\n\n    fn __init__(inout self, path: String):\n        # Error handling omitted, call the open(2) syscall.\n        self = FileDescriptor(open(path, ...))\n\n    fn __del__(owned self):\n        close(self.fd)   # pseudo code, call close(2)\n\n    fn dup(self) -> Self:\n        # Invoke the dup(2) system call.\n        return Self(dup(self.fd))\n    fn read(...): ...\n    fn write(...): ...\n```\n--- cell type: markdown ---\nThe consuming move constructor (`__moveinit__`) takes ownership of an existing\n`FileDescriptor`, and moves its internal implementation details over to a new\ninstance.  This is because instances of `FileDescriptor` may exist at different\nlocations, and they can be logically moved aroundâ€”stealing the body of one\nvalue and moving it into another.\n\nHere is an egregious example that will invoke `__moveinit__` multiple times:\n--- cell type: markdown ---\n```mojo\nfn egregious_moves(owned fd1: FileDescriptor):\n    # fd1 and fd2 have different addresses in memory, but the\n    # transfer operator moves unique ownership from fd1 to fd2.\n    let fd2 = fd1^\n\n    # Do it again, a use of fd2 after this point will produce an error.\n    let fd3 = fd2^\n\n    # We can do this all day...\n    let fd4 = fd3^\n    fd4.read(...)\n    # fd4.__del__() runs here\n```\n--- cell type: markdown ---\nNote how ownership of the value is transferred between various values that own\nit, using the postfix-`^` \"transfer\" operator, which destroys a previous\nbinding and transfer ownership to a new constant. If you are familiar with C++,\nthe simple way to think about the transfer operator is like `std::move`, but in\nthis case, we can see that it is able to move things without resetting them to\na state that can be destroyed: in C++, if your move operator failed to change\nthe old valueâ€™s `fd` instance, it would get closed twice.\n\nMojo tracks the liveness of values and allows you to define custom move\nconstructors. This is rarely needed, but extremely powerful when it is. For\nexample, some types like the [`llvm::SmallVector\ntype`](https://llvm.org/docs/ProgrammersManual.html#llvm-adt-smallvector-h)\nuse the \"inline storage\" optimization technique, and they may want to be\nimplemented with an \"inner pointer\" into their instance. This is a well-known\ntrick to reduce pressure on the malloc memory allocator, but it means that a\n\"move\" operation needs custom logic to update the pointer when that happens.\n\nWith Mojo, this is as simple as implementing a custom `__moveinit__` method.\nThis is something that is also easy to implement in C++ (though, with\nboilerplate in the cases where you donâ€™t need custom logic) but is difficult to\nimplement in other popular memory-safe languages.\n\nOne additional note is that while the Mojo compiler provides good predictability\nand control, it is also very sophisticated.  It reserves the right to eliminate\ntemporaries and the corresponding copy/move operations.  If this is\ninappropriate for your type, you should use explicit methods like `copy()`\ninstead of the dunder methods.\n--- cell type: markdown ---\n### Types that support a \"taking move\"\n\nOne challenge with memory-safe languages is that they need to provide a\npredictable programming model around what the compiler is able to track, and\nstatic analysis in a compiler is inherently limited.  For example, while it is\npossible for a compiler to understand that the two array accesses in the\nfirst example below are to different array elements, it is (in general)\nimpossible to reason about the second example (this is C++ code):\n\n```c++\nstd::pair<T, T> getValues1(MutableArray<T> &array) {\n    return { std::move(array[0]), std::move(array[1]) };\n}\nstd::pair<T, T> getValues2(MutableArray<T> &array, size_t i, size_t j) {\n    return { std::move(array[i]), std::move(array[j]) };\n}\n```\n\nThe problem here is that there is simply no way (looking at just the function\nbody above) to know or prove that the dynamic values of `i` and `j` are not the\nsame.  While it is possible to maintain dynamic state to track whether\nindividual elements of the array are live, this often causes significant\nruntime expense (even when move/transfers are not used), which is something that\nMojo and other systems programming languages are not keen to do.  There are a\nvariety of ways to deal with this, including some pretty complicated solutions\nthat arenâ€™t always easy to learn.\n\nMojo takes a pragmatic approach to let Mojo programmers get their job done\nwithout having to work around its type system. As seen above, it doesnâ€™t force\ntypes to be copyable, movable, or even constructable, but it does want types to\nexpress their full contract, and it wants to enable fluent design patterns that\nprogrammers expect from languages like C++.  The (well known) observation here\nis that many objects have contents that can be \"taken away\" without needing to\ndisable their destructor, either because they have a \"null state\" (like an\noptional type or nullable pointer) or because they have a null value that is\nefficient to create and a no-op to destroy (e.g. `std::vector` can have a null\npointer for its data).\n\nTo support these use-cases, the [`^` transfer operator](#owned-arguments)\nsupports arbitrary LValues, and when applied to one, it invokes the \"taking\nmove constructor,\" which is spelled `__takeinit__`. This constructor must set up the new value to be in a live\nstate, and it can mutate the old value, but it must put the old value into a\nstate where its destructor still works. For example, if we want to put our\n`FileDescriptor` into a vector and move out of it, we might choose to extend it\nto know that `-1` is a sentinel which means that it is \"null\". We can implement\nthis like so:\n--- cell type: markdown ---\n```mojo\n# This is a simple wrapper around POSIX-style fcntl.h functions.\nstruct FileDescriptor:\n    var fd: Int\n\n    # This is the new key capability.\n    fn __takeinit__(inout self, inout existing: Self):\n        self.fd = existing.fd\n        existing.fd = -1  # neutralize 'existing'.\n\n    fn __moveinit__(inout self, owned existing: Self): # as above\n    fn __init__(inout self, fd: Int): # as above\n    fn __init__(inout self, path: String): # as above\n\n    fn __del__(owned self):\n        if self.fd != -1:\n            close(self.fd)   # pseudo code, call close(2)\n```\n--- cell type: markdown ---\nNotice how the \"stealing move\" constructor takes the file descriptor from an\nexisting value and mutates that value so that its destructor wonâ€™t do anything.\nThis technique has tradeoffs and is not the best for every type. We can see\nthat it adds one (inexpensive) branch to the destructor because it has to check\nfor the sentinel case. It is also generally considered bad form to make types\nlike this nullable because a more general feature like an `Optional[T]` type is\na better way to handle this.\n\nFurthermore, we plan to implement `Optional[T]` in Mojo itself, and `Optional`\nneeds this functionality.  We also believe that the library authors understand\ntheir domain problem better than language designers do, and generally prefer to\ngive library authors full power over that domain.  As such you can choose (but\ndonâ€™t have to) to make your types participate in this behavior in an opt-in way.\n\n### Copyable types\n\nThe next step up from movable types are copyable types.  Copyable types are\nalso very common - programmers generally expect things like strings and arrays\nto be copyable, and every Python Object reference is copyable - by copying the\npointer and adjusting the reference count.\n\nThere are many ways to implement copyable types.  One can implement reference\nsemantic types like Python or Java, where you propagate shared pointers around,\none can use immutable data structures that are easily shareable because they are\nnever mutated once created, and one can implement deep value semantics through\nlazy copy-on-write as Swift does.  Each of these approaches has different\ntradeoffs, and Mojo takes the opinion that while we want a few common sets of\ncollection types, we can also support a wide range of specialized ones that\nfocus on particular use cases.\n\nIn Mojo, you can do this by implementing the `__copyinit__` method.  Here is an\nexample of that using a simple `String` (in pseudo-code):\n--- cell type: markdown ---\n```mojo\nstruct MyString:\n    var data: Pointer[UI8]\n\n    # StringRef is a pointer + length and works with StringLiteral.\n    def __init__(inout self, input: StringRef):\n        self.data = ...\n\n    # Copy the string by deep copying the underlying malloc'd data.\n    def __copyinit__(inout self, existing: Self):\n        self.data = strdup(existing.data)\n\n    # This isn't required, but optimizes unneeded copies.\n    def __moveinit__(inout self, owned existing: Self):\n        self.data = existing.data\n\n    def __del__(owned self):\n        free(self.data.address)\n\n    def __add__(self, rhs: MyString) -> MyString: ...\n```\n--- cell type: markdown ---\nThis simple type is a pointer to a \"null-terminated\" string data allocated with\nmalloc, using old-school C APIs for clarity. It implements the `__copyinit__`,\nwhich maintains the invariant that each instance of `MyString` owns its\nunderlying pointer and frees it upon destruction. This implementation builds on\ntricks weâ€™ve seen above, and implements a `__moveinit__` constructor, which\nallows it to completely eliminate temporary copies in some common cases. You\ncan see this behavior in this code sequence:\n--- cell type: markdown ---\n```mojo\nfn test_my_string():\n    var s1 = MyString(\"hello \")\n\n    var s2 = s1    # s2.__copyinit__(s1) runs here\n\n    print(s1)\n\n    var s3 = s1^   # s3.__moveinit__(s1) runs here\n\n    print(s2)\n    # s2.__del__() runs here\n    print(s3)\n    # s3.__del__() runs here\n```\n--- cell type: markdown ---\nIn this case, you can see both why a copy constructor is needed: without one,\nthe duplication of the `s1` value into `s2` would be an error - because you\ncannot have two live instances of the same non-copyable type. The move\nconstructor is optional but helps the assignment into `s3`: without it, the\ncompiler would invoke the copy constructor from s1, then destroy the old `s1`\ninstance. This is logically correct but introduces extra runtime overhead.\n\nMojo destroys values eagerly, which allows it to transform\ncopy+destroy pairs into single move operations, which can lead to much better\nperformance than C++ without requiring the need for pervasive micromanagement\nof `std::move`.\n--- cell type: markdown ---\n### Trivial types\n\nThe most flexible types are ones that are just \"bags of bits\".  These types are\n\"trivial\" because they can be copied, moved, and destroyed without invoking\ncustom code.  Types like these are arguably the most common basic type that\nsurrounds us: things like integers and floating point values are all trivial.\nFrom a language perspective, Mojo doesnâ€™t need special support for these, it\nwould be perfectly fine for type authors to implement these things as no-ops,\nand allow the inliner to just make them go away.\n\nThere are two reasons that approach would be suboptimal: one is that we donâ€™t\nwant the boilerplate of having to define a bunch of methods on trivial types,\nand second, we donâ€™t want the compile-time overhead of generating and pushing\naround a bunch of function calls, only to have them inline away to nothing.\nFurthermore, there is an orthogonal concern, which is that many of these types\nare trivial in another way: they are tiny, and should be passed around in the\nregisters of a CPU, not indirectly in memory.\n\nAs such, Mojo provides a struct decorator that solves all of these problems.\nYou can implement a type with the `@register_passable(\"trivial\")` decorator,\nand this tells Mojo that the type should be copyable and movable but that it\nhas no user-defined logic for doing this.  It also tells Mojo to prefer to pass\nthe value in CPU registers, which can lead to efficiency benefits.\n\nTODO: This decorator is due for reconsideration.  Lack of custom logic\ncopy/move/destroy logic and \"passability in a register\" are orthogonal concerns\nand should be split.  This former logic should be subsumed into a more general\n`@value(\"trivial\")` decorator, which is orthogonal from `@register_passable`.\n--- cell type: markdown ---\n### `@value` decorator\n\nMojo's [value lifecycle](#value-lifecycle-birth-life-and-death-of-a-value) provides simple and predictable\nhooks that give you the ability to express exotic low-level things like\n`Atomic` correctly. This is great for control and for a simple programming\nmodel, but most structs are simple aggregations of other types,\nand we don't want to write a lot of boilerplate for them. To solve\nthis, Mojo provides a `@value` decorator for structs that synthesizes a lot of\nboilerplate for you.\n\nYou can think of `@value` as an extension of Python's\n[`@dataclass`](https://docs.python.org/3/library/dataclasses.html) that also\nhandles Mojo's `__moveinit__` and `__copyinit__` methods.\n\nThe `@value` decorator takes a look at the fields of your type, and generates\nsome members that are missing. For example, consider a simple struct like this:\n--- cell type: code ---\n@value\nstruct MyPet:\n    var name: String\n    var age: Int\n\n--- cell type: markdown ---\nMojo will notice that you do not have a member-wise initializer, a move\nconstructor, or a copy constructor, and it will synthesize these for you as if\nyou had written:\n--- cell type: code ---\nstruct MyPet:\n    var name: String\n    var age: Int\n\n    fn __init__(inout self, owned name: String, age: Int):\n        self.name = name^\n        self.age = age\n\n    fn __copyinit__(inout self, existing: Self):\n        self.name = existing.name\n        self.age = existing.age\n\n    fn __moveinit__(inout self, owned existing: Self):\n        self.name = existing.name^\n        self.age = existing.age\n\n--- cell type: markdown ---\nWhen you add the `@value` decorator, Mojo synthesizes each of these special\nmethods only when it doesn't exist. You can override the behavior of one or\nmore by defining your own version. For example, it is fairly common to want a\ncustom copy constructor but use the default member-wise and move constructor.\n\nThe arguments to`__init__` are all passed as `owned` arguments since the struct\ntakes ownership and stores the value.  This is a useful micro-optimization and\nenables the use of move-only types.  Trivial types like `Int` are also passed\nas owned values, but since that doesn't mean anything for them, we elide the\nmarker and the transfer operator (`^`) for clarity.\n\n<div class=\"alert alert-block alert-success\">\n\n**Note:** If your type contains any [move-only](#unique-move-only-types)\nfields, Mojo will not generate a copy constructor because it cannot copy those\nfields.  Further, the `@value` decorator only works on types whose members are\ncopyable and/or movable.  If you have something like `Atomic` in your struct,\nthen it probably isn't a value type, and you don't want these members anyway.\n\nAlso notice that the `MyPet` struct above doesn't include the `__del__()`\ndestructorâ€”Mojo also synthesizes this, but it doesn't require the `@value`\ndecorator (see the section below about [destructors](#behavior-of-destructors)).\n\n</div>\n\nThere is no way to suppress the generation of specific methods or customize\ngeneration at this time, but we can add arguments to the `@value` generator to\ndo this if there is demand.\n--- cell type: markdown ---\n## Behavior of destructors\n\nAny struct in Mojo can have a destructor (a `__del__()` method), which is\nautomatically run when the value's lifetime ends (typically the point at which\nthe value is last used). For example, a simple string might look like this (in\npseudo code):\n\n```mojo\n@value\nstruct MyString:\n    var data: Pointer[UInt8]\n\n    def __init__(inout self, input: StringRef): ...\n    def __add__(self, rhs: String) -> MyString: ...\n    def __del__(owned self):\n        free(self.data.address)\n```\n\nMojo destroys values like `MyString` (it calls the `__del__()` destructor)\nusing an **\"As Soon As Possible\"** (ASAP) policy that runs after every call.\nMojo does *not* wait until the end of the code block to destroy unused values.\nEven in an expression like `a+b+c+d`, Mojo destroys the intermediate\nexpressions eagerly, as soon as they are no longer neededâ€”it does not wait\nuntil the end of the statement.\n\nThe Mojo compiler automatically invokes the destructor when a value is dead\nand provides strong guarantees about when the destructor is run. Mojo uses\nstatic compiler analysis to reason about your code and decide when to insert\ncalls to the destructor. For example:\n--- cell type: code ---\nfn use_strings():\n    var a = String(\"hello a\")\n    let b = String(\"hello b\")\n    print(a)\n    # a.__del__() runs here for \"hello a\"\n\n\n    print(b)\n    # b.__del__() runs here\n\n    a = String(\"temporary a\")\n    # a.__del__() runs here because \"temporary a\" is never used\n\n    # Other stuff happens here\n\n    a = String(\"final a\")\n    print(a)\n    # a.__del__() runs again here for \"final a\"\n\nuse_strings()\n\n--- cell type: markdown ---\nIn the code above, youâ€™ll see that the `a` and `b` values are created early on,\nand each initialization of a value is matched with a call to a destructor.\nNotice that `a` is destroyed multiple timesâ€”once for each time it receives a\nnew value.\n\nNow, this might be surprising to a C++ programmer, because it's different from\nthe [RAII pattern](https://en.cppreference.com/w/cpp/language/raii) in which\nC++ destroys values at the end of a scope. Mojo also follows the principle\nthat values acquire resources in a constructor and release resources in a\ndestructor, but eager destruction in Mojo has a number of strong advantages\nover scope-based destruction in C++:\n\n- The Mojo approach eliminates the need for types to implement re-assignment\n  operators, like `operator=(const T&)` and `operator=(T&&)` in C++, making it\n  easier to define types and eliminating a concept.\n\n- Mojo does not allow mutable references to overlap with other mutable\n  references or with immutable borrows. One major way that it provides a\n  predictable programming model is by making sure that references to objects die\n  as soon as possible, avoiding confusing situations where the compiler thinks a\n  value could still be alive and interfere with another value, but that isnâ€™t\n  clear to the user.\n\n- Destroying values at last-use composes nicely with \"move\" optimization, which\n  transforms a \"copy+del\" pair into a \"move\" operation, a generalization of\n  C++ move optimizations like NRVO (named return value optimization).\n\n- Destroying values at end-of-scope in C++ is problematic for some common\n  patterns like tail recursion because the destructor calls happen after the\n  tail call. This can be a significant performance and memory problem for\n  certain functional programming patterns.\n\nImportantly, Mojo's eager destruction also works well within Python-style `def`\nfunctions to provide destruction guarantees (without a garbage collector) at a\nfine-grain levelâ€”recall that Python doesnâ€™t really provide scopes beyond a\nfunction, so C++-style destruction in Mojo would be a lot less useful.\n\n<div class=\"alert alert-block alert-success\">\n\n**Note:** Mojo also supports the Python-style [`with`\nstatement](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement),\nwhich provides more deliberately-scoped access to resources.\n\n</div>\n\nThe Mojo approach is more similar to how Rust and Swift work, because they both\nhave strong value ownership tracking and provide memory safety.  One difference\nis that their implementations require the use of a [dynamic \"drop\nflag\"](https://doc.rust-lang.org/nomicon/drop-flags.html)â€”they maintain hidden\nshadow variables to keep track of the state of your values to provide safety.\nThese are often optimized away, but the Mojo approach eliminates this overhead\nentirely, making the generated code faster and avoiding ambiguity.\n--- cell type: markdown ---\n### Field-sensitive lifetime management\n\nIn addition to Mojoâ€™s lifetime analysis being fully control-flow aware, it is\nalso fully field-sensitive (each field of a structure is tracked\nindependently). That is, Mojo separately keeps track of whether a \"whole\nobject\" is fully or only partially initialized/destroyed.\n\nFor example, consider this code:\n--- cell type: code ---\n@value\nstruct TwoStrings:\n    var str1: String\n    var str2: String\n\nfn use_two_strings():\n    var ts = TwoStrings(\"foo\", \"bar\")\n    print(ts.str1)\n    # ts.str1.__del__() runs here\n\n    # Other stuff happens here\n\n    ts.str1 = String(\"hello\") # Overwrite ts.str1\n    print(ts.str1)\n    # ts.__del__() runs here\n\nuse_two_strings()\n\n--- cell type: markdown ---\nNote that the `ts.str1` field is destroyed almost immediately,\nbecause Mojo knows that it will be overwritten down below.  You can also see\nthis when using the [transfer operator](#owned-arguments), for example:\n--- cell type: code ---\nfn consume(owned arg: String):\n    pass\n\nfn use(arg: TwoStrings):\n    print(arg.str1)\n\nfn consume_and_use_two_strings():\n    var ts = TwoStrings(\"foo\", \"bar\")\n    consume(ts.str1^)\n    # ts.str1.__moveinit__() runs here\n\n    # ts is now only partially initialized here!\n\n    ts.str1 = String(\"hello\")  # All together now\n    use(ts)                    # This is ok\n    # ts.__del__() runs here\n\nconsume_and_use_two_strings()\n\n--- cell type: markdown ---\nNotice that the code transfers ownership of the `str1` field: for the duration\nof `other_stuff()`, the `str1` field is completely uninitialized because\nownership was transferred to `consume()`. Then\n`str1` is reinitialized before it is used by the `use()` function (if it\nwerenâ€™t, Mojo would reject the code with an uninitialized field error).\n\nMojo's rule on this is powerful and intentionally straight-forward: fields can\nbe temporarily transferred, but the \"whole object\" must be constructed with the\naggregate typeâ€™s initializer and destroyed with the aggregate destructor. This\nmeans that it isnâ€™t possible to create an object by initializing only its\nfields, nor is it possible to tear down an object by destroying only its\nfields. For example, this code does not compile:\n--- cell type: code ---\nfn consume_and_use_two_strings():\n    let ts = TwoStrings(\"foo\", \"bar\") # ts is initialized\n    # Uncomment to see an error:\n    # consume(ts.str1^)\n    # Because `ts` is not used anymore, it should be destroyed here, but\n    # the object is not whole, preventing the overall value from being destroyed\n\n    let ts2 : TwoStrings # ts2 type is declared but not initialized\n    ts2.str1 = String(\"foo\")\n    ts2.str2 = String(\"bar\")  # Both the member are initalized\n    # Uncomment to see an error:\n    # use(ts2) # Error: 'ts2' isn't fully initialized\n\n--- cell type: markdown ---\nWhile we could allow patterns like this to happen, we reject this because a\nvalue is more than a sum of its parts.  Consider a `FileDescriptor` that\ncontains a POSIX file descriptor as an integer value: there is a\nbig difference between destroying the integer (a no-op) and destroying the\n`FileDescriptor` (it might call the `close()` system call).  Because of this, we\nrequire all full-value initialization to go through initializers and be\ndestroyed with their full-value destructor.\n\nFor what it's worth, Mojo does internally have an equivalent of the Rust\n[`mem::forget`](https://doc.rust-lang.org/std/mem/fn.forget.html) function,\nwhich explicitly disables a destructor and has a corresponding internal feature\nfor \"blessing\" an object, but they arenâ€™t exposed for user consumption at this\npoint.\n--- cell type: markdown ---\n### Field lifetimes in `__init__`\n\nThe behavior of an `__init__` method works almost like any other methodâ€”there\nis a small bit of magic: it knows that the fields of an object\nare uninitialized, but it believes the full object is initialized.  This means\nthat you can use `self` as a whole object as soon as all the fields are\ninitialized:\n--- cell type: code ---\nfn use(arg: TwoStrings2):\n    pass\n\nstruct TwoStrings2:\n    var str1: String\n    var str2: String\n\n    fn __init__(inout self, cond: Bool, other: String):\n        self.str1 = String()\n        if cond:\n            self.str2 = other\n            use(self)  # Safe to use immediately!\n            # self.str2.__del__(): destroyed because overwritten below.\n\n        self.str2 = self.str1\n        use(self)  # Safe to use immediately!\n\n--- cell type: markdown ---\n\nSimilarly, it's safe for initializers in Mojo to completely\noverwrite `self`, such as by delegating to other initializers:\n--- cell type: code ---\nstruct TwoStrings3:\n    var str1: String\n    var str2: String\n\n    fn __init__(inout self):\n        self.str1 = String()\n        self.str2 = String()\n\n    fn __init__(inout self, one: String):\n        self = TwoStrings3()  # Delegate to the basic init\n        self.str1 = one\n\n--- cell type: markdown ---\n### Field lifetimes of `owned` arguments in `__moveinit__` and `__del__` \n\nA final bit of magic exists for the `owned` arguments of a `__moveinit__()` move\ninitializer and a `__del__()` destructor. To recap, these method signatures look\nlike this:\n\n```mojo\nstruct TwoStrings:\n    ...\n    fn __moveinit__(inout self, owned existing: Self):\n        # Initializes a new `self` by consuming the contents of `existing`\n    fn __del__(owned self):\n        # Destroys all resources in `self`\n```\n\nThese methods face an interesting but obscure problem: both methods are in\ncharge of dismantling the `owned` `existing`/`self` value. That is,\n`__moveinit__()` destroys sub-elements of `existing` in order to transfer\nownership to a new instance, while `__del__()` implements the deletion logic\nfor its `self`. As such, they both want to own and transform elements of the\n`owned` value, and they definitely donâ€™t want the `owned` value's destructor to\nalso run (in the case of the `__del__()` method, that would turn into an\ninfinite loop).\n\nTo solve this problem, Mojo handles these two methods specially by assuming\nthat their whole values are destroyed upon reaching any return from the method.\nThis means that the whole object may be used before the field values are\ntransferred. For example, this works as you expect:\n--- cell type: code ---\nfn consume(owned str: String):\n    print('Consumed', str)\n\nstruct TwoStrings4:\n    var str1: String\n    var str2: String\n\n    fn __init__(inout self, one: String):\n        self.str1 = one\n        self.str2 = String(\"bar\")\n\n    fn __moveinit__(inout self, owned existing: Self):\n        self.str1 = existing.str1\n        self.str2 = existing.str2\n\n    fn __del__(owned self):\n        self.dump() # Self is still whole here\n        # Mojo calls self.str2.__del__() since str2 isn't used anymore\n\n        consume(self.str1^)\n        # str1 has now been transferred;\n        # `self.__del__()` is not called (avoiding an infinite loop).\n\n    fn dump(inout self):\n        print('str1:', self.str1)\n        print('str2:', self.str2)\n\nfn use_two_strings():\n    let two_strings = TwoStrings4(\"foo\")\n\n# We use a function call to ensure the `two_strings` ownership is enforced\n# (Currently, ownership is not enforced for top-level code in notebooks)\nuse_two_strings()\n\n--- cell type: markdown ---\nYou should not generally have to think about this, but if you have logic with\ninner pointers into members, you may need to keep them alive for some logic\nwithin the destructor or move initializer itself.  You can do this by assigning\nto the `_` \"discard\" pattern:\n\n```mojo\nfn __del__(owned self):\n    self.dump() # Self is still whole here\n\n    consume(self.str1^)\n    _ = self.str2\n    # self.str2.__del__(): Mojo destroys str2 after its last use.\n```\n--- cell type: markdown ---\nIn this case, if `consume()` implicitly refers to some value in `str2` somehow,\nthis will ensure that `str2` isnâ€™t destroyed until the last use when it is\naccessed by the `_` discard pattern.\n--- cell type: markdown ---\n### Defining the `__del__` destructor\n\nYou should define the `__del__()` method to perform any kind of cleanup the\ntype requires. Usually, that includes freeing memory for any fields that are\nnot trivial or destructibleâ€”Mojo automatically destroys any trivial and\ndestructible types as soon as they're not used anymore.\n\nFor example, consider this struct:\n--- cell type: code ---\nstruct MyPet:\n    var name: String\n    var age: Int\n\n    fn __init__(inout self, owned name: String, age: Int):\n        self.name = name^\n        self.age = age\n\n--- cell type: markdown ---\nThere's no need to define the `__del__()` method because `String` is a\ndestructible (it has its own `__del__()` method) and Mojo destroys it as soon\nas it's no longer used (which is exactly when the `MyPet` instance is no longer\nused), and `Int` is a [trivial type](#trivial-types) and Mojo reclaims this\nmemory also as soon as possible (although a little differently, without need\nfor a `__del__()` method).\n\nWhereas, the following struct must define the `__del__()` method to free\nthe memory allocated for its `Pointer`:\n--- cell type: code ---\nstruct Array[Type: AnyType]:\n    var data: Pointer[Type]\n    var size: Int\n\n    fn __init__(inout self, size: Int, value: Type):\n        self.size = size\n        self.data = Pointer[Type].alloc(self.size)\n        for i in range(self.size):\n            self.data.store(i, value)\n\n    fn __del__(owned self):\n        self.data.free()\n\n--- cell type: markdown ---\n## Lifetimes\n\nTODO: Explain how returning references work, tied into lifetimes which dovetail\nwith parameters.  This is not enabled yet.\n\n## Type traits\n\nThis is a feature very much like Rust traits or Swift protocols or Haskell type\nclasses. Note, this is not implemented yet.\n--- cell type: markdown ---\n## Advanced/Obscure Mojo features\n\nThis section describes power-user features that are important for building the\nbottom-est level of the standard library. This level of the stack is inhabited\nby narrow features that require experience with compiler internals to\nunderstand and utilize effectively.\n\n### `@register_passable` struct decorator\n\nThe default model for working with values is\nthey live in memory, so they have an identity, which means they are passed\nindirectly to and from functions (equivalently, they are passed \"by reference\"\nat the machine level). This is great for types that cannot be moved, and is a\nsafe default for large objects or things with expensive copy operations.\nHowever, it is inefficient for tiny things like a single integer or\nfloating point number.\n\nTo solve this, Mojo allows structs to opt-in to being passed in a register\ninstead of passing through memory with the `@register_passable` decorator.\nYou'll see this decorator on types like `Int` in the standard library:\n\n```mojo\n@register_passable(\"trivial\")\nstruct Int:\n    var value: __mlir_type.`!pop.scalar<index>`\n\n    fn __init__(value: __mlir_type.`!pop.scalar<index>`) -> Self:\n        return Self {value: value}\n    ...\n```\n\nThe basic `@register_passable` decorator does not change the fundamental\nbehavior of a type: it still needs to have a `__copyinit__` method to be\ncopyable, may still have a `__init__` and `__del__` methods, etc. The major\neffect of this decorator is on\ninternal implementation details: `@register_passable` types are typically\npassed in machine registers (subject to the details of the underlying\narchitecture).\n\nThere are only a few observable effects of this decorator to the typical Mojo\nprogrammer:\n\n1. `@register_passable` types are not able to hold instances of types\nthat are not themselves `@register_passable`.\n\n2. Instances of `@register_passable` types do not have predictable identity,\nand so the `self` pointer is not stable/predictable (e.g. in hash tables).\n\n3. `@register_passable` arguments and result are exposed to C and C++ directly,\ninstead of being passed by-pointer.\n\n4. The `__init__` and `__copyinit__` methods of this type are implicitly static\n(like `__new__` in Python) and return their results by-value instead of taking\n`inout self`.\n\nWe expect that this decorator will be used pervasively on core standard library\ntypes, but is safe to ignore for general application level code.\n\nThe `Int` example above actually uses the \"trivial\" variant of this decorator.\nIt changes the passing convention as described above but also disallows copy\nand move constructors and destructors (synthesizing them all trivially).\n\n> TODO: Trivial needs to be decoupled to its own decorator since it applies to\nmemory types as well.\n\n<!--\nTOWRITE: Each builtin decorator should be mentioned. Eventually, decorators\nshould appear in the API docs.\n\n> TODO: We need to decide how to namespace these, should these go into a 'mojo'\npackage or something?\n-->\n\n### `@always_inline` decorator\n\n`@always_inline(\"nodebug\")`: same thing but without debug information so you\ndon't step into the + method on Int.\n\n### `@parameter` decorator\n\nThe `@parameter` decorator can be placed on nested functions that capture\nruntime values to create \"parametric\" capturing closures. This is an unsafe\nfeature in Mojo, because we do not currently model the lifetimes of\ncapture-by-reference. A particular aspect of this feature is that it allows\nclosures that capture runtime values to be passed as parameter values.\n\n### Magic operators\n\nC++ code has a number of magic operators that intersect with value lifecycle,\nthings like \"placement new\", \"placement delete\" and \"operator=\" that reassign\nover an existing value. Mojo is a safe language when you use all its language\nfeatures and compose on top of safe constructs, but of any stack is a world of\nC-style pointers and rampant unsafety. Mojo is a pragmatic language, and since\nwe are interested in both interoperating with C/C++ and in implementing safe\nconstructs like String directly in Mojo itself, we need a way to express unsafe\nthings.\n\nThe Mojo standard library `Pointer[element_type]` type is implemented with an\nunderlying `!kgen.pointer<element_type>` type in MLIR, and we desire a way to\nimplement these C++-equivalent unsafe constructs in Mojo. Eventually, these\nwill migrate to all being methods on the Pointer type, but until then, some\nneed to be exposed as built-in operators.\n\n<!--\nTODO: document all of these:\n\n```mojo\n__get_address_as_lvalue(x)\n__get_address_as_uninit_lvalue(x)\n__get_lvalue_as_address(x):  use Pointer.address_of instead\n__get_address_as_owned_value(x)\n```\n-->\n\n### Direct access to MLIR\n\nMojo provides full access to the MLIR dialects and ecosystem. Please take a\nlook at the [Low level IR in Mojo](/mojo/notebooks/BoolMLIR.html) to learn how\nto use the `__mlir_type`, `__mlir_op`, and `__mlir_type` constructs. All of the\nbuilt-in and standard library APIs are implemented by just calling the\nunderlying MLIR constructs, and in doing so, Mojo effectively serves as syntax\nsugar on top of MLIR.\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/pymatmul.py",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# Simple program demonstrating a naive matrix multiplication in Python\nfrom timeit import timeit\nimport check_mod\n\ncheck_mod.install_if_missing(\"numpy\")\nimport numpy as np\n\n\nclass PyMatrix:\n    def __init__(self, value, rows, cols):\n        self.value = value\n        self.rows = rows\n        self.cols = cols\n\n    def __getitem__(self, idxs):\n        return self.value[idxs[0]][idxs[1]]\n\n    def __setitem__(self, idxs, value):\n        self.value[idxs[0]][idxs[1]] = value\n\n\ndef matmul_python(C, A, B):\n    for m in range(C.rows):\n        for k in range(A.cols):\n            for n in range(C.cols):\n                C[m, n] += A[m, k] * B[k, n]\n\n\ndef benchmark_matmul_python(M, N, K):\n    A = PyMatrix(list(np.random.rand(M, K)), M, K)\n    B = PyMatrix(list(np.random.rand(K, N)), K, N)\n    C = PyMatrix(list(np.zeros((M, N))), M, N)\n    secs = timeit(lambda: matmul_python(C, A, B), number=2) / 2\n    gflops = ((2 * M * N * K) / secs) / 1e9\n    return gflops\n\n\ndef benchmark_matmul_numpy(M, N, K):\n    A = np.random.rand(M, K)\n    B = np.random.rand(K, N)\n    secs = timeit(lambda: A @ B, number=10) / 10\n    gflops = ((2 * M * N * K) / secs) / 1e9\n    return gflops\n\n\nif __name__ == \"__main__\":\n    print(\"Throughput of a 128x128 matrix multiplication in Python:\")\n    print(benchmark_matmul_python(128, 128, 128))\n    print(\"Throughput of a 128x128 matrix multiplication in Python numpy:\")\n    print(benchmark_matmul_numpy(128, 128, 128))\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/reduce.mojo",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n# This sample implements a simple reduction operation on a\n# large array of values to produce a single result.\n# Reductions and scans are common algorithm patterns in parallel computing.\n\nimport benchmark\nfrom time import now\nfrom algorithm import sum\nfrom random import rand\nfrom memory.buffer import Buffer\nfrom python import Python\n\n# Change these numbers to reduce on different sizes\nalias size_small: Int = 1 << 21\nalias size_large: Int = 1 << 27\n\n# Datatype for Tensor/Array\nalias type = DType.float32\n\n\n# Use the https://en.wikipedia.org/wiki/Kahan_summation_algorithm\n# Simple summation of the array elements\nfn naive_reduce_sum[size: Int](array: Tensor[type]) -> Float32:\n    let A = array\n    var my_sum = array[0]\n    var c: Float32 = 0.0\n    for i in range(array.dim(0)):\n        let y = array[i] - c\n        let t = my_sum + y\n        c = (t - my_sum) - y\n        my_sum = t\n    return my_sum\n\n\nfn stdlib_reduce_sum[size: Int](array: Tensor[type]) -> Float32:\n    let my_sum = sum(array._to_buffer())\n    return my_sum\n\n\nfn pretty_print(name: StringLiteral, elements: Int, time: Float64) raises:\n    let py = Python.import_module(\"builtins\")\n    _ = py.print(\n        py.str(\"{:<16} {:>11,} {:>8.2f}ms\").format(\n            String(name) + \" elements:\", elements, time\n        )\n    )\n\n\nfn bench[\n    func: fn[size: Int] (array: Tensor[type]) -> Float32,\n    size: Int,\n    name: StringLiteral,\n](array: Tensor[type]) raises:\n    @parameter\n    fn runner():\n        let result = func[size](array)\n        benchmark.keep(result)\n\n    let ms = benchmark.run[runner]().mean[benchmark.Unit.ms]()\n    pretty_print(name, size, ms)\n\n\nfn main() raises:\n    print(\n        \"Sum all values in a small array and large array\\n\"\n        \"Shows algorithm.sum from stdlib with much better scaling\\n\"\n    )\n    # Create two 1-dimensional tensors i.e. arrays\n    let small_array = rand[type](size_small)\n    let large_array = rand[type](size_large)\n\n    bench[naive_reduce_sum, size_small, \"naive\"](small_array)\n    bench[naive_reduce_sum, size_large, \"naive\"](large_array)\n\n    bench[stdlib_reduce_sum, size_small, \"stdlib\"](small_array)\n    bench[stdlib_reduce_sum, size_large, \"stdlib\"](large_array)\n"
    },
    {
        "url": "https://github.com/modularml/mojo/blob/main/examples/simple_interop.py",
        "content": "# ===----------------------------------------------------------------------=== #\n# Copyright (c) 2023, Modular Inc. All rights reserved.\n#\n# Licensed under the Apache License v2.0 with LLVM Exceptions:\n# https://llvm.org/LICENSE.txt\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===----------------------------------------------------------------------=== #\n\n\n# Simple python program to test interop\n\nimport check_mod\n\ncheck_mod.install_if_missing(\"numpy\")\nimport numpy as np\n\n\ndef test_interop_func():\n    print(\"Hello from Python!\")\n    a = np.array([1, 2, 3])\n    print(\"I can even print a numpy array: \", a)\n\n\nif __name__ == \"__main__\":\n    from timeit import timeit\n\n    print(timeit(lambda: test_interop_func(), number=1))\n"
    }
]